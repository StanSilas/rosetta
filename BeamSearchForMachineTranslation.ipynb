{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeamSearch for seq2seq model in keras that translates english to german"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After [implementing Bytepairencoding](BytepairencodingForMachineTranslation.ipynb), I'll now optimize the decoding inference mechanism. Instead of always taking the most likely next symbol when decoding, BeamSearch keeps a candidate list of `beam_width` best translations so far and expands them all for the next symbol and takes from those new candidates again the best `beam_width` ones. I will implement it by scratch in python here. It would be an alternative to use the BeamSearch method from `tensorflow` (and probably use `tf.keras` instead of `Keras`). As my main purpose here is to go step by step on my own, I'll follow the educational approach to do it myself and use https://gist.github.com/udibr/67be473cf053d8c38730 as a template.\n",
    "\n",
    "As trainings set I use the [European Parliament Proceedings Parallel Corpus 1996-2011](http://statmt.org/europarl/) German-English corpus with medium sized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T11:38:45.267544Z",
     "start_time": "2018-05-16T11:38:42.086213Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# technical detail so that an instance (maybe running in a different window)\n",
    "# doesn't take all the GPU memory resulting in some strange error messages\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T11:38:46.031641Z",
     "start_time": "2018-05-16T11:38:45.269186Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import keras\n",
    "import keras.layers as L\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sentencepiece as spm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import bytepairencoding as bpe\n",
    "import seq2seq\n",
    "from utils import download_and_extract_resources, read_europarl, preprocess_europarl as preprocess\n",
    "\n",
    "\n",
    "# Fixing random state ensure reproducible results\n",
    "RANDOM_STATE=42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "pd.set_option('max_colwidth', 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T11:38:46.035521Z",
     "start_time": "2018-05-16T11:38:46.033043Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 50\n",
    "MAX_TARGET_LENGTH = 65\n",
    "LATENT_DIM = 512\n",
    "EMBEDDING_DIM = 300\n",
    "BPE_MERGE_OPERATIONS = 5_000  # I'd love to use 10_000 x 300, but this one is broken: https://github.com/bheinzerling/bpemb/issues/6\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "DROPOUT = 0.5\n",
    "TEST_SIZE = 500\n",
    "EMBEDDING_TRAINABLE = True  # Improves results significant and for at least it's not the most dominant training time factor (that's the output softmax layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T11:38:46.041343Z",
     "start_time": "2018-05-16T11:38:46.037098Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "PATH = 'data'\n",
    "INPUT_LANG = 'en'\n",
    "TARGET_LANG = 'de'\n",
    "LANGUAGES = [INPUT_LANG, TARGET_LANG]\n",
    "BPE_URL = {lang: f'http://cosyne.h-its.org/bpemb/data/{lang}/' for lang in LANGUAGES}\n",
    "BPE_MODEL_NAME = {lang: f'{lang}.wiki.bpe.op{BPE_MERGE_OPERATIONS}.model' for lang in LANGUAGES}\n",
    "BPE_WORD2VEC_NAME = {lang: f'{lang}.wiki.bpe.op{BPE_MERGE_OPERATIONS}.d{EMBEDDING_DIM}.w2v.bin' for lang in LANGUAGES}\n",
    "\n",
    "EXTERNAL_RESOURCES = {\n",
    "    # Europarl Corpus\n",
    "    'de-en.tgz': 'http://statmt.org/europarl/v7/de-en.tgz',\n",
    "    \n",
    "    # Bytepairencoding subwords (_MODEL_) and pretrained embeddings (_WORD2VEC_)\n",
    "    BPE_MODEL_NAME[INPUT_LANG]: f'{BPE_URL[INPUT_LANG]}/{BPE_MODEL_NAME[INPUT_LANG]}',\n",
    "    BPE_WORD2VEC_NAME[INPUT_LANG] + '.tar.gz': f'{BPE_URL[INPUT_LANG]}/{BPE_WORD2VEC_NAME[INPUT_LANG]}' + '.tar.gz',\n",
    "    BPE_MODEL_NAME[TARGET_LANG]: f'{BPE_URL[TARGET_LANG]}/{BPE_MODEL_NAME[TARGET_LANG]}',\n",
    "    BPE_WORD2VEC_NAME[TARGET_LANG] + '.tar.gz': f'{BPE_URL[TARGET_LANG]}/{BPE_WORD2VEC_NAME[TARGET_LANG]}' + '.tar.gz',\n",
    "    \n",
    "    # Bytepairencoded model weights from BytepairencodingForMachineTranslation.ipynb\n",
    "    'bytepairencoding_model_weights.h5': 'https://drive.google.com/open?id=1xK2QVTsIpJLmphSEUZl1Unqmz85MYeQK',\n",
    "    'bytepairencoding_inference_encoder_model_weights.h5': 'https://drive.google.com/open?id=115Kp7ZIMqxu6YDvk4RhjvfYShcQdRP_o',\n",
    "    'bytepairencoding_inference_decoder_model_weights.h5': 'https://drive.google.com/open?id=1_e3DE5lDw10joIb83UFbzGJyvQrMfb8w',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:50:57.098324Z",
     "start_time": "2018-05-15T14:50:57.093601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n"
     ]
    }
   ],
   "source": [
    "download_and_extract_resources(fnames_and_urls=EXTERNAL_RESOURCES, dest_path=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:50:59.217881Z",
     "start_time": "2018-05-15T14:50:57.100009Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\n",
    "    'input_texts': read_europarl(INPUT_LANG),\n",
    "    'target_texts': read_europarl(TARGET_LANG)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:51:00.196869Z",
     "start_time": "2018-05-15T14:50:59.220421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr total input: 1920209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resumption of the session</td>\n",
       "      <td>wiederaufnahme der sitzungsperiode</td>\n",
       "      <td>25</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i declare resumed the session of the european parliament...</td>\n",
       "      <td>ich erkläre die am freitag, dem 0. dezember unterbrochen...</td>\n",
       "      <td>203</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>although, as you will have seen, the dreaded 'millennium...</td>\n",
       "      <td>wie sie feststellen konnten, ist der gefürchtete \"millen...</td>\n",
       "      <td>191</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you have requested a debate on this subject in the cours...</td>\n",
       "      <td>im parlament besteht der wunsch nach einer aussprache im...</td>\n",
       "      <td>105</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in the meantime, i should like to observe a minute' s si...</td>\n",
       "      <td>heute möchte ich sie bitten - das ist auch der wunsch ei...</td>\n",
       "      <td>232</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input_texts  \\\n",
       "0                                    resumption of the session   \n",
       "1  i declare resumed the session of the european parliament...   \n",
       "2  although, as you will have seen, the dreaded 'millennium...   \n",
       "3  you have requested a debate on this subject in the cours...   \n",
       "4  in the meantime, i should like to observe a minute' s si...   \n",
       "\n",
       "                                                  target_texts  input_length  \\\n",
       "0                           wiederaufnahme der sitzungsperiode            25   \n",
       "1  ich erkläre die am freitag, dem 0. dezember unterbrochen...           203   \n",
       "2  wie sie feststellen konnten, ist der gefürchtete \"millen...           191   \n",
       "3  im parlament besteht der wunsch nach einer aussprache im...           105   \n",
       "4  heute möchte ich sie bitten - das ist auch der wunsch ei...           232   \n",
       "\n",
       "   target_length  \n",
       "0             34  \n",
       "1            217  \n",
       "2            185  \n",
       "3            110  \n",
       "4            217  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Nr total input:\", len(df))\n",
    "df.target_texts = df.target_texts  # encode a start symbol (doesn't occur in texts)\n",
    "df['input_length'] = df.input_texts.apply(len)\n",
    "df['target_length'] = df.target_texts.apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:51:00.320395Z",
     "start_time": "2018-05-15T14:51:00.198592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167211"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_empty = (df.input_length > 1) & (df.target_length > 1)  # there are empty phrases like '\\n' --> 'Frau Präsidentin\\n'\n",
    "short_inputs = (df.input_length < MAX_INPUT_LENGTH) & (df.target_length < MAX_TARGET_LENGTH)\n",
    "sum(non_empty & short_inputs)\n",
    "df = df[non_empty & short_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:51:00.387276Z",
     "start_time": "2018-05-15T14:51:00.322203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:51:00.527157Z",
     "start_time": "2018-05-15T14:51:00.388990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English subwords ['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n",
      "German subwords ['▁d', 'as', '▁is', 't', '▁e', 'in', '▁test', '▁f', 'ür', '▁v', 'ort', 'rain', 'ier', 'te', '▁ze', 'ic', 'hen', 'gr', 'up', 'p', 'en']\n"
     ]
    }
   ],
   "source": [
    "bpe_input, bpe_target = [bpe.Bytepairencoding(\n",
    "    word2vec_fname=os.path.join(PATH, BPE_WORD2VEC_NAME[lang]),\n",
    "    sentencepiece_fname=os.path.join(PATH, BPE_MODEL_NAME[lang]),\n",
    ") for lang in [INPUT_LANG, TARGET_LANG]] \n",
    "print(\"English subwords\", bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", bpe_input.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:51:07.729044Z",
     "start_time": "2018-05-15T14:51:00.528314Z"
    }
   },
   "outputs": [],
   "source": [
    "df['input_sequences'] = df.input_texts.apply(bpe_input.subword_indices)  #input_subword_indices)\n",
    "df['target_sequences'] = df.target_texts.apply(bpe_target.subword_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:51:07.793252Z",
     "start_time": "2018-05-15T14:51:07.730421Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len_input = df.input_sequences.apply(len).max()\n",
    "max_len_target = df.target_sequences.apply(len).max()\n",
    "# nr_input_tokens = len(input_wordvec_index)  \n",
    "# nr_target_tokens = len(target_wordvec_index)\n",
    "# \n",
    "# # one hot encoded y_t_output wouldn't fit into memory any longer\n",
    "# # so need to train/validate on batches generated on the fly\n",
    "# def create_batch_generator(samples_ids):\n",
    "#     \n",
    "#     def batch_generator():\n",
    "#         nr_batches = np.ceil(len(samples_ids) / BATCH_SIZE)\n",
    "#         while True:\n",
    "#             shuffled_ids = np.random.permutation(samples_ids)\n",
    "#             batch_splits = np.array_split(shuffled_ids, nr_batches)\n",
    "#             for batch_ids in batch_splits:\n",
    "#                 batch_X = pad_sequences(df.iloc[batch_ids].input_sequences, padding='post', maxlen=max_len_input)\n",
    "#                 batch_y = pad_sequences(df.iloc[batch_ids].target_sequences, padding='post', maxlen=max_len_target)\n",
    "#                 batch_y_t_output = keras.utils.to_categorical(batch_y[:,1:], num_classes=nr_target_tokens)\n",
    "#                 batch_x_t_input = batch_y[:,:-1]\n",
    "#                 yield ([batch_X, batch_x_t_input], batch_y_t_output)\n",
    "#     \n",
    "#     return batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:51:07.801059Z",
     "start_time": "2018-05-15T14:51:07.794574Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(df.shape[0]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:51:07.810996Z",
     "start_time": "2018-05-15T14:51:07.802581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150489, 16722)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nr_input_tokens, nr_target_tokens\n",
    "len(train_ids), len(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:51:09.172264Z",
     "start_time": "2018-05-15T14:51:07.813010Z"
    }
   },
   "outputs": [],
   "source": [
    "# encoder_gru = L.Bidirectional(\n",
    "#     L.GRU(LATENT_DIM // 2, dropout=DROPOUT, return_state=True, name='encoder_gru'),\n",
    "#     name='encoder_bidirectional'\n",
    "# )\n",
    "# decoder_gru = L.GRU(LATENT_DIM, dropout=DROPOUT, return_sequences=True, return_state=True, name='decoder_gru', dtype=tf.float32)\n",
    "# decoder_dense = L.Dense(nr_target_tokens, activation='softmax', name='decoder_outputs', dtype=tf.float32)\n",
    "# \n",
    "# input_embedding = L.Embedding(\n",
    "#     nr_input_tokens,\n",
    "#     FULL_EMBEDDING_DIM,\n",
    "#     mask_zero=True,\n",
    "#     weights=[input_embedding_matrix],\n",
    "#     trainable=EMBEDDING_TRAINABLE,\n",
    "#     name='input_embedding',\n",
    "#     dtype=tf.float32,\n",
    "# )\n",
    "# target_embedding = L.Embedding(\n",
    "#     nr_target_tokens,\n",
    "#     FULL_EMBEDDING_DIM,\n",
    "#     mask_zero=True,\n",
    "#     weights=[target_embedding_matrix],\n",
    "#     trainable=EMBEDDING_TRAINABLE,\n",
    "#     name='target_embedding',\n",
    "#     dtype=tf.float32,\n",
    "# )\n",
    "# \n",
    "# encoder_inputs = L.Input(shape=(max_len_input, ), dtype='int32', name='encoder_inputs')\n",
    "# encoder_embeddings = input_embedding(encoder_inputs)\n",
    "# _, encoder_state_1, encoder_state_2 = encoder_gru(encoder_embeddings)\n",
    "# encoder_states = L.concatenate([encoder_state_1, encoder_state_2])\n",
    "# \n",
    "# decoder_inputs = L.Input(shape=(max_len_target-1, ), dtype='int32', name='decoder_inputs')\n",
    "# decoder_mask = L.Masking(mask_value=0)(decoder_inputs)\n",
    "# decoder_embeddings_inputs = target_embedding(decoder_mask)\n",
    "# decoder_embeddings_outputs, _ = decoder_gru(decoder_embeddings_inputs, initial_state=encoder_states) \n",
    "# decoder_outputs = decoder_dense(decoder_embeddings_outputs)\n",
    "# \n",
    "# model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "# \n",
    "# inference_encoder_model = Model(encoder_inputs, encoder_states)\n",
    "#     \n",
    "# inference_decoder_state_inputs = L.Input(shape=(LATENT_DIM, ), dtype='float32', name='inference_decoder_state_inputs')\n",
    "# inference_decoder_embeddings_outputs, inference_decoder_states = decoder_gru(\n",
    "#     decoder_embeddings_inputs, initial_state=inference_decoder_state_inputs\n",
    "# )\n",
    "# inference_decoder_outputs = decoder_dense(inference_decoder_embeddings_outputs)\n",
    "# \n",
    "# inference_decoder_model = Model(\n",
    "#     [decoder_inputs, inference_decoder_state_inputs], \n",
    "#     [inference_decoder_outputs, inference_decoder_states]\n",
    "# )\n",
    "\n",
    "s2s = seq2seq.Seq2SeqWithBPE(\n",
    "    bpe_input=bpe_input,\n",
    "    bpe_target=bpe_target,\n",
    "    max_len_input=max_len_input,\n",
    "    max_len_target=max_len_target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:51:09.178546Z",
     "start_time": "2018-05-15T14:51:09.173751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, 27)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     (None, 39)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_embedding (Embedding)     (None, 27, 302)      1760358     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 39)           0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bidirectional (Bidirect [(None, 512), (None, 858624      input_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "target_embedding (Embedding)    (None, 39, 302)      1620834     masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           encoder_bidirectional[0][1]      \n",
      "                                                                 encoder_bidirectional[0][2]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 39, 512), (N 1251840     target_embedding[0][0]           \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 39, 5367)     2753271     decoder_gru[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 8,244,927\n",
      "Trainable params: 8,244,927\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, 39)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 39)           0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "target_embedding (Embedding)    (None, 39, 302)      1620834     masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "inference_decoder_state_inputs  (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 39, 512), (N 1251840     target_embedding[0][0]           \n",
      "                                                                 inference_decoder_state_inputs[0]\n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 39, 5367)     2753271     decoder_gru[1][0]                \n",
      "==================================================================================================\n",
      "Total params: 5,625,945\n",
      "Trainable params: 5,625,945\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "s2s.model.summary()\n",
    "s2s.inference_decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:51:09.185565Z",
     "start_time": "2018-05-15T14:51:09.183592Z"
    }
   },
   "outputs": [],
   "source": [
    "# s2s.model.compile(optimizer=keras.optimizers.Adam(clipnorm=1., clipvalue=.5), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:51:09.336088Z",
     "start_time": "2018-05-15T14:51:09.187223Z"
    }
   },
   "outputs": [],
   "source": [
    "# s2s.model.load_weights('models/bytepairencoding_model_weights.h5')\n",
    "s2s.inference_encoder_model.load_weights(os.path.join(DATA, 'bytepairencoding_inference_encoder_model_weights.h5'))\n",
    "s2s.inference_decoder_model.load_weights(os.path.join(DATA, 'bytepairencoding_inference_decoder_model_weights.h5'))\n",
    "# train_generator = create_batch_generator(train_ids)\n",
    "# val_generator = create_batch_generator(val_ids)\n",
    "# model.fit_generator(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=np.ceil(len(train_ids) / BATCH_SIZE),\n",
    "#     epochs=EPOCHS,\n",
    "#     validation_data=val_generator,\n",
    "#     validation_steps=np.ceil(len(val_ids) / BATCH_SIZE),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T15:55:32.557028Z",
     "start_time": "2018-05-15T15:55:32.544525Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = s2s.inference_encoder_model.predict(input_seq)\n",
    "    \n",
    "    tokens = {idx: token for (token, idx) in bpe_target.wordvec_index.items()}\n",
    "    start_token_idx = bpe_target.wordvec_index['<s>']\n",
    "    end_token_idx = bpe_target.wordvec_index['</s>']\n",
    "    \n",
    "    target_seq = np.zeros((1, max_len_target-1))\n",
    "    target_seq[0, 0] = start_token_idx\n",
    "    \n",
    "    decoded_sequence = [] \n",
    "    for i in range(max_len_target):\n",
    "        output_tokens, output_states = s2s.inference_decoder_model.predict(\n",
    "            [target_seq, states_value]\n",
    "        )\n",
    "        \n",
    "        # greedy search\n",
    "        sampled_token_idx = np.argmax(output_tokens[0, 0, :])\n",
    "        if sampled_token_idx == end_token_idx:\n",
    "            break\n",
    "        sampled_token = tokens.get(sampled_token_idx, '~')\n",
    "        decoded_sequence.append(sampled_token)\n",
    "            \n",
    "        target_seq[0, 0] = sampled_token_idx\n",
    "        states_value = output_states\n",
    "    \n",
    "    return bpe_target.sentencepiece.DecodePieces(decoded_sequence)\n",
    "\n",
    "def decode_beam_search(input_seq, beam_width, branch_width=None):\n",
    "    if branch_width is None:\n",
    "        branch_width = beam_width\n",
    "        \n",
    "    initial_states = s2s.inference_encoder_model.predict(input_seq)\n",
    "    \n",
    "    top_candidates = [{\n",
    "        'states': initial_states,\n",
    "        'idx_sequence': [bpe_target.start_token_idx],\n",
    "        'token_sequence': [bpe_target.start_token],\n",
    "        'score': 0.0,\n",
    "        'live': True\n",
    "    }]\n",
    "    live_k = 1\n",
    "    dead_k = 0\n",
    "    \n",
    "    for _ in range(max_len_target):\n",
    "        if not(live_k and dead_k < beam_width):\n",
    "            break\n",
    "        new_candidates = []\n",
    "        for candidate in top_candidates:\n",
    "            if not candidate['live']:\n",
    "                new_candidates.append(candidate)\n",
    "                continue\n",
    "         \n",
    "            target_seq = np.zeros((1, max_len_target - 1))\n",
    "            target_seq[0, 0] = candidate['idx_sequence'][-1]\n",
    "            output, states = s2s.inference_decoder_model.predict(\n",
    "                [target_seq, candidate['states']]\n",
    "            )\n",
    "            probs = output[0, 0, :]\n",
    "        \n",
    "            for idx in np.argsort(-probs)[:branch_width]:\n",
    "                new_candidates.append({\n",
    "                    'states': states,\n",
    "                    'idx_sequence': candidate['idx_sequence'] + [idx],\n",
    "                    'token_sequence': candidate['token_sequence'] + [bpe_target.tokens[idx]],\n",
    "                    # sum -log(prob) numerical more stable than to multiplate probs                    \n",
    "                    # goal now to minimize the score\n",
    "                    'score': candidate['score'] - np.log(probs[idx]),  \n",
    "                    'live': idx != bpe_target.stop_token_idx,\n",
    "                })\n",
    "        \n",
    "        top_candidates = sorted(\n",
    "            new_candidates, key=lambda c: c['score']\n",
    "        )[:beam_width]\n",
    "        \n",
    "    return bpe_target.sentencepiece.DecodePieces(top_candidates[0]['token_sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T16:14:28.434803Z",
     "start_time": "2018-05-15T16:14:28.430080Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence, beam_width=10, branch_width=2):\n",
    "    return decode_beam_search(keras.preprocessing.sequence.pad_sequences(\n",
    "        [bpe_input.subword_indices(preprocess(sentence))],\n",
    "        padding='post',\n",
    "        maxlen=max_len_input,\n",
    "    ), beam_width=beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T16:14:39.079012Z",
     "start_time": "2018-05-15T16:14:29.566642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello.' --> 'hallam.'\n",
      "'you are welcome.' --> 'seien sie willkommen.'\n",
      "'how do you do?' --> 'was tun sie?'\n",
      "'i hate mondays.' --> 'ich habe vielleicht.'\n",
      "'i am a programmer.' --> 'ich bin ein programm.'\n",
      "'data is the new oil.' --> 'daten sind die öls.'\n",
      "'it could be worse.' --> 'das könnte besorgniserregend sein.'\n",
      "'i am on top of it.' --> 'ich gehe davon aus.'\n",
      "'n° uno' --> 'änderungsantrag 0'\n",
      "'awesome!' --> 'einverstanden!'\n",
      "'put your feet up!' --> 'fangen sie hart!'\n",
      "'from the start till the end!' --> 'mit dem zielsetzungen!'\n",
      "'from dusk till dawn.' --> 'als dilemma.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T15:56:41.092138Z",
     "start_time": "2018-05-15T15:56:31.287197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \"please rise, then, for this minute' s silence.\", got 'bitte lassen sie mich das wort äußern.', exp: 'ich bitte sie, sich zu einer schweigeminute zu erheben.'\n",
      "Original \"(the house rose and observed a minute' s silence)\", got '(das parlament erhebt sich zu einer schweigeminute.)', exp: '(das parlament erhebt sich zu einer schweigeminute.)'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'thank you, mr segni, i shall do so gladly.', got 'vielen dank, herr segni.', exp: 'vielen dank, herr segni, das will ich gerne tun.'\n",
      "Original 'it is the case of alexander nikitin.', got 'das ist der fall von alexander nikitin.', exp: 'das ist der fall von alexander nikitin.'\n",
      "Original 'it will, i hope, be examined in a positive light.', got 'ich hoffe, dass dort hineingekommen wird.', exp: 'ich hoffe, daß dort in ihrem sinne entschieden wird.'\n",
      "Original 'why are there no fire instructions?', got 'warum gibt es keine brandschutzbelehrungen?', exp: 'warum finden keine brandschutzbelehrungen statt?'\n",
      "Original 'mr berenguer fuster, we shall check all this.', got 'herr berriege, wir werden dies prüfen.', exp: 'lieber kollege, wir werden das prüfen.'\n",
      "Original 'we do not know what is happening.', got 'wir wissen nicht, was geschieht.', exp: 'wir wissen nicht, was passiert.'\n",
      "Original 'agenda', got 'tagesordnung', exp: 'arbeitsplan'\n",
      "Original 'relating to wednesday:', got 'zum mittwoch:', exp: 'zum mittwoch:'\n",
      "Original '(applause from the pse group)', got '(beifall von der pse-fraktion)', exp: '(beifall der pse-fraktion)'\n",
      "Original 'mr hänsch represented you on this occasion.', got 'herr seppänen hat sie vertreten.', exp: 'der kollege hänsch hat sie dort vertreten.'\n",
      "Original 'we then put it to a vote.', got 'wir haben dann zustimmen.', exp: 'wir haben dann abgestimmt.'\n",
      "Original 'there was a vote on this matter.', got 'es gab eine abstimmung.', exp: 'es gab eine abstimmung zu diesem punkt.'\n",
      "Original 'all of the others were of a different opinion.', got 'alle anderen waren anderer meinung.', exp: 'alle anderen waren anderer meinung.'\n",
      "Original 'that was the decision.', got 'das war die entscheidung.', exp: 'das war der beschluß.'\n",
      "Original 'i should now like to comment on the issue itself.', got 'jetzt möchte ich mich ansprechen.', exp: 'jetzt möchte ich zur sache selbst etwas sagen.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T15:57:32.855044Z",
     "start_time": "2018-05-15T15:57:23.015635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'a lot of inspiration is needed.', got 'eine vielversprechnung ist erforderlich.', exp: 'diese denkanstöße sind dringend nötig.'\n",
      "Original 'so what was cancún about?', got 'was ist also in cancún?', exp: 'worum ging es in cancún?'\n",
      "Original 'i now turn to another subject.', got 'ich komme nun zu einem weiteren punkt.', exp: 'ich komme jetzt zu einem anderen punkt.'\n",
      "Original 'thank you, mr prodi.', got 'vielen dank, herr prodi.', exp: 'vielen dank, herr prodi.'\n",
      "Original 'the experience acquired varied somewhat.', got 'die erfahrungen haben etwas ähnliches.', exp: 'die damit gemachten erfahrungen sind recht unterschiedlich.'\n",
      "Original 'that is what mr smith suggested.', got 'das hat smith gesagt.', exp: 'das hat herr smith vorgeschlagen.'\n",
      "Original 'it is stable internally and externally.', got 'das ist in der tat exprimierend.', exp: 'er ist stabil nach innen und außen.'\n",
      "Original 'mr president, i am wearing my irish scarf today.', got 'herr präsident, ich möchte meinen irischen bedauern.', exp: 'herr präsident, ich trage heute mein irisches tuch.'\n",
      "Original 'it will not lead to any savings.', got 'es wird keinen beitrag leisten.', exp: 'es werden keine einsparungen eintreten.'\n",
      "Original '(applause)', got '(beifall)', exp: '(beifall)'\n",
      "Original 'it will be noted in the minutes.', got 'das wird im protokoll aufgenommen.', exp: 'wir nehmen dies zu protokoll.'\n",
      "Original 'we need another great leap forward.', got 'wir müssen schneller voranbringen.', exp: 'was wir wieder brauchen, ist ein großer sprung.'\n",
      "Original 'that is the real problem.', got 'das ist das eigentliche problem.', exp: 'das ist das eigentliche problem.'\n",
      "Original '(the sitting was closed at 0.0 p.m.)', got '(die sitzung wird um 0.0 uhr geschlossen.)', exp: '(die sitzung wird um 0.0 uhr geschlossen.)0'\n",
      "Original 'how committed are you?', got 'wie stehen sie verpflichtet?', exp: 'wie engagiert sind sie wirklich?'\n",
      "Original 'what was lacking was the necessary legal force.', got 'was fehlte gelten, war notwendig.', exp: 'was fehlte, waren die gesetzlichen auflagen.'\n",
      "Original 'each of the speakers will have one minute.', got 'jede minute einer minute werden das wort ergreifen.', exp: 'jeder sprecher hat eine minute zeit.'\n",
      "Original 'but that is not enough.', got 'aber das ist nicht genug.', exp: 'aber das ist nicht genug.'\n",
      "Original 'this seems to me to be a workable solution.', got 'das halte ich für eine echte lösung.', exp: 'ich halte dieses vorgehen für angemessen.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T16:23:41.892234Z",
     "start_time": "2018-05-15T16:14:58.926722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbebe3e17a54bc1951dae53d07f37d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average bleu score 0.3276391602399966\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "try:\n",
    "    from spacy.lang.de import German\n",
    "except ModuleNotFoundError:\n",
    "    spacy.cli.download('de')\n",
    "    from spacy.lang.de import German\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "parser = German()\n",
    "chencherry = SmoothingFunction()  # to handle short sequences, see also http://www.nltk.org/_modules/nltk/translate/bleu_score.html#SmoothingFunction.method3\n",
    "\n",
    "def remove_spaces_and_puncts(tokens):\n",
    "     return [token.orth_ for token in tokens if not (token.is_space or token.is_punct)]  \n",
    "\n",
    "bleu_scores = np.zeros(TEST_SIZE)\n",
    "nist_scores = np.zeros(TEST_SIZE)\n",
    "\n",
    "for i in tqdm(range(TEST_SIZE)):\n",
    "    pred_tokens = remove_spaces_and_puncts(parser(predict(df.iloc[val_ids[i]].input_texts)))\n",
    "    ref_tokens = remove_spaces_and_puncts(parser(df.iloc[val_ids[i]].target_texts))\n",
    "    bleu_scores[i] = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=chencherry.method3)\n",
    "    \n",
    "print(\"Average bleu score\", bleu_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.992652Z",
     "start_time": "2018-05-13T18:17:20.758836Z"
    }
   },
   "outputs": [],
   "source": [
    "name = 'bytepairencoding'\n",
    "model.save_weights(f'models/{name}_model_weights.h5')  # https://drive.google.com/open?id=1xK2QVTsIpJLmphSEUZl1Unqmz85MYeQK\n",
    "inference_encoder_model.save_weights(f'models/{name}_inference_encoder_model_weights.h5')  # https://drive.google.com/open?id=115Kp7ZIMqxu6YDvk4RhjvfYShcQdRP_o\n",
    "inference_decoder_model.save_weights(f'models/{name}_inference_decoder_model_weights.h5')  # https://drive.google.com/open?id=1_e3DE5lDw10joIb83UFbzGJyvQrMfb8w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "It is definitly an improvement to the pure char level, allthough a lot of the translations still are more funny than decent. The bleu scores correlates with this feeling with a very significant increase ($0.316 > 0.213$).\n",
    "On a side note, the training could be early stopped after a few epochs (and indeed overfits from that moment). So, this new model has a lot of power left to be filled with either more data or a better model. But the next thing missing is the Beam Search instead of the Greedy Search. I guess, this will reduce a bit some of the strangeness/funny translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 301,
   "position": {
    "height": "316px",
    "left": "554px",
    "right": "191px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
