{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple seq2seq model in keras that translates english <-> german"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step I take the model used for the [toy problem of adding/subtracting numbers](SimpleModelForAddingAndSubstraction.ipynb) and train it with english/german data for machine translation.\n",
    "\n",
    "As trainings set I use the [European Parliament Proceedings Parallel Corpus 1996-2011](http://statmt.org/europarl/) German-English corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:05.739042Z",
     "start_time": "2018-05-12T11:14:04.407795Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# technical detail so that an instance (maybe running in a different window)\n",
    "# doesn't take all the GPU memory resulting in some strange error messages\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:05.997225Z",
     "start_time": "2018-05-12T11:14:05.740395Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import keras\n",
    "import keras.layers as L\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Fixing random state ensure reproducible results\n",
    "RANDOM_STATE=42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.set_random_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:06.000588Z",
     "start_time": "2018-05-12T11:14:05.998404Z"
    }
   },
   "outputs": [],
   "source": [
    "START = '^'\n",
    "END = '\\n'\n",
    "\n",
    "MAX_INPUT_LENGTH = 25 #50\n",
    "MAX_TARGET_LENGTH = 35 #65\n",
    "LATENT_DIM = 512\n",
    "EMBEDDING_DIM = 64\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "TEST_SIZE = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:06.011246Z",
     "start_time": "2018-05-12T11:14:06.001918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n"
     ]
    }
   ],
   "source": [
    "def download_file(fname, url):\n",
    "    print(f\"Downloading {fname} from {url} ...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    total_size = int(response.headers.get('content-length', 0)); \n",
    "    block_size = 1024\n",
    "\n",
    "    download = tqdm(\n",
    "        response.iter_content(block_size),\n",
    "        total=math.ceil(total_size // block_size),\n",
    "        unit='KB',\n",
    "        unit_scale=True\n",
    "    )\n",
    "    with open(f\"{fname}\", \"wb\") as handle:\n",
    "        for data in download:\n",
    "            handle.write(data)\n",
    "\n",
    "PATH = 'data'\n",
    "FILES = {\n",
    "    'de-en.tgz': 'http://statmt.org/europarl/v7/de-en.tgz',  # incredible: really only http, not https :-o\n",
    "}\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "for name, url in FILES.items():\n",
    "    fname = os.path.join(PATH, name)\n",
    "    exists = os.path.exists(fname)\n",
    "    size = os.path.getsize(fname) if exists else -1\n",
    "    if exists and size > 0:\n",
    "        print(f'{name} already downloaded ({size / 2**20:3.1f} MB)')\n",
    "        continue\n",
    "    download_file(fname, url)\n",
    "    if (fname.endswith(\".tgz\")):\n",
    "        tar = tarfile.open(fname, \"r:gz\")\n",
    "        tar.extractall(path=PATH)\n",
    "        tar.close()\n",
    "        print(f'Extracted {fname} ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:54.986266Z",
     "start_time": "2018-05-12T11:14:06.012780Z"
    }
   },
   "outputs": [],
   "source": [
    "# Following https://github.com/bheinzerling/bpemb/blob/master/preprocess_text.sh\n",
    "# (ignoring urls as there shouldn't be any in parliament discussions)\n",
    "def preprocess(line):\n",
    "    line = re.sub(r'\\d+', '0', line)  # replace all numbers by '0'\n",
    "    line = re.sub(r'\\s+', ' ', line)  # strip whitespaces as a ' '\n",
    "    return line.lower().strip()\n",
    "\n",
    "def read_corpus_lines(language):\n",
    "    return [preprocess(line) for line in open(f'{PATH}/europarl-v7.de-en.{language}', 'r').readlines()]\n",
    "    \n",
    "pd.set_option('max_colwidth', 60)\n",
    "df = pd.DataFrame(data={\n",
    "    'input_texts': read_corpus_lines('en'),\n",
    "    'target_texts': read_corpus_lines('de'), \n",
    "})\n",
    "df.input_texts = START + df.input_texts + END\n",
    "df.target_texts = START + df.target_texts + END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:55.994664Z",
     "start_time": "2018-05-12T11:14:54.988693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920209"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>^resumption of the session\\n</td>\n",
       "      <td>^wiederaufnahme der sitzungsperiode\\n</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>^i declare resumed the session of the european parliamen...</td>\n",
       "      <td>^ich erkläre die am freitag, dem 0. dezember unterbroche...</td>\n",
       "      <td>205</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>^although, as you will have seen, the dreaded 'millenniu...</td>\n",
       "      <td>^wie sie feststellen konnten, ist der gefürchtete \"mille...</td>\n",
       "      <td>193</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>^you have requested a debate on this subject in the cour...</td>\n",
       "      <td>^im parlament besteht der wunsch nach einer aussprache i...</td>\n",
       "      <td>107</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>^in the meantime, i should like to observe a minute' s s...</td>\n",
       "      <td>^heute möchte ich sie bitten - das ist auch der wunsch e...</td>\n",
       "      <td>234</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input_texts  \\\n",
       "0                                 ^resumption of the session\\n   \n",
       "1  ^i declare resumed the session of the european parliamen...   \n",
       "2  ^although, as you will have seen, the dreaded 'millenniu...   \n",
       "3  ^you have requested a debate on this subject in the cour...   \n",
       "4  ^in the meantime, i should like to observe a minute' s s...   \n",
       "\n",
       "                                                  target_texts  input_length  \\\n",
       "0                        ^wiederaufnahme der sitzungsperiode\\n            27   \n",
       "1  ^ich erkläre die am freitag, dem 0. dezember unterbroche...           205   \n",
       "2  ^wie sie feststellen konnten, ist der gefürchtete \"mille...           193   \n",
       "3  ^im parlament besteht der wunsch nach einer aussprache i...           107   \n",
       "4  ^heute möchte ich sie bitten - das ist auch der wunsch e...           234   \n",
       "\n",
       "   target_length  \n",
       "0             36  \n",
       "1            219  \n",
       "2            187  \n",
       "3            112  \n",
       "4            219  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)\n",
    "df['input_length'] = df.input_texts.apply(len)\n",
    "df['target_length'] = df.target_texts.apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only use short translations right now\n",
    "\n",
    "So, first I plot sentence length on a logarithmic scale, \n",
    "then I only choose short input texts (and a bit longer target texts as german is more verbose than english)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:56.519560Z",
     "start_time": "2018-05-12T11:14:55.996865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = df.input_length\n",
    "logbins = np.logspace(1,5,20)\n",
    "plt.hist(x, bins=logbins)\n",
    "plt.xscale('log')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:56.613770Z",
     "start_time": "2018-05-12T11:14:56.521086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37048"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_empty = (df.input_length > 3) & (df.target_length > 3)  # there are empty phrases like '\\n' --> 'Frau Präsidentin\\n'\n",
    "short_inputs = (df.input_length < MAX_INPUT_LENGTH) & (df.target_length < MAX_TARGET_LENGTH)\n",
    "sum(short_inputs)\n",
    "df = df[non_empty & short_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:56.620232Z",
     "start_time": "2018-05-12T11:14:56.615714Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = pd.concat([df.input_texts, df.target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:57.290081Z",
     "start_time": "2018-05-12T11:14:56.622371Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=100, filters=None, char_level=True, oov_token='~')\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "df['input_sequences'] = tokenizer.texts_to_sequences(df.input_texts)\n",
    "df['target_sequences'] = tokenizer.texts_to_sequences(df.target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:57.298612Z",
     "start_time": "2018-05-12T11:14:57.291443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 110091),\n",
       " ('e', 108525),\n",
       " ('s', 77830),\n",
       " ('i', 65877),\n",
       " ('a', 63543),\n",
       " ('t', 62244),\n",
       " ('\\n', 55276),\n",
       " ('^', 55276),\n",
       " ('n', 49940),\n",
       " ('h', 42837),\n",
       " ('r', 39035),\n",
       " ('l', 36762),\n",
       " ('o', 36097),\n",
       " ('d', 34400),\n",
       " ('.', 29935),\n",
       " ('c', 28517),\n",
       " ('u', 27674),\n",
       " ('p', 18168),\n",
       " ('b', 17963),\n",
       " ('g', 17747),\n",
       " ('m', 14753),\n",
       " ('w', 14353),\n",
       " ('f', 11380),\n",
       " ('(', 8198),\n",
       " ('k', 8167),\n",
       " (')', 8032),\n",
       " ('y', 6654),\n",
       " ('v', 6087),\n",
       " ('?', 5693),\n",
       " ('0', 3852),\n",
       " ('z', 3610),\n",
       " ('!', 3374),\n",
       " ('-', 2320),\n",
       " (',', 2308),\n",
       " (':', 2073),\n",
       " ('ü', 1623),\n",
       " ('ä', 1436),\n",
       " ('j', 940),\n",
       " ('ö', 901),\n",
       " ('/', 804),\n",
       " ('ß', 690),\n",
       " ('x', 575),\n",
       " ('q', 418),\n",
       " (\"'\", 221),\n",
       " ('\"', 156),\n",
       " ('%', 57),\n",
       " ('–', 47),\n",
       " ('é', 37),\n",
       " ('á', 33),\n",
       " ('í', 27),\n",
       " ('°', 24),\n",
       " ('’', 19),\n",
       " (';', 16),\n",
       " ('*', 15),\n",
       " ('è', 15),\n",
       " ('“', 10),\n",
       " (']', 10),\n",
       " ('…', 9),\n",
       " ('ã', 9),\n",
       " ('[', 9),\n",
       " ('‘', 8),\n",
       " ('ï', 8),\n",
       " ('å', 7),\n",
       " ('ó', 6),\n",
       " ('α', 5),\n",
       " ('ă', 5),\n",
       " ('š', 5),\n",
       " ('„', 4),\n",
       " ('ñ', 4),\n",
       " ('π', 4),\n",
       " ('ň', 4),\n",
       " ('о', 4),\n",
       " ('д', 4),\n",
       " ('•', 4),\n",
       " ('ł', 4),\n",
       " ('ο', 3),\n",
       " ('τ', 3),\n",
       " ('´', 2),\n",
       " ('<', 2),\n",
       " ('ç', 2),\n",
       " ('ą', 2),\n",
       " ('ή', 2),\n",
       " ('χ', 2),\n",
       " ('ά', 2),\n",
       " ('κ', 2),\n",
       " ('ρ', 2),\n",
       " ('έ', 2),\n",
       " ('υ', 2),\n",
       " ('ž', 2),\n",
       " ('ǎ', 2),\n",
       " ('¡', 2),\n",
       " ('ý', 2),\n",
       " ('ń', 2),\n",
       " ('č', 2),\n",
       " ('и', 2),\n",
       " ('л', 2),\n",
       " ('ш', 2),\n",
       " ('е', 2),\n",
       " ('р', 2),\n",
       " ('б', 2),\n",
       " ('û', 2),\n",
       " ('à', 2),\n",
       " ('ò', 2),\n",
       " ('ì', 1),\n",
       " ('ę', 1),\n",
       " ('æ', 1),\n",
       " ('ν', 1),\n",
       " ('ī', 1),\n",
       " ('\\xad', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(sorted(tokenizer.word_counts.items(), key=lambda d: d[1])))\n",
    "sum(1 for w, count in tokenizer.word_counts.items() if count > 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:57.304600Z",
     "start_time": "2018-05-12T11:14:57.299666Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len_input = df.input_length.max()\n",
    "max_len_target = df.target_length.max()\n",
    "nr_tokens = len(tokenizer.word_index) + 1  # add 0 padding not in word_index contained\n",
    "\n",
    "# one hot encoded y_t_output wouldn't fit into memory any longer\n",
    "# so need to train/validate on batches generated on the fly\n",
    "def create_batch_generator(samples_ids):\n",
    "    \n",
    "    def batch_generator():\n",
    "        nr_batches = np.ceil(len(samples_ids) / BATCH_SIZE)\n",
    "        while True:\n",
    "            shuffled_ids = np.random.permutation(samples_ids)\n",
    "            batch_splits = np.array_split(shuffled_ids, nr_batches)\n",
    "            for batch_ids in batch_splits:\n",
    "                batch_X = pad_sequences(df.iloc[batch_ids].input_sequences, padding='post', maxlen=max_len_input)\n",
    "                batch_y = pad_sequences(df.iloc[batch_ids].target_sequences, padding='post', maxlen=max_len_target)\n",
    "                batch_y_t_output = keras.utils.to_categorical(batch_y[:,1:], num_classes=len(tokenizer.word_index)+1)\n",
    "                batch_x_t_input = batch_y[:,:-1]\n",
    "                yield ([batch_X, batch_x_t_input], batch_y_t_output)\n",
    "    \n",
    "    return batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:57.311431Z",
     "start_time": "2018-05-12T11:14:57.305825Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(df.shape[0]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:57.319036Z",
     "start_time": "2018-05-12T11:14:57.312752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(24874, 2764)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_tokens\n",
    "len(tokenizer.word_index)\n",
    "len(train_ids), len(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:58.669560Z",
     "start_time": "2018-05-12T11:14:57.320278Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_gru = L.Bidirectional(\n",
    "    L.GRU(LATENT_DIM // 2, dropout=DROPOUT, return_state=True, name='encoder_gru'),\n",
    "    name='encoder_bidirectional'\n",
    ")\n",
    "decoder_gru = L.GRU(LATENT_DIM, dropout=DROPOUT, return_sequences=True, return_state=True, name='decoder_gru')\n",
    "decoder_dense = L.Dense(nr_tokens, activation='softmax', name='decoder_outputs')\n",
    "\n",
    "shared_embedding = L.Embedding(nr_tokens, EMBEDDING_DIM, mask_zero=True, name='shared_embedding')\n",
    "\n",
    "encoder_inputs = L.Input(shape=(max_len_input, ), dtype='int32', name='encoder_inputs')\n",
    "encoder_embeddings = shared_embedding(encoder_inputs)\n",
    "_, encoder_state_1, encoder_state_2 = encoder_gru(encoder_embeddings)\n",
    "encoder_states = L.concatenate([encoder_state_1, encoder_state_2])\n",
    "\n",
    "decoder_inputs = L.Input(shape=(max_len_target-1, ), dtype='int32', name='decoder_inputs')\n",
    "decoder_mask = L.Masking(mask_value=0)(decoder_inputs)\n",
    "decoder_embeddings_inputs = shared_embedding(decoder_mask)\n",
    "decoder_embeddings_outputs, _ = decoder_gru(decoder_embeddings_inputs, initial_state=encoder_states) \n",
    "decoder_outputs = decoder_dense(decoder_embeddings_outputs)\n",
    "\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "\n",
    "inference_encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "inference_decoder_state_inputs = L.Input(shape=(LATENT_DIM, ), dtype='float32', name='inference_decoder_state_inputs')\n",
    "inference_decoder_embeddings_outputs, inference_decoder_states = decoder_gru(\n",
    "    decoder_embeddings_inputs, initial_state=inference_decoder_state_inputs\n",
    ")\n",
    "inference_decoder_outputs = decoder_dense(inference_decoder_embeddings_outputs)\n",
    "\n",
    "inference_decoder_model = Model(\n",
    "    [decoder_inputs, inference_decoder_state_inputs], \n",
    "    [inference_decoder_outputs, inference_decoder_states]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:58.676706Z",
     "start_time": "2018-05-12T11:14:58.671005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, 33)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 33)           0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_inputs (InputLayer)     (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    multiple             7104        encoder_inputs[0][0]             \n",
      "                                                                 masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bidirectional (Bidirect [(None, 512), (None, 493056      shared_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           encoder_bidirectional[0][1]      \n",
      "                                                                 encoder_bidirectional[0][2]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 33, 512), (N 886272      shared_embedding[1][0]           \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 33, 111)      56943       decoder_gru[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,443,375\n",
      "Trainable params: 1,443,375\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, 33)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 33)           0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    multiple             7104        masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "inference_decoder_state_inputs  (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 33, 512), (N 886272      shared_embedding[1][0]           \n",
      "                                                                 inference_decoder_state_inputs[0]\n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 33, 111)      56943       decoder_gru[1][0]                \n",
      "==================================================================================================\n",
      "Total params: 950,319\n",
      "Trainable params: 950,319\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "inference_decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:14:58.715501Z",
     "start_time": "2018-05-12T11:14:58.678663Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(clipnorm=1.), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:19:27.246062Z",
     "start_time": "2018-05-12T11:14:58.716888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "195/195 [==============================] - 16s 80ms/step - loss: 2.2041 - val_loss: 1.3468\n",
      "Epoch 2/20\n",
      "195/195 [==============================] - 13s 69ms/step - loss: 1.1757 - val_loss: 0.9938\n",
      "Epoch 3/20\n",
      "195/195 [==============================] - 13s 65ms/step - loss: 0.9389 - val_loss: 0.8588\n",
      "Epoch 4/20\n",
      "195/195 [==============================] - 13s 65ms/step - loss: 0.8076 - val_loss: 0.7945\n",
      "Epoch 5/20\n",
      "195/195 [==============================] - 14s 70ms/step - loss: 0.7247 - val_loss: 0.6938\n",
      "Epoch 6/20\n",
      "195/195 [==============================] - 13s 68ms/step - loss: 0.6636 - val_loss: 0.6648\n",
      "Epoch 7/20\n",
      "195/195 [==============================] - 13s 67ms/step - loss: 0.6160 - val_loss: 0.6361\n",
      "Epoch 8/20\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.5785 - val_loss: 0.6210\n",
      "Epoch 9/20\n",
      "195/195 [==============================] - 14s 72ms/step - loss: 0.5459 - val_loss: 0.5890\n",
      "Epoch 10/20\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.5179 - val_loss: 0.5744\n",
      "Epoch 11/20\n",
      "195/195 [==============================] - 13s 68ms/step - loss: 0.4916 - val_loss: 0.5748\n",
      "Epoch 12/20\n",
      "195/195 [==============================] - 13s 69ms/step - loss: 0.4703 - val_loss: 0.5602\n",
      "Epoch 13/20\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.4494 - val_loss: 0.5718\n",
      "Epoch 14/20\n",
      "195/195 [==============================] - 13s 65ms/step - loss: 0.4306 - val_loss: 0.5490\n",
      "Epoch 15/20\n",
      "195/195 [==============================] - 13s 67ms/step - loss: 0.4143 - val_loss: 0.5355\n",
      "Epoch 16/20\n",
      "195/195 [==============================] - 13s 67ms/step - loss: 0.3993 - val_loss: 0.5389\n",
      "Epoch 17/20\n",
      "195/195 [==============================] - 13s 67ms/step - loss: 0.3844 - val_loss: 0.5366\n",
      "Epoch 18/20\n",
      "195/195 [==============================] - 13s 69ms/step - loss: 0.3704 - val_loss: 0.5514\n",
      "Epoch 19/20\n",
      "195/195 [==============================] - 14s 71ms/step - loss: 0.3569 - val_loss: 0.5531\n",
      "Epoch 20/20\n",
      "195/195 [==============================] - 14s 70ms/step - loss: 0.3465 - val_loss: 0.5481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff4fe224ba8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = create_batch_generator(train_ids)\n",
    "val_generator = create_batch_generator(val_ids)\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(len(train_ids) / BATCH_SIZE),\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=np.ceil(len(val_ids) / BATCH_SIZE),\n",
    ")\n",
    "#model.fit([X, x_t_input], y_t_output, validation_split=0.1, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:19:27.253624Z",
     "start_time": "2018-05-12T11:19:27.247741Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = inference_encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, max_len_target-1))\n",
    "    target_seq[0, 0] = tokenizer.word_index[START]\n",
    "    \n",
    "    tokens = {idx: token for (token, idx) in tokenizer.word_index.items()}\n",
    "    \n",
    "    decoded_sequence = ''\n",
    "    for i in range(max_len_target):\n",
    "        output_tokens, output_states = inference_decoder_model.predict(\n",
    "            [target_seq, states_value]\n",
    "        )\n",
    "        \n",
    "        # greedy search\n",
    "        sampled_token_idx = np.argmax(output_tokens[0, 0, :])\n",
    "        sampled_token = tokens.get(sampled_token_idx, '.')\n",
    "        if sampled_token == END:\n",
    "            break\n",
    "        decoded_sequence += sampled_token\n",
    "            \n",
    "        target_seq[0, 0] = sampled_token_idx\n",
    "        states_value = output_states\n",
    "    \n",
    "    return decoded_sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:19:27.270899Z",
     "start_time": "2018-05-12T11:19:27.255472Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    return decode_sequence(keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences([preprocess(sentence)]),\n",
    "        padding='post',\n",
    "        maxlen=max_len_input,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:19:30.071718Z",
     "start_time": "2018-05-12T11:19:27.272555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello.' --> 'hallo von allein.'\n",
      "'you are welcome.' --> 'sie haben verständlich.'\n",
      "'how do you do?' --> 'wie können wir das tun?'\n",
      "'i hate mondays.' --> 'ich habe dafür gestimmt.'\n",
      "'i am a programmer.' --> 'ich bin falsch.'\n",
      "'data is the new oil.' --> 'heute ist der stand.'\n",
      "'it could be worse.' --> 'das war das zeit begrüßen.'\n",
      "'i am on top of it.' --> 'ich bin nicht darüber beispiel.'\n",
      "'n° uno' --> 'neine'\n",
      "'awesome!' --> 'absolut gemeint!'\n",
      "'put your feet up!' --> 'wie sind das ein anfank!'\n",
      "'from the start till the end!' --> 'wir müssen das aber abgehen!'\n",
      "'from dusk till dawn.' --> 'aus dazu gehört geran werden.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:19:33.685453Z",
     "start_time": "2018-05-12T11:19:30.072969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original '^relating to wednesday:', got 'zum mittwoch:', exp: 'zum mittwoch:\\n'\n",
      "Original '^that was the decision.', got 'das war die frage.', exp: 'das war der beschluß.\\n'\n",
      "Original '^thank you very much.', got 'vielen dank.', exp: 'vielen dank.\\n'\n",
      "Original '^that did not happen.', got 'das ist nicht geschehen.', exp: 'dazu kam es nicht.\\n'\n",
      "Original '^the debate is closed.', got 'die aussprache ist geschlossen.', exp: 'die aussprache ist geschlossen.\\n'\n",
      "Original '^the debate is closed.', got 'die aussprache ist geschlossen.', exp: 'die aussprache ist geschlossen.\\n'\n",
      "Original '^the debate is closed.', got 'die aussprache ist geschlossen.', exp: 'die aussprache ist geschlossen.\\n'\n",
      "Original '^what is the result?', got 'was ist das problem?', exp: 'was sind die folgen?\\n'\n",
      "Original '^the debate is closed.', got 'die aussprache ist geschlossen.', exp: 'die aussprache ist geschlossen.\\n'\n",
      "Original '^thank you very much.', got 'vielen dank.', exp: 'vielen dank!\\n'\n",
      "Original '^with what aim?', got 'wie wird das alles abgelehnt?', exp: 'zu welchem zweck?\\n'\n",
      "Original '^we all agree on this.', got 'das wissen wir alle wissen.', exp: 'das sehen wir alle ein.\\n'\n",
      "Original '^why?', got 'warum?', exp: 'wieso?\\n'\n",
      "Original '^no.', got 'nein.', exp: 'nein.\\n'\n",
      "Original '^i do not believe so.', got 'ich glaube nicht.', exp: 'ich glaube nicht daran.\\n'\n",
      "Original '^just like europol.', got 'lassen sie uns der erste meinen.', exp: 'genau wie europol.\\n'\n",
      "Original '^vote', got 'abstimmungen', exp: 'abstimmungen\\n'\n",
      "Original '^koch report (a0-0/0)', got 'bericht berès (a0-0/0)', exp: 'bericht koch (a0-0/0)\\n'\n",
      "Original '^koch report (a0-0/0)', got 'bericht berès (a0-0/0)', exp: 'bericht koch (a0-0/0)\\n'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {preprocess(en)!r}, got {predict(en)!r}, exp: {de[1:]!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:19:37.544420Z",
     "start_time": "2018-05-12T11:19:33.686763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original '^it is very important.', got 'das ist wichtig.', exp: 'sie ist sehr wichtig.\\n'\n",
      "Original '^the debate is closed.', got 'die aussprache ist geschlossen.', exp: 'die aussprache ist geschlossen.\\n'\n",
      "Original '^the debate is closed', got 'die aussprache ist geschlossen.', exp: 'die aussprache ist geschlossen.\\n'\n",
      "Original '^thank you, mr camre.', got 'vielen dank, herr marín.', exp: 'vielen dank, lieber kollege.\\n'\n",
      "Original \"^signed: joão correa' .\", got 'das gegenteil ist der fall.', exp: 'gezeichnet: joão correa. \"\\n'\n",
      "Original '^the debate is closed.', got 'die aussprache ist geschlossen.', exp: 'die aussprache ist geschlossen.\\n'\n",
      "Original '^we need priorities.', got 'wir brauchen reformen.', exp: 'wir brauchen prioritäten.\\n'\n",
      "Original '^and so we shall be.', got 'und das wollen wir alle tun.', exp: 'das werden wir auch.\\n'\n",
      "Original '^- report: thomsen', got '- bericht vergen', exp: '- bericht: thomsen\\n'\n",
      "Original '^definitely not.', got 'das gegenteil ist der fall.', exp: 'ganz gewiss nicht.\\n'\n",
      "Original '^the debate is closed.', got 'die aussprache ist geschlossen.', exp: 'die aussprache ist geschlossen.\\n'\n",
      "Original '^part ii', got 'teil ii', exp: 'teil ii\\n'\n",
      "Original '^(applause)', got '(beifall)', exp: '(beifall)\\n'\n",
      "Original '^i regret this.', got 'das bedauere ich.', exp: 'ich bedauere dies.\\n'\n",
      "Original '^(applause)', got '(beifall)', exp: '(beifall)\\n'\n",
      "Original '^the debate is closed.', got 'die aussprache ist geschlossen.', exp: 'die aussprache ist geschlossen.\\n'\n",
      "Original '^petitions: see minutes', got 'petitionen: siehe protokoll', exp: 'petitionen: siehe protokoll\\n'\n",
      "Original '^the item is closed.', got 'die aussprache ist geschlossen.', exp: 'die aussprache ist geschlossen.\\n'\n",
      "Original '^that is important.', got 'das ist wichtig.', exp: 'das ist wichtig.\\n'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {preprocess(en)!r}, got {predict(en)!r}, exp: {de[1:]!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:21:11.197601Z",
     "start_time": "2018-05-12T11:19:37.545772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c99217e04e4b0881000de454437067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average bleu score: 0.3790106443397856\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "try:\n",
    "    from spacy.lang.de import German\n",
    "except ModuleNotFoundError:\n",
    "    spacy.cli.download('de')\n",
    "    from spacy.lang.de import German\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "parser = German()\n",
    "chencherry = SmoothingFunction()  # to handle short sequences, see also http://www.nltk.org/_modules/nltk/translate/bleu_score.html#SmoothingFunction.method3\n",
    "\n",
    "def remove_spaces_and_puncts(tokens):\n",
    "     return [token.orth_ for token in tokens if not (token.is_space or token.is_punct)]  \n",
    "\n",
    "bleu_scores = np.zeros(TEST_SIZE)\n",
    "nist_scores = np.zeros(TEST_SIZE)\n",
    "\n",
    "for i in tqdm(range(TEST_SIZE)):\n",
    "    pred_tokens = remove_spaces_and_puncts(parser(predict(df.iloc[i].input_texts)))\n",
    "    ref_tokens = remove_spaces_and_puncts(parser(df.iloc[i].target_texts[1:]))\n",
    "    bleu_scores[i] = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=chencherry.method3)\n",
    "    \n",
    "print(\"Average bleu score:\", bleu_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T11:21:11.200575Z",
     "start_time": "2018-05-12T11:21:11.198810Z"
    }
   },
   "outputs": [],
   "source": [
    "## Conclusion\n",
    "\n",
    "# It doesn't work perfect, but fine enough to show that seq2seq works in some way. I wouldn't be surprised if the mean average error is better than average human bias for calculating without any tools.\n",
    "# For improvements and further discussions I'll move to a real problem (translating) and main steps will be:\n",
    "# * Bytepairencoding/Word embeddings\n",
    "# * Beam Search\n",
    "# * Attention models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 301,
   "position": {
    "height": "40px",
    "left": "987px",
    "right": "23px",
    "top": "124px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
