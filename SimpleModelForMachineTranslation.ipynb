{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple seq2seq model in keras that translates english <-> german"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step I take the model used for the [toy problem of adding/subtracting numbers](SimpleModelForAddingAndSubstraction.ipynb) and train it with english/german data for machine translation.\n",
    "\n",
    "As training set I use the [European Parliament Proceedings Parallel Corpus 1996-2011](http://statmt.org/europarl/) German-English corpus. I'm preprocessing the dataset (lowercase, replace numbers with 0) as I will later train it with a pretrained bytepair embedding for such a preprocessed dataset and so I get comparable results. For an application it wouldn't be much of a practical problem to train bytepair embeddings if needed.\n",
    "\n",
    "I'll train it now on medium length sentences (up to 50 characters in english, 65 characters in german), so we can see problems from handling longer sentences (losing focus and similiar). In the end I'll show results for a mini test set (unknown, short own written english->german samples) and for sentences from the training and validation set. To have a reasonable automatic scoring, I'll use BLEU scoring (with smoothing) on a part of the validation set.\n",
    "\n",
    "The code for the seq2seq part is close to be the same as for the toy problem. For the batch trainings, I had to write a batch generator (as the one-hot-encoded output sequence wouldn't fit into memory when using bytepairs). Also, I added a dropout layer after the decoder GRU to reduce overfitting. I also tweaked a bit hyperparameters (lower learning rate, reduced batch size again to reduce memory consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:19:09.123851Z",
     "start_time": "2018-05-13T12:19:07.790818Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# technical detail so that an instance (maybe running in a different window)\n",
    "# doesn't take all the GPU memory resulting in some strange error messages\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:19:09.405572Z",
     "start_time": "2018-05-13T12:19:09.125127Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import keras\n",
    "import keras.layers as L\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Fixing random state ensure reproducible results\n",
    "RANDOM_STATE=42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.set_random_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:19:09.414440Z",
     "start_time": "2018-05-13T12:19:09.409225Z"
    }
   },
   "outputs": [],
   "source": [
    "START = '^'\n",
    "END = '\\n'\n",
    "\n",
    "MAX_INPUT_LENGTH = 50\n",
    "MAX_TARGET_LENGTH = 65\n",
    "LATENT_DIM = 512\n",
    "EMBEDDING_DIM = 64\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "DROPOUT = 0.5\n",
    "TEST_SIZE = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:19:09.425703Z",
     "start_time": "2018-05-13T12:19:09.417411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n"
     ]
    }
   ],
   "source": [
    "def download_file(fname, url):\n",
    "    print(f\"Downloading {fname} from {url} ...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    total_size = int(response.headers.get('content-length', 0)); \n",
    "    block_size = 1024\n",
    "\n",
    "    download = tqdm(\n",
    "        response.iter_content(block_size),\n",
    "        total=math.ceil(total_size // block_size),\n",
    "        unit='KB',\n",
    "        unit_scale=True\n",
    "    )\n",
    "    with open(f\"{fname}\", \"wb\") as handle:\n",
    "        for data in download:\n",
    "            handle.write(data)\n",
    "\n",
    "PATH = 'data'\n",
    "FILES = {\n",
    "    'de-en.tgz': 'http://statmt.org/europarl/v7/de-en.tgz',  # incredible: really only http, not https :-o\n",
    "}\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "for name, url in FILES.items():\n",
    "    fname = os.path.join(PATH, name)\n",
    "    exists = os.path.exists(fname)\n",
    "    size = os.path.getsize(fname) if exists else -1\n",
    "    if exists and size > 0:\n",
    "        print(f'{name} already downloaded ({size / 2**20:3.1f} MB)')\n",
    "        continue\n",
    "    download_file(fname, url)\n",
    "    if (fname.endswith(\".tgz\")):\n",
    "        tar = tarfile.open(fname, \"r:gz\")\n",
    "        tar.extractall(path=PATH)\n",
    "        tar.close()\n",
    "        print(f'Extracted {fname} ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:20:00.912945Z",
     "start_time": "2018-05-13T12:19:09.427396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Following https://github.com/bheinzerling/bpemb/blob/master/preprocess_text.sh\n",
    "# (ignoring urls as there shouldn't be any in parliament discussions)\n",
    "def preprocess(line):\n",
    "    line = re.sub(r'\\d+', '0', line)  # replace all numbers by '0'\n",
    "    line = re.sub(r'\\s+', ' ', line)  # strip whitespaces as a ' '\n",
    "    return line.lower().strip()\n",
    "\n",
    "def read_corpus_lines(language):\n",
    "    return [preprocess(line) for line in open(f'{PATH}/europarl-v7.de-en.{language}', 'r').readlines()]\n",
    "    \n",
    "pd.set_option('max_colwidth', 60)\n",
    "df = pd.DataFrame(data={\n",
    "    'input_texts': read_corpus_lines('en'),\n",
    "    'target_texts': read_corpus_lines('de'), \n",
    "})\n",
    "df.input_texts = START + df.input_texts + END\n",
    "df.target_texts = START + df.target_texts + END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:20:01.856856Z",
     "start_time": "2018-05-13T12:20:00.915075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920209"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>^resumption of the session\\n</td>\n",
       "      <td>^wiederaufnahme der sitzungsperiode\\n</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>^i declare resumed the session of the european parliamen...</td>\n",
       "      <td>^ich erkläre die am freitag, dem 0. dezember unterbroche...</td>\n",
       "      <td>205</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>^although, as you will have seen, the dreaded 'millenniu...</td>\n",
       "      <td>^wie sie feststellen konnten, ist der gefürchtete \"mille...</td>\n",
       "      <td>193</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>^you have requested a debate on this subject in the cour...</td>\n",
       "      <td>^im parlament besteht der wunsch nach einer aussprache i...</td>\n",
       "      <td>107</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>^in the meantime, i should like to observe a minute' s s...</td>\n",
       "      <td>^heute möchte ich sie bitten - das ist auch der wunsch e...</td>\n",
       "      <td>234</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input_texts  \\\n",
       "0                                 ^resumption of the session\\n   \n",
       "1  ^i declare resumed the session of the european parliamen...   \n",
       "2  ^although, as you will have seen, the dreaded 'millenniu...   \n",
       "3  ^you have requested a debate on this subject in the cour...   \n",
       "4  ^in the meantime, i should like to observe a minute' s s...   \n",
       "\n",
       "                                                  target_texts  input_length  \\\n",
       "0                        ^wiederaufnahme der sitzungsperiode\\n            27   \n",
       "1  ^ich erkläre die am freitag, dem 0. dezember unterbroche...           205   \n",
       "2  ^wie sie feststellen konnten, ist der gefürchtete \"mille...           193   \n",
       "3  ^im parlament besteht der wunsch nach einer aussprache i...           107   \n",
       "4  ^heute möchte ich sie bitten - das ist auch der wunsch e...           234   \n",
       "\n",
       "   target_length  \n",
       "0             36  \n",
       "1            219  \n",
       "2            187  \n",
       "3            112  \n",
       "4            219  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)\n",
    "df['input_length'] = df.input_texts.apply(len)\n",
    "df['target_length'] = df.target_texts.apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only use short translations right now\n",
    "\n",
    "So, first I plot sentence length on a logarithmic scale, \n",
    "then I only choose short input texts (and a bit longer target texts as german is more verbose than english)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:20:02.317679Z",
     "start_time": "2018-05-13T12:20:01.858942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = df.input_length\n",
    "logbins = np.logspace(1,5,20)\n",
    "plt.hist(x, bins=logbins)\n",
    "plt.xscale('log')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:20:02.416258Z",
     "start_time": "2018-05-13T12:20:02.318953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167211"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_empty = (df.input_length > 3) & (df.target_length > 3)  # there are empty phrases like '\\n' --> 'Frau Präsidentin\\n'\n",
    "short_inputs = (df.input_length < MAX_INPUT_LENGTH + 2) & (df.target_length < MAX_TARGET_LENGTH + 2)  # add start + end symbols\n",
    "sum(non_empty & short_inputs)\n",
    "df = df[non_empty & short_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:20:02.426278Z",
     "start_time": "2018-05-13T12:20:02.417859Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = pd.concat([df.input_texts, df.target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:20:08.370354Z",
     "start_time": "2018-05-13T12:20:02.427939Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=100, filters=None, char_level=True, oov_token='~')\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "df['input_sequences'] = tokenizer.texts_to_sequences(df.input_texts)\n",
    "df['target_sequences'] = tokenizer.texts_to_sequences(df.target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:20:08.382925Z",
     "start_time": "2018-05-13T12:20:08.372204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 1656706),\n",
       " ('e', 1393381),\n",
       " ('i', 830566),\n",
       " ('t', 817881),\n",
       " ('n', 783850),\n",
       " ('s', 714344),\n",
       " ('a', 653289),\n",
       " ('r', 643182),\n",
       " ('o', 490686),\n",
       " ('h', 486235),\n",
       " ('d', 382862),\n",
       " ('l', 353517),\n",
       " ('\\n', 334422),\n",
       " ('^', 334422),\n",
       " ('u', 322169),\n",
       " ('c', 287330),\n",
       " ('m', 270604),\n",
       " ('.', 268267),\n",
       " ('g', 229490),\n",
       " ('w', 193711),\n",
       " ('b', 165185),\n",
       " ('p', 158586),\n",
       " ('f', 158468),\n",
       " ('k', 110148),\n",
       " ('v', 94920),\n",
       " ('y', 79537),\n",
       " ('z', 57233),\n",
       " (',', 50769),\n",
       " ('0', 48071),\n",
       " ('ü', 35607),\n",
       " ('ä', 30461),\n",
       " ('(', 28874),\n",
       " (')', 27913),\n",
       " ('?', 26063),\n",
       " ('-', 15932),\n",
       " ('j', 15901),\n",
       " ('ö', 13841),\n",
       " (':', 12059),\n",
       " ('!', 12003),\n",
       " ('ß', 11001),\n",
       " ('x', 9319),\n",
       " ('q', 8628),\n",
       " ('/', 6557),\n",
       " (\"'\", 3757),\n",
       " ('\"', 1769),\n",
       " ('%', 1066),\n",
       " ('–', 897),\n",
       " ('’', 406),\n",
       " (';', 388),\n",
       " ('é', 387),\n",
       " ('á', 302),\n",
       " ('í', 292),\n",
       " ('ó', 216),\n",
       " ('‘', 129),\n",
       " ('“', 127),\n",
       " ('„', 120),\n",
       " ('è', 107),\n",
       " (']', 57),\n",
       " ('…', 56),\n",
       " ('à', 56),\n",
       " ('[', 56),\n",
       " ('š', 43),\n",
       " ('æ', 39),\n",
       " ('ł', 38),\n",
       " ('ú', 33),\n",
       " ('ç', 32),\n",
       " ('ñ', 30),\n",
       " ('č', 29),\n",
       " ('ï', 29),\n",
       " ('°', 26),\n",
       " ('τ', 26),\n",
       " ('ø', 25),\n",
       " ('μ', 22),\n",
       " ('+', 22),\n",
       " ('ò', 22),\n",
       " ('ô', 21),\n",
       " ('ã', 21),\n",
       " ('ê', 21),\n",
       " ('ş', 19),\n",
       " ('ő', 18),\n",
       " ('ă', 18),\n",
       " ('&', 17),\n",
       " ('”', 17),\n",
       " ('\\xad', 17),\n",
       " ('•', 16),\n",
       " ('*', 15),\n",
       " ('σ', 14),\n",
       " ('α', 14),\n",
       " ('ρ', 14),\n",
       " ('ń', 12),\n",
       " ('η', 11),\n",
       " ('å', 11),\n",
       " ('ε', 10),\n",
       " ('ė', 10),\n",
       " ('ο', 10),\n",
       " ('ς', 9),\n",
       " ('ι', 9),\n",
       " ('ą', 9),\n",
       " ('π', 9),\n",
       " ('ţ', 9),\n",
       " ('ί', 8),\n",
       " ('ν', 8),\n",
       " ('ý', 8),\n",
       " ('¡', 7),\n",
       " ('ę', 7),\n",
       " ('´', 7),\n",
       " ('#', 6),\n",
       " ('υ', 6),\n",
       " ('â', 6),\n",
       " ('β', 6),\n",
       " ('ž', 6),\n",
       " ('·', 6),\n",
       " ('δ', 5),\n",
       " ('ή', 5),\n",
       " ('ć', 5),\n",
       " ('ì', 5),\n",
       " ('‟', 4),\n",
       " ('λ', 4),\n",
       " ('‚', 4),\n",
       " ('ň', 4),\n",
       " ('ë', 4),\n",
       " ('о', 4),\n",
       " ('д', 4),\n",
       " ('<', 3),\n",
       " ('γ', 3),\n",
       " ('ś', 3),\n",
       " ('ż', 3),\n",
       " ('χ', 3),\n",
       " ('κ', 3),\n",
       " ('î', 3),\n",
       " ('φ', 2),\n",
       " ('ό', 2),\n",
       " ('ř', 2),\n",
       " ('ά', 2),\n",
       " ('έ', 2),\n",
       " ('\\u200b', 2),\n",
       " ('ū', 2),\n",
       " ('ǎ', 2),\n",
       " ('œ', 2),\n",
       " ('¹', 2),\n",
       " ('и', 2),\n",
       " ('л', 2),\n",
       " ('ш', 2),\n",
       " ('е', 2),\n",
       " ('р', 2),\n",
       " ('б', 2),\n",
       " ('ğ', 2),\n",
       " ('ı', 2),\n",
       " ('ź', 2),\n",
       " ('û', 2),\n",
       " ('ξ', 1),\n",
       " ('ύ', 1),\n",
       " ('ψ', 1),\n",
       " ('ζ', 1),\n",
       " ('ŏ', 1),\n",
       " ('`', 1),\n",
       " ('º', 1),\n",
       " ('>', 1),\n",
       " ('£', 1),\n",
       " ('ī', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(sorted(tokenizer.word_counts.items(), key=lambda d: d[1])))\n",
    "sum(1 for w, count in tokenizer.word_counts.items() if count > 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:20:08.390997Z",
     "start_time": "2018-05-13T12:20:08.384640Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len_input = df.input_length.max()\n",
    "max_len_target = df.target_length.max()\n",
    "nr_tokens = len(tokenizer.word_index) + 1  # add 0 padding not in word_index contained\n",
    "\n",
    "# one hot encoded y_t_output wouldn't fit into memory any longer\n",
    "# so need to train/validate on batches generated on the fly\n",
    "def create_batch_generator(samples_ids):\n",
    "    \n",
    "    def batch_generator():\n",
    "        nr_batches = np.ceil(len(samples_ids) / BATCH_SIZE)\n",
    "        while True:\n",
    "            shuffled_ids = np.random.permutation(samples_ids)\n",
    "            batch_splits = np.array_split(shuffled_ids, nr_batches)\n",
    "            for batch_ids in batch_splits:\n",
    "                batch_X = pad_sequences(df.iloc[batch_ids].input_sequences, padding='post', maxlen=max_len_input)\n",
    "                batch_y = pad_sequences(df.iloc[batch_ids].target_sequences, padding='post', maxlen=max_len_target)\n",
    "                batch_y_t_output = keras.utils.to_categorical(batch_y[:,1:], num_classes=len(tokenizer.word_index)+1)\n",
    "                batch_x_t_input = batch_y[:,:-1]\n",
    "                yield ([batch_X, batch_x_t_input], batch_y_t_output)\n",
    "    \n",
    "    return batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:20:08.399290Z",
     "start_time": "2018-05-13T12:20:08.392294Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(df.shape[0]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:20:08.407563Z",
     "start_time": "2018-05-13T12:20:08.400719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(150489, 16722)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_tokens\n",
    "len(tokenizer.word_index)\n",
    "len(train_ids), len(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:20:09.568534Z",
     "start_time": "2018-05-13T12:20:08.408950Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_gru = L.Bidirectional(\n",
    "    L.GRU(LATENT_DIM // 2, dropout=DROPOUT, return_state=True, name='encoder_gru'),\n",
    "    name='encoder_bidirectional'\n",
    ")\n",
    "decoder_gru = L.GRU(LATENT_DIM, dropout=DROPOUT, return_sequences=True, return_state=True, name='decoder_gru')\n",
    "decoder_dense = L.Dense(nr_tokens, activation='softmax', name='decoder_outputs')\n",
    "\n",
    "shared_embedding = L.Embedding(nr_tokens, EMBEDDING_DIM, mask_zero=True, name='shared_embedding')\n",
    "\n",
    "encoder_inputs = L.Input(shape=(max_len_input, ), dtype='int32', name='encoder_inputs')\n",
    "encoder_embeddings = shared_embedding(encoder_inputs)\n",
    "_, encoder_state_1, encoder_state_2 = encoder_gru(encoder_embeddings)\n",
    "encoder_states = L.concatenate([encoder_state_1, encoder_state_2])\n",
    "\n",
    "decoder_inputs = L.Input(shape=(max_len_target-1, ), dtype='int32', name='decoder_inputs')\n",
    "decoder_mask = L.Masking(mask_value=0)(decoder_inputs)\n",
    "decoder_embeddings_inputs = shared_embedding(decoder_mask)\n",
    "decoder_embeddings_outputs, _ = decoder_gru(decoder_embeddings_inputs, initial_state=encoder_states) \n",
    "decoder_outputs = decoder_dense(decoder_embeddings_outputs)\n",
    "\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "\n",
    "inference_encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "inference_decoder_state_inputs = L.Input(shape=(LATENT_DIM, ), dtype='float32', name='inference_decoder_state_inputs')\n",
    "inference_decoder_embeddings_outputs, inference_decoder_states = decoder_gru(\n",
    "    decoder_embeddings_inputs, initial_state=inference_decoder_state_inputs\n",
    ")\n",
    "inference_decoder_outputs = decoder_dense(inference_decoder_embeddings_outputs)\n",
    "\n",
    "inference_decoder_model = Model(\n",
    "    [decoder_inputs, inference_decoder_state_inputs], \n",
    "    [inference_decoder_outputs, inference_decoder_states]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:20:09.575020Z",
     "start_time": "2018-05-13T12:20:09.569792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, 65)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 65)           0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_inputs (InputLayer)     (None, 51)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    multiple             10368       encoder_inputs[0][0]             \n",
      "                                                                 masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bidirectional (Bidirect [(None, 512), (None, 493056      shared_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           encoder_bidirectional[0][1]      \n",
      "                                                                 encoder_bidirectional[0][2]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 65, 512), (N 886272      shared_embedding[1][0]           \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 65, 162)      83106       decoder_gru[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,472,802\n",
      "Trainable params: 1,472,802\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, 65)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 65)           0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    multiple             10368       masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "inference_decoder_state_inputs  (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 65, 512), (N 886272      shared_embedding[1][0]           \n",
      "                                                                 inference_decoder_state_inputs[0]\n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 65, 162)      83106       decoder_gru[1][0]                \n",
      "==================================================================================================\n",
      "Total params: 979,746\n",
      "Trainable params: 979,746\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "inference_decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T12:20:09.603813Z",
     "start_time": "2018-05-13T12:20:09.577305Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(clipnorm=1., clipvalue=.5), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T14:09:34.363847Z",
     "start_time": "2018-05-13T12:20:09.604999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2352/2352 [==============================] - 338s 144ms/step - loss: 1.3357 - val_loss: 0.9149\n",
      "Epoch 2/20\n",
      "2352/2352 [==============================] - 335s 142ms/step - loss: 0.8936 - val_loss: 0.8118\n",
      "Epoch 3/20\n",
      "2352/2352 [==============================] - 340s 145ms/step - loss: 0.8162 - val_loss: 0.7631\n",
      "Epoch 4/20\n",
      "2352/2352 [==============================] - 344s 146ms/step - loss: 0.7749 - val_loss: 0.7392\n",
      "Epoch 5/20\n",
      "2352/2352 [==============================] - 347s 147ms/step - loss: 0.7480 - val_loss: 0.7207\n",
      "Epoch 6/20\n",
      "2352/2352 [==============================] - 313s 133ms/step - loss: 0.7290 - val_loss: 0.7095\n",
      "Epoch 7/20\n",
      "2352/2352 [==============================] - 322s 137ms/step - loss: 0.7150 - val_loss: 0.6980\n",
      "Epoch 8/20\n",
      "2352/2352 [==============================] - 346s 147ms/step - loss: 0.7038 - val_loss: 0.6907\n",
      "Epoch 9/20\n",
      "2352/2352 [==============================] - 345s 146ms/step - loss: 0.6943 - val_loss: 0.6869\n",
      "Epoch 10/20\n",
      "2352/2352 [==============================] - 341s 145ms/step - loss: 0.6872 - val_loss: 0.6814\n",
      "Epoch 11/20\n",
      "2352/2352 [==============================] - 330s 140ms/step - loss: 0.6807 - val_loss: 0.6747\n",
      "Epoch 12/20\n",
      "2352/2352 [==============================] - 336s 143ms/step - loss: 0.6753 - val_loss: 0.6763\n",
      "Epoch 13/20\n",
      "2352/2352 [==============================] - 321s 137ms/step - loss: 0.6703 - val_loss: 0.6708\n",
      "Epoch 14/20\n",
      "2352/2352 [==============================] - 315s 134ms/step - loss: 0.6657 - val_loss: 0.6695\n",
      "Epoch 15/20\n",
      "2352/2352 [==============================] - 314s 134ms/step - loss: 0.6620 - val_loss: 0.6663\n",
      "Epoch 16/20\n",
      "2352/2352 [==============================] - 315s 134ms/step - loss: 0.6587 - val_loss: 0.6673\n",
      "Epoch 17/20\n",
      "2352/2352 [==============================] - 315s 134ms/step - loss: 0.6557 - val_loss: 0.6591\n",
      "Epoch 18/20\n",
      "2352/2352 [==============================] - 315s 134ms/step - loss: 0.6525 - val_loss: 0.6596\n",
      "Epoch 19/20\n",
      "2352/2352 [==============================] - 315s 134ms/step - loss: 0.6500 - val_loss: 0.6582\n",
      "Epoch 20/20\n",
      "2352/2352 [==============================] - 315s 134ms/step - loss: 0.6476 - val_loss: 0.6545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f10b99b9d68>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = create_batch_generator(train_ids)\n",
    "val_generator = create_batch_generator(val_ids)\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(len(train_ids) / BATCH_SIZE),\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=np.ceil(len(val_ids) / BATCH_SIZE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T14:09:34.383102Z",
     "start_time": "2018-05-13T14:09:34.369221Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = inference_encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, max_len_target-1))\n",
    "    target_seq[0, 0] = tokenizer.word_index[START]\n",
    "    \n",
    "    tokens = {idx: token for (token, idx) in tokenizer.word_index.items()}\n",
    "    \n",
    "    decoded_sequence = ''\n",
    "    for i in range(max_len_target):\n",
    "        output_tokens, output_states = inference_decoder_model.predict(\n",
    "            [target_seq, states_value]\n",
    "        )\n",
    "        \n",
    "        # greedy search\n",
    "        sampled_token_idx = np.argmax(output_tokens[0, 0, :])\n",
    "        sampled_token = tokens.get(sampled_token_idx, '.')\n",
    "        if sampled_token == END:\n",
    "            break\n",
    "        decoded_sequence += sampled_token\n",
    "            \n",
    "        target_seq[0, 0] = sampled_token_idx\n",
    "        states_value = output_states\n",
    "    \n",
    "    return decoded_sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T14:09:34.399274Z",
     "start_time": "2018-05-13T14:09:34.386876Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    return decode_sequence(keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences([preprocess(sentence)]),\n",
    "        padding='post',\n",
    "        maxlen=max_len_input,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T14:09:40.854895Z",
     "start_time": "2018-05-13T14:09:34.401472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello.' --> 'dank.'\n",
      "'you are welcome.' --> 'das wissen wir alle.'\n",
      "'how do you do?' --> 'wie soll das geschehen?'\n",
      "'i hate mondays.' --> 'ich habe mit der geschäftsordnung geben.'\n",
      "'i am a programmer.' --> 'ich bin eine gemeinsame priorität.'\n",
      "'data is the new oil.' --> 'bedeutet das gegenteil.'\n",
      "'it could be worse.' --> 'er kann das ergebnissen bestehen.'\n",
      "'i am on top of it.' --> 'ich bin dazu bereits gesagt.'\n",
      "'n° uno' --> 'eine antwort auf den tag für eine antwort!'\n",
      "'awesome!' --> 'machen wir uns alle!'\n",
      "'put your feet up!' --> 'das ist eine gute nachricht!'\n",
      "'from the start till the end!' --> 'was für ein gutes beispiel dafür?'\n",
      "'from dusk till dawn.' --> 'die geschichte haben in der tat geschehen.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T14:09:53.147182Z",
     "start_time": "2018-05-13T14:09:40.856782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \"^please rise, then, for this minute' s silence.\", got 'bitte bedanken sie diese frage an dieser stelle.', exp: 'ich bitte sie, sich zu einer schweigeminute zu erheben.\\n'\n",
      "Original \"^(the house rose and observed a minute' s silence)\", got '(das parlament erhebt sich zu einer schweigeminute.)', exp: '(das parlament erhebt sich zu einer schweigeminute.)\\n'\n",
      "Original '^madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.\\n'\n",
      "Original '^madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.\\n'\n",
      "Original '^thank you, mr segni, i shall do so gladly.', got 'vielen dank, herr barroso, ich danke ihnen dafür.', exp: 'vielen dank, herr segni, das will ich gerne tun.\\n'\n",
      "Original '^it is the case of alexander nikitin.', got 'das ist die situation von kontrolle.', exp: 'das ist der fall von alexander nikitin.\\n'\n",
      "Original '^it will, i hope, be examined in a positive light.', got 'ich werde das also eine gemeinsame antwort erhalten.', exp: 'ich hoffe, daß dort in ihrem sinne entschieden wird.\\n'\n",
      "Original '^why are there no fire instructions?', got 'weshalb kann die kontrolle nicht vergessen?', exp: 'warum finden keine brandschutzbelehrungen statt?\\n'\n",
      "Original '^mr berenguer fuster, we shall check all this.', got 'herr barroso, wir haben diese frage gesprochen.', exp: 'lieber kollege, wir werden das prüfen.\\n'\n",
      "Original '^we do not know what is happening.', got 'wir wissen nicht, was das gesagt werden muss.', exp: 'wir wissen nicht, was passiert.\\n'\n",
      "Original '^agenda', got 'eigentlich', exp: 'arbeitsplan\\n'\n",
      "Original '^relating to wednesday:', got 'zum bericht prodi der präsidentschaft', exp: 'zum mittwoch:\\n'\n",
      "Original '^(applause from the pse group)', got '(beifall von der evp-fraktion)', exp: '(beifall der pse-fraktion)\\n'\n",
      "Original '^mr hänsch represented you on this occasion.', got 'herr marktorisch hat diese frage gesprochen.', exp: 'der kollege hänsch hat sie dort vertreten.\\n'\n",
      "Original '^we then put it to a vote.', got 'wir sollten daher nun an diesen bericht nicht akzeptieren.', exp: 'wir haben dann abgestimmt.\\n'\n",
      "Original '^there was a vote on this matter.', got 'es war der bericht von herrn castadier.', exp: 'es gab eine abstimmung zu diesem punkt.\\n'\n",
      "Original '^all of the others were of a different opinion.', got 'alle diese fragen sind nicht vergessen.', exp: 'alle anderen waren anderer meinung.\\n'\n",
      "Original '^that was the decision.', got 'das war der erste punkt.', exp: 'das war der beschluß.\\n'\n",
      "Original '^i should now like to comment on the issue itself.', got 'ich möchte dazu nur drei bemerkungen machen.', exp: 'jetzt möchte ich zur sache selbst etwas sagen.\\n'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {preprocess(en)!r}, got {predict(en)!r}, exp: {de[1:]!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T14:10:02.594411Z",
     "start_time": "2018-05-13T14:09:53.148628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original '^perhaps there is a connection.', got 'vielleicht ist ein großer fehler.', exp: 'vielleicht besteht da ein zusammenhang.\\n'\n",
      "Original '^(applause)', got '(beifall)', exp: '(beifall)\\n'\n",
      "Original '^mr president, what is going on?', got 'herr präsident, wir werden das tun?', exp: 'herr präsident! was steckt dahinter?\\n'\n",
      "Original '^it is something that we must do ourselves.', got 'das müssen wir uns alle einigen.', exp: 'es ist etwas, das wir selbst tun müssen.\\n'\n",
      "Original \"^no reform'.\", got 'nie wieder kommen.', exp: 'keine reform.\"\\n'\n",
      "Original '^there must be no more lockerbies.', got 'es muss keine gewinner sein.', exp: 'es darf keine lockerbies mehr geben.\\n'\n",
      "Original '^the european union must lead by example.', got 'die europäische union muss geleistet werden.', exp: 'die europäische union muss mit gutem beispiel vorangehen.\\n'\n",
      "Original '^that is the point.', got 'das ist das problem.', exp: 'darum geht es.\\n'\n",
      "Original '^it is as simple as that.', got 'das ist genau das gegenteil.', exp: 'so einfach ist der grund.\\n'\n",
      "Original '^how does he see those conflicts being resolved?', got 'wie lange gibt es die kontrolle?', exp: 'wie sollen diese konflikte seiner ansicht nach gelöst werden?\\n'\n",
      "Original '^not even rio could agree on that.', got 'das ist aber nicht der fall.', exp: 'darauf konnte sich rio noch nicht einmal einigen.\\n'\n",
      "Original '^situation in iraq', got 'lage in der tat in der tat (', exp: 'lage im irak\\n'\n",
      "Original '^quisthoudt-rowohl report (a0-0/0)', got 'bericht de vitorino (a0-0/0)', exp: 'bericht quisthoudt-rowohl (a0-0/0)\\n'\n",
      "Original '^the debate is closed.', got 'die aussprache ist geschlossen.', exp: 'die aussprache ist geschlossen.\\n'\n",
      "Original '^the vote will take place at 0.0 p.m. tomorrow.', got 'die abstimmung findet morgen um 0.0 uhr statt.', exp: 'die abstimmung findet morgen um 0.0 uhr statt.\\n'\n",
      "Original '^that has already been mentioned too.', got 'auch das ist ein großes problem.', exp: 'das wurde schon angesprochen.\\n'\n",
      "Original '^is this the example we wish to follow?', got 'ist das der grund für diese maßnahmen?', exp: 'ist dies das beispiel, dem wir folgen wollen?\\n'\n",
      "Original '^the debate is closed.', got 'die aussprache ist geschlossen.', exp: 'die aussprache ist geschlossen.\\n'\n",
      "Original '^my next point concerns liquids.', got 'meine zweite frage betrifft die kontrolle.', exp: 'nächster punkt: liquids.\\n'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {preprocess(en)!r}, got {predict(en)!r}, exp: {de[1:]!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T14:15:25.395366Z",
     "start_time": "2018-05-13T14:10:02.595645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865aceaa6ec64b188cb40748eb0ae0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average bleu score: 0.2131365083574276\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "try:\n",
    "    from spacy.lang.de import German\n",
    "except ModuleNotFoundError:\n",
    "    spacy.cli.download('de')\n",
    "    from spacy.lang.de import German\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "parser = German()\n",
    "chencherry = SmoothingFunction()  # to handle short sequences, see also http://www.nltk.org/_modules/nltk/translate/bleu_score.html#SmoothingFunction.method3\n",
    "\n",
    "def remove_spaces_and_puncts(tokens):\n",
    "     return [token.orth_ for token in tokens if not (token.is_space or token.is_punct)]  \n",
    "\n",
    "bleu_scores = np.zeros(TEST_SIZE)\n",
    "nist_scores = np.zeros(TEST_SIZE)\n",
    "\n",
    "for i in tqdm(range(TEST_SIZE)):\n",
    "    pred_tokens = remove_spaces_and_puncts(parser(predict(df.iloc[val_ids[i]].input_texts)))\n",
    "    ref_tokens = remove_spaces_and_puncts(parser(df.iloc[val_ids[i]].target_texts[1:]))\n",
    "    bleu_scores[i] = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=chencherry.method3)\n",
    "    \n",
    "print(\"Average bleu score:\", bleu_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T15:50:17.703056Z",
     "start_time": "2018-05-13T15:50:17.559191Z"
    }
   },
   "outputs": [],
   "source": [
    "name = 'simple_char_mt'\n",
    "model.save_weights(f'data/{name}_model_weights.h5')  # https://drive.google.com/open?id=1f7DuteA2qU3mkzEGOr3cjeVToiBBmCB_\n",
    "inference_encoder_model.save_weights(f'data/{name}_inference_encoder_model_weights.h5')  # https://drive.google.com/open?id=1OWOCDhVCoTNKkwoaZMPg0IFEW4vJnb01\n",
    "inference_decoder_model.save_weights(f'data/{name}_inference_decoder_model_weights.h5')  # https://drive.google.com/open?id=1IfRbB1F-ivdLsjuMUYBAxsQ5OzSRcGi1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T13:55:31.318315Z",
     "start_time": "2018-05-12T13:55:31.315672Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "From a technical point of view it's nice to see that overfitting doesn't matter due to the relatively large dataset of 150k training dataset. (It seemed to be a problem when playing around with a small dataset)\n",
    "\n",
    "It's obvious that the seq2seq-model learned to write german sentences, but on the other hand, they aren't really appropriate translations. Often enough they are completely wrong. We'll need improvements. My first step will be to use a higher than char-level embedding, in my case bytepairencoding embedding. To really train end-to end char-level, the usual guidance is that we need training data in the millions - ok, here we would have (but most of the training data would be very long sentences where we'd get further problems with RNNs without attentions, also already this training needs ~2h on a GTX 1080 GPU). So, my next step will be to use bytepairencoding embeddings and after that implementing Beam Search instead of Greedy Search and Attention Mechanism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 301,
   "position": {
    "height": "40px",
    "left": "987px",
    "right": "23px",
    "top": "124px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
