{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bytepairencoding seq2seq model in keras that translates english to german"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next I'll use higher than of char-level-embedding. The first natural choice would be to use word embeddings like word2vec, glove or the more modern facebook's fasttext (especially for german) or the most modern starspace. It has the advantage of getting very quick good results, so it would be the most natural choice for prototyping. The disadvantage is of course that out-of-vocabulary words can't be handled and that linguistic patterns (like plural or case) are harder to detect. The vocabulary size is the most critical, especially from terms of performance and memory consumption. A big vocabulary (like >50k) would be very problematic as we have softmax layer of that size for the outputs. Not only it's slow, also the one-hot-encoding takes enourmous space in memory and would get tricky to work around (even here, I had to reduce the batch size to work with 5k bytepairs). So, the state-of-the art technique is to use bytepairencodings with a medium size (around 10k). Here, I'll use the pretrained models from https://github.com/bheinzerling/bpemb \n",
    "\n",
    "One of the consequences is that I'll preprocess the raw sentences (lower case, convert numbers to $0$) to be able to use the pretrained bytepair embeddings. Wouldn't be a big hassle for production to train it on your own, but it's not necessary for demonstration purposes here.\n",
    "\n",
    "As trainings set I use the [European Parliament Proceedings Parallel Corpus 1996-2011](http://statmt.org/europarl/) German-English corpus with medium sized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:18:36.044978Z",
     "start_time": "2018-05-13T16:18:34.673322Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# technical detail so that an instance (maybe running in a different window)\n",
    "# doesn't take all the GPU memory resulting in some strange error messages\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:18:36.407565Z",
     "start_time": "2018-05-13T16:18:36.046413Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import keras\n",
    "import keras.layers as L\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sentencepiece as spm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Fixing random state ensure reproducible results\n",
    "RANDOM_STATE=42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.set_random_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:18:36.411923Z",
     "start_time": "2018-05-13T16:18:36.409086Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 50\n",
    "MAX_TARGET_LENGTH = 65\n",
    "LATENT_DIM = 512\n",
    "EMBEDDING_DIM = 300\n",
    "BPE_MERGE_OPERATIONS = 5_000  # I'd love to use 10_000 x 300, but this one is broken: https://github.com/bheinzerling/bpemb/issues/6\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "DROPOUT = 0.5\n",
    "TEST_SIZE = 500\n",
    "EMBEDDING_TRAINABLE = True  # Improves results significant and for at least it's not the most dominant training time factor (that's the output softmax layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:18:36.424309Z",
     "start_time": "2018-05-13T16:18:36.414174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n"
     ]
    }
   ],
   "source": [
    "def download_file(fname, url):\n",
    "    print(f\"Downloading {fname} from {url} ...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    total_size = int(response.headers.get('content-length', 0)); \n",
    "    block_size = 1024\n",
    "\n",
    "    download = tqdm(\n",
    "        response.iter_content(block_size),\n",
    "        total=math.ceil(total_size // block_size),\n",
    "        unit='KB',\n",
    "        unit_scale=True\n",
    "    )\n",
    "    with open(f\"{fname}\", \"wb\") as handle:\n",
    "        for data in download:\n",
    "            handle.write(data)\n",
    "\n",
    "PATH = 'data'\n",
    "INPUT_LANG = 'en'\n",
    "TARGET_LANG = 'de'\n",
    "LANGUAGES = [INPUT_LANG, TARGET_LANG]\n",
    "BPE_URL = {lang: f'http://cosyne.h-its.org/bpemb/data/{lang}/' for lang in LANGUAGES}\n",
    "BPE_MODEL_NAME = {lang: f'{lang}.wiki.bpe.op{BPE_MERGE_OPERATIONS}.model' for lang in LANGUAGES}\n",
    "BPE_WORD2VEC_NAME = {lang: f'{lang}.wiki.bpe.op{BPE_MERGE_OPERATIONS}.d{EMBEDDING_DIM}.w2v.bin' for lang in LANGUAGES}\n",
    "DOWNLOAD_FILES = {\n",
    "    'de-en.tgz': 'http://statmt.org/europarl/v7/de-en.tgz',\n",
    "    BPE_MODEL_NAME[INPUT_LANG]: f'{BPE_URL[INPUT_LANG]}/{BPE_MODEL_NAME[INPUT_LANG]}',\n",
    "    BPE_WORD2VEC_NAME[INPUT_LANG] + '.tar.gz': f'{BPE_URL[INPUT_LANG]}/{BPE_WORD2VEC_NAME[INPUT_LANG]}' + '.tar.gz',\n",
    "    BPE_MODEL_NAME[TARGET_LANG]: f'{BPE_URL[TARGET_LANG]}/{BPE_MODEL_NAME[TARGET_LANG]}',\n",
    "    BPE_WORD2VEC_NAME[TARGET_LANG] + '.tar.gz': f'{BPE_URL[TARGET_LANG]}/{BPE_WORD2VEC_NAME[TARGET_LANG]}' + '.tar.gz',\n",
    "}\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "for name, url in DOWNLOAD_FILES.items():\n",
    "    fname = os.path.join(PATH, name)\n",
    "    exists = os.path.exists(fname)\n",
    "    size = os.path.getsize(fname) if exists else -1\n",
    "    if exists and size > 0:\n",
    "        print(f'{name} already downloaded ({size / 2**20:3.1f} MB)')\n",
    "        continue\n",
    "    download_file(fname, url)\n",
    "    if (re.search(r'\\.(tgz|tar\\.gz)$', fname)):\n",
    "        tar = tarfile.open(fname, \"r:gz\")\n",
    "        tar.extractall(path=PATH)\n",
    "        tar.close()\n",
    "        print(f'Extracted {fname} ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:19:24.787313Z",
     "start_time": "2018-05-13T16:18:36.425938Z"
    }
   },
   "outputs": [],
   "source": [
    "# Following https://github.com/bheinzerling/bpemb/blob/master/preprocess_text.sh\n",
    "# (ignoring urls as there shouldn't be any in parliament discussions)\n",
    "def preprocess(line):\n",
    "    line = re.sub(r'\\d+', '0', line)\n",
    "    line = re.sub(r'\\s+', ' ', line)\n",
    "    return line.lower().strip()\n",
    "\n",
    "def read_corpus_lines(language):\n",
    "    return [preprocess(line) for line in open(f'{PATH}/europarl-v7.de-en.{language}', 'r').readlines()]\n",
    "    \n",
    "pd.set_option('max_colwidth', 60)\n",
    "df = pd.DataFrame(data={\n",
    "    'input_texts': read_corpus_lines('en'),\n",
    "    'target_texts': read_corpus_lines('de'), \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:19:25.756019Z",
     "start_time": "2018-05-13T16:19:24.790343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr total input: 1920209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resumption of the session</td>\n",
       "      <td>wiederaufnahme der sitzungsperiode</td>\n",
       "      <td>25</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i declare resumed the session of the european parliament...</td>\n",
       "      <td>ich erkläre die am freitag, dem 0. dezember unterbrochen...</td>\n",
       "      <td>203</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>although, as you will have seen, the dreaded 'millennium...</td>\n",
       "      <td>wie sie feststellen konnten, ist der gefürchtete \"millen...</td>\n",
       "      <td>191</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you have requested a debate on this subject in the cours...</td>\n",
       "      <td>im parlament besteht der wunsch nach einer aussprache im...</td>\n",
       "      <td>105</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in the meantime, i should like to observe a minute' s si...</td>\n",
       "      <td>heute möchte ich sie bitten - das ist auch der wunsch ei...</td>\n",
       "      <td>232</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input_texts  \\\n",
       "0                                    resumption of the session   \n",
       "1  i declare resumed the session of the european parliament...   \n",
       "2  although, as you will have seen, the dreaded 'millennium...   \n",
       "3  you have requested a debate on this subject in the cours...   \n",
       "4  in the meantime, i should like to observe a minute' s si...   \n",
       "\n",
       "                                                  target_texts  input_length  \\\n",
       "0                           wiederaufnahme der sitzungsperiode            25   \n",
       "1  ich erkläre die am freitag, dem 0. dezember unterbrochen...           203   \n",
       "2  wie sie feststellen konnten, ist der gefürchtete \"millen...           191   \n",
       "3  im parlament besteht der wunsch nach einer aussprache im...           105   \n",
       "4  heute möchte ich sie bitten - das ist auch der wunsch ei...           232   \n",
       "\n",
       "   target_length  \n",
       "0             34  \n",
       "1            217  \n",
       "2            185  \n",
       "3            110  \n",
       "4            217  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Nr total input:\", len(df))\n",
    "df.target_texts = df.target_texts  # encode a start symbol (doesn't occur in texts)\n",
    "df['input_length'] = df.input_texts.apply(len)\n",
    "df['target_length'] = df.target_texts.apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:19:25.855319Z",
     "start_time": "2018-05-13T16:19:25.757768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167211"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_empty = (df.input_length > 1) & (df.target_length > 1)  # there are empty phrases like '\\n' --> 'Frau Präsidentin\\n'\n",
    "short_inputs = (df.input_length < MAX_INPUT_LENGTH) & (df.target_length < MAX_TARGET_LENGTH)\n",
    "sum(non_empty & short_inputs)\n",
    "df = df[non_empty & short_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:19:25.973342Z",
     "start_time": "2018-05-13T16:19:25.856952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁das', '▁ist', '▁ein', '▁test', '▁für', '▁v', 'ort', 'rain', 'iert', '▁teil', 'wort', '-', 'em', 'be', 'd', 'dings']\n"
     ]
    }
   ],
   "source": [
    "input_pretrained_bpe = KeyedVectors.load_word2vec_format(os.path.join(PATH, BPE_WORD2VEC_NAME[INPUT_LANG]), binary=True)\n",
    "target_pretrained_bpe = KeyedVectors.load_word2vec_format(os.path.join(PATH, BPE_WORD2VEC_NAME[TARGET_LANG]), binary=True)\n",
    "sp_input = spm.SentencePieceProcessor()\n",
    "sp_input.Load(os.path.join(PATH, BPE_MODEL_NAME[INPUT_LANG]))\n",
    "subwords = sp_input.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\")\n",
    "print(subwords)\n",
    "sp_target = spm.SentencePieceProcessor()\n",
    "sp_target.Load(os.path.join(PATH, BPE_MODEL_NAME[TARGET_LANG]))\n",
    "subwords = sp_target.EncodeAsPieces(\"das ist ein test für vortrainiert teilwort-embeddings\")\n",
    "print(subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:19:33.337106Z",
     "start_time": "2018-05-13T16:19:25.974701Z"
    }
   },
   "outputs": [],
   "source": [
    "input_wordvec_index = dict({\n",
    "    word: index \n",
    "    for index, word \n",
    "    in enumerate(['<pad>', '<s>', '</s>'] + input_pretrained_bpe.index2word)  # haven't found start/stop tokens, so add them manually\n",
    "})\n",
    "input_unk_index = input_wordvec_index['<unk>']\n",
    "\n",
    "target_wordvec_index = dict({\n",
    "    word: index \n",
    "    for index, word \n",
    "    in enumerate(['<pad>', '<s>', '</s>'] + target_pretrained_bpe.index2word)  # haven't found start/stop tokens, so add them manually\n",
    "})\n",
    "target_unk_index = target_wordvec_index['<unk>']\n",
    "\n",
    "def subword_indices(text, unk_index, sp, wordvec_index):\n",
    "    subwords = ['<s>'] + sp.EncodeAsPieces(text) + ['</s>']  # automatic add start/stop index\n",
    "    return [wordvec_index.get(subword, unk_index) for subword in subwords]\n",
    "\n",
    "def input_subword_indices(text):\n",
    "    return subword_indices(text, input_unk_index, sp_input, input_wordvec_index)\n",
    "\n",
    "def target_subword_indices(text):\n",
    "    return subword_indices(text, target_unk_index, sp_target, target_wordvec_index)\n",
    "\n",
    "FULL_EMBEDDING_DIM = EMBEDDING_DIM + 2\n",
    "input_embedding_matrix = np.zeros((len(input_wordvec_index), FULL_EMBEDDING_DIM), dtype=np.float32)\n",
    "input_embedding_matrix[0, :] = 1e-6 * np.random.standard_normal(FULL_EMBEDDING_DIM)  # pad symbol as close to zero\n",
    "input_embedding_matrix[1, -1] = 1  # one hot encode start symbol\n",
    "input_embedding_matrix[2, -2] = 1  # one hot encode stop symbol\n",
    "input_embedding_matrix[3:, :-2] = input_pretrained_bpe.vectors\n",
    "\n",
    "target_embedding_matrix = np.zeros((len(target_wordvec_index), FULL_EMBEDDING_DIM), dtype=np.float32)\n",
    "target_embedding_matrix[0, :] = 1e-6 * np.random.standard_normal(FULL_EMBEDDING_DIM)  # pad symbol as close to zero\n",
    "target_embedding_matrix[1, -1] = 1  # one hot encode start symbol\n",
    "target_embedding_matrix[2, -2] = 1  # one hot encode stop symbol\n",
    "target_embedding_matrix[3:, :-2] = target_pretrained_bpe.vectors\n",
    "\n",
    "df['input_sequences'] = df.input_texts.apply(input_subword_indices)\n",
    "df['target_sequences'] = df.target_texts.apply(target_subword_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:19:33.348152Z",
     "start_time": "2018-05-13T16:19:33.338753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.5701549e-07, -6.9290962e-07,  8.9959985e-07,  3.0729953e-07,\n",
       "         8.1286214e-07,  6.2962886e-07, -8.2899498e-07, -5.6018104e-07],\n",
       "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
       "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  1.0000000e+00,  0.0000000e+00],\n",
       "       [-7.1291000e-02,  1.3662800e-01, -9.3874000e-02, -1.3716000e-02,\n",
       "         5.3709999e-02,  5.3034998e-02,  0.0000000e+00,  0.0000000e+00],\n",
       "       [-5.3902999e-02, -1.0761100e-01, -3.5625100e-01,  6.3295998e-02,\n",
       "         1.0626500e-01, -1.8801700e-01,  0.0000000e+00,  0.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.240234,  0.259846,  0.233096,  1.183777, -0.265572, -0.147195,\n",
       "         0.      ,  0.      ],\n",
       "       [ 0.741856,  0.96104 , -0.303011, -0.01168 , -0.268806,  0.085153,\n",
       "         0.      ,  0.      ],\n",
       "       [-0.086124, -0.168327,  0.055605,  1.531863, -0.044677,  0.176467,\n",
       "         0.      ,  0.      ],\n",
       "       [-0.368369,  0.186533,  0.397875,  0.504644,  0.097682, -0.148711,\n",
       "         0.      ,  0.      ],\n",
       "       [ 0.272315,  0.356335,  0.225856,  0.584159, -0.160238,  0.018695,\n",
       "         0.      ,  0.      ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.6987493e-07, -9.7876375e-07, -4.4429325e-07,  3.7730049e-07,\n",
       "         7.5698864e-07, -9.2216533e-07,  8.6960591e-07,  1.3556379e-06],\n",
       "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
       "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  1.0000000e+00,  0.0000000e+00],\n",
       "       [-3.5677999e-02, -7.2214000e-02, -6.2057000e-02,  2.3062800e-01,\n",
       "         1.2143200e-01,  1.2812901e-01,  0.0000000e+00,  0.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embedding_matrix[:5, -8:]\n",
    "input_embedding_matrix[-5:, -8:]\n",
    "target_embedding_matrix[:4, -8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:19:33.428096Z",
     "start_time": "2018-05-13T16:19:33.350239Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len_input = df.input_sequences.apply(len).max()\n",
    "max_len_target = df.target_sequences.apply(len).max()\n",
    "nr_input_tokens = len(input_wordvec_index)  \n",
    "nr_target_tokens = len(target_wordvec_index)\n",
    "\n",
    "# one hot encoded y_t_output wouldn't fit into memory any longer\n",
    "# so need to train/validate on batches generated on the fly\n",
    "def create_batch_generator(samples_ids):\n",
    "    \n",
    "    def batch_generator():\n",
    "        nr_batches = np.ceil(len(samples_ids) / BATCH_SIZE)\n",
    "        while True:\n",
    "            shuffled_ids = np.random.permutation(samples_ids)\n",
    "            batch_splits = np.array_split(shuffled_ids, nr_batches)\n",
    "            for batch_ids in batch_splits:\n",
    "                batch_X = pad_sequences(df.iloc[batch_ids].input_sequences, padding='post', maxlen=max_len_input)\n",
    "                batch_y = pad_sequences(df.iloc[batch_ids].target_sequences, padding='post', maxlen=max_len_target)\n",
    "                batch_y_t_output = keras.utils.to_categorical(batch_y[:,1:], num_classes=nr_target_tokens)\n",
    "                batch_x_t_input = batch_y[:,:-1]\n",
    "                yield ([batch_X, batch_x_t_input], batch_y_t_output)\n",
    "    \n",
    "    return batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:19:33.437034Z",
     "start_time": "2018-05-13T16:19:33.429576Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(df.shape[0]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:19:33.443138Z",
     "start_time": "2018-05-13T16:19:33.438807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5829, 5367)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(150489, 16722)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_input_tokens, nr_target_tokens\n",
    "len(train_ids), len(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:19:34.770345Z",
     "start_time": "2018-05-13T16:19:33.444602Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_gru = L.Bidirectional(\n",
    "    L.GRU(LATENT_DIM // 2, dropout=DROPOUT, return_state=True, name='encoder_gru'),\n",
    "    name='encoder_bidirectional'\n",
    ")\n",
    "decoder_gru = L.GRU(LATENT_DIM, dropout=DROPOUT, return_sequences=True, return_state=True, name='decoder_gru', dtype=tf.float32)\n",
    "decoder_dense = L.Dense(nr_target_tokens, activation='softmax', name='decoder_outputs', dtype=tf.float32)\n",
    "\n",
    "input_embedding = L.Embedding(\n",
    "    nr_input_tokens,\n",
    "    FULL_EMBEDDING_DIM,\n",
    "    mask_zero=True,\n",
    "    weights=[input_embedding_matrix],\n",
    "    trainable=EMBEDDING_TRAINABLE,\n",
    "    name='input_embedding',\n",
    "    dtype=tf.float32,\n",
    ")\n",
    "target_embedding = L.Embedding(\n",
    "    nr_target_tokens,\n",
    "    FULL_EMBEDDING_DIM,\n",
    "    mask_zero=True,\n",
    "    weights=[target_embedding_matrix],\n",
    "    trainable=EMBEDDING_TRAINABLE,\n",
    "    name='target_embedding',\n",
    "    dtype=tf.float32,\n",
    ")\n",
    "\n",
    "encoder_inputs = L.Input(shape=(max_len_input, ), dtype='int32', name='encoder_inputs')\n",
    "encoder_embeddings = input_embedding(encoder_inputs)\n",
    "_, encoder_state_1, encoder_state_2 = encoder_gru(encoder_embeddings)\n",
    "encoder_states = L.concatenate([encoder_state_1, encoder_state_2])\n",
    "\n",
    "decoder_inputs = L.Input(shape=(max_len_target-1, ), dtype='int32', name='decoder_inputs')\n",
    "decoder_mask = L.Masking(mask_value=0)(decoder_inputs)\n",
    "decoder_embeddings_inputs = target_embedding(decoder_mask)\n",
    "decoder_embeddings_outputs, _ = decoder_gru(decoder_embeddings_inputs, initial_state=encoder_states) \n",
    "decoder_outputs = decoder_dense(decoder_embeddings_outputs)\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "\n",
    "inference_encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "inference_decoder_state_inputs = L.Input(shape=(LATENT_DIM, ), dtype='float32', name='inference_decoder_state_inputs')\n",
    "inference_decoder_embeddings_outputs, inference_decoder_states = decoder_gru(\n",
    "    decoder_embeddings_inputs, initial_state=inference_decoder_state_inputs\n",
    ")\n",
    "inference_decoder_outputs = decoder_dense(inference_decoder_embeddings_outputs)\n",
    "\n",
    "inference_decoder_model = Model(\n",
    "    [decoder_inputs, inference_decoder_state_inputs], \n",
    "    [inference_decoder_outputs, inference_decoder_states]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:19:34.777312Z",
     "start_time": "2018-05-13T16:19:34.771604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, 27)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     (None, 39)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_embedding (Embedding)     (None, 27, 302)      1760358     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 39)           0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bidirectional (Bidirect [(None, 512), (None, 858624      input_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "target_embedding (Embedding)    (None, 39, 302)      1620834     masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           encoder_bidirectional[0][1]      \n",
      "                                                                 encoder_bidirectional[0][2]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 39, 512), (N 1251840     target_embedding[0][0]           \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 39, 5367)     2753271     decoder_gru[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 8,244,927\n",
      "Trainable params: 8,244,927\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, 39)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 39)           0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "target_embedding (Embedding)    (None, 39, 302)      1620834     masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "inference_decoder_state_inputs  (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 39, 512), (N 1251840     target_embedding[0][0]           \n",
      "                                                                 inference_decoder_state_inputs[0]\n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 39, 5367)     2753271     decoder_gru[1][0]                \n",
      "==================================================================================================\n",
      "Total params: 5,625,945\n",
      "Trainable params: 5,625,945\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "inference_decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T16:19:34.811513Z",
     "start_time": "2018-05-13T16:19:34.779491Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(clipnorm=1., clipvalue=.5), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:16:13.187368Z",
     "start_time": "2018-05-13T16:19:34.812977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2352/2352 [==============================] - 348s 148ms/step - loss: 2.7290 - val_loss: 1.9636\n",
      "Epoch 2/20\n",
      "2352/2352 [==============================] - 347s 147ms/step - loss: 1.8300 - val_loss: 1.7030\n",
      "Epoch 3/20\n",
      "2352/2352 [==============================] - 358s 152ms/step - loss: 1.6013 - val_loss: 1.5985\n",
      "Epoch 4/20\n",
      "2352/2352 [==============================] - 364s 155ms/step - loss: 1.4721 - val_loss: 1.5419\n",
      "Epoch 5/20\n",
      "2352/2352 [==============================] - 355s 151ms/step - loss: 1.3864 - val_loss: 1.5193\n",
      "Epoch 6/20\n",
      "2352/2352 [==============================] - 356s 152ms/step - loss: 1.3251 - val_loss: 1.5160\n",
      "Epoch 7/20\n",
      "2352/2352 [==============================] - 347s 147ms/step - loss: 1.2769 - val_loss: 1.4939\n",
      "Epoch 8/20\n",
      "2352/2352 [==============================] - 344s 146ms/step - loss: 1.2390 - val_loss: 1.4941\n",
      "Epoch 9/20\n",
      "2352/2352 [==============================] - 343s 146ms/step - loss: 1.2078 - val_loss: 1.5028\n",
      "Epoch 10/20\n",
      "2352/2352 [==============================] - 343s 146ms/step - loss: 1.1827 - val_loss: 1.4978\n",
      "Epoch 11/20\n",
      "2352/2352 [==============================] - 344s 146ms/step - loss: 1.1601 - val_loss: 1.4966\n",
      "Epoch 12/20\n",
      "2352/2352 [==============================] - 343s 146ms/step - loss: 1.1400 - val_loss: 1.5164\n",
      "Epoch 13/20\n",
      "2352/2352 [==============================] - 343s 146ms/step - loss: 1.1234 - val_loss: 1.5026\n",
      "Epoch 14/20\n",
      "2352/2352 [==============================] - 344s 146ms/step - loss: 1.1090 - val_loss: 1.5036\n",
      "Epoch 15/20\n",
      "2352/2352 [==============================] - 343s 146ms/step - loss: 1.0957 - val_loss: 1.5123\n",
      "Epoch 16/20\n",
      "2352/2352 [==============================] - 347s 148ms/step - loss: 1.0842 - val_loss: 1.5138\n",
      "Epoch 17/20\n",
      "2352/2352 [==============================] - 365s 155ms/step - loss: 1.0724 - val_loss: 1.5175\n",
      "Epoch 18/20\n",
      "2352/2352 [==============================] - 358s 152ms/step - loss: 1.0631 - val_loss: 1.5269\n",
      "Epoch 19/20\n",
      "2352/2352 [==============================] - 353s 150ms/step - loss: 1.0534 - val_loss: 1.5111\n",
      "Epoch 20/20\n",
      "2352/2352 [==============================] - 351s 149ms/step - loss: 1.0455 - val_loss: 1.5333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8088884a58>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = create_batch_generator(train_ids)\n",
    "val_generator = create_batch_generator(val_ids)\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(len(train_ids) / BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=np.ceil(len(val_ids) / BATCH_SIZE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:16:13.198951Z",
     "start_time": "2018-05-13T18:16:13.190699Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = inference_encoder_model.predict(input_seq)\n",
    "    \n",
    "    tokens = {idx: token for (token, idx) in target_wordvec_index.items()}\n",
    "    start_token_idx = target_wordvec_index['<s>']\n",
    "    end_token_idx = target_wordvec_index['</s>']\n",
    "    \n",
    "    target_seq = np.zeros((1, max_len_target-1))\n",
    "    target_seq[0, 0] = start_token_idx\n",
    "    \n",
    "    decoded_sequence = [] \n",
    "    for i in range(max_len_target):\n",
    "        output_tokens, output_states = inference_decoder_model.predict(\n",
    "            [target_seq, states_value]\n",
    "        )\n",
    "        \n",
    "        # greedy search\n",
    "        sampled_token_idx = np.argmax(output_tokens[0, 0, :])\n",
    "        if sampled_token_idx == end_token_idx:\n",
    "            break\n",
    "        sampled_token = tokens.get(sampled_token_idx, '~')\n",
    "        decoded_sequence.append(sampled_token)\n",
    "            \n",
    "        target_seq[0, 0] = sampled_token_idx\n",
    "        states_value = output_states\n",
    "    \n",
    "    return sp_target.DecodePieces(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:16:13.209066Z",
     "start_time": "2018-05-13T18:16:13.201172Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    return decode_sequence(keras.preprocessing.sequence.pad_sequences(\n",
    "        [input_subword_indices(preprocess(sentence))],\n",
    "        padding='post',\n",
    "        maxlen=max_len_input,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:16:14.704911Z",
     "start_time": "2018-05-13T18:16:13.211387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hello.\\n' --> 'hallam.'\n",
      "'You are welcome.\\n' --> 'seien sie willkommen.'\n",
      "'How do you do?\\n' --> 'was tun sie?'\n",
      "'I hate mondays.\\n' --> 'ich habe mir meine fehler.'\n",
      "'I am a programmer.\\n' --> 'ich bin ein programm.'\n",
      "'Data is the new oil.\\n' --> 'die erdöl ist ein ölliger anfang.'\n",
      "'It could be worse.\\n' --> 'das könnte weiter verschlechtert werden.'\n",
      "'I am on top of it.\\n' --> 'ich bin überzeugt.'\n",
      "'N° Uno\\n' --> 'napvess'\n",
      "'Awesome!\\n' --> 'das ist eine schande!'\n",
      "'Put your feet up!\\n' --> 'fangen sie hart!'\n",
      "'From the start till the end!\\n' --> 'mit dem zielsetzungen!'\n",
      "'From dusk till dawn.\\n' --> 'von einem dialog wird sie tun.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{en!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:16:16.907785Z",
     "start_time": "2018-05-13T18:16:14.706125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \"please rise, then, for this minute' s silence.\", got 'bitte fahren sie sich das wort für mich.', exp: 'ich bitte sie, sich zu einer schweigeminute zu erheben.'\n",
      "Original \"(the house rose and observed a minute' s silence)\", got '(das parlament erhebt sich zu einer schweigeminute.)', exp: '(das parlament erhebt sich zu einer schweigeminute.)'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'thank you, mr segni, i shall do so gladly.', got 'vielen dank, herr segni, ich möchte es danken.', exp: 'vielen dank, herr segni, das will ich gerne tun.'\n",
      "Original 'it is the case of alexander nikitin.', got 'das ist der fall von alexander nikitin.', exp: 'das ist der fall von alexander nikitin.'\n",
      "Original 'it will, i hope, be examined in a positive light.', got 'ich hoffe, dass dort hineingekommen wird.', exp: 'ich hoffe, daß dort in ihrem sinne entschieden wird.'\n",
      "Original 'why are there no fire instructions?', got 'warum gibt es keine brandschutzbelehrungen?', exp: 'warum finden keine brandschutzbelehrungen statt?'\n",
      "Original 'mr berenguer fuster, we shall check all this.', got 'herr berlatuelles, wir prüfen das überhaupt beschreiben.', exp: 'lieber kollege, wir werden das prüfen.'\n",
      "Original 'we do not know what is happening.', got 'wir wissen nicht, was geschieht.', exp: 'wir wissen nicht, was passiert.'\n",
      "Original 'agenda', got 'tagesordnung', exp: 'arbeitsplan'\n",
      "Original 'relating to wednesday:', got 'zum mittwoch:', exp: 'zum mittwoch:'\n",
      "Original '(applause from the pse group)', got '(beifall von der pse-fraktion)', exp: '(beifall der pse-fraktion)'\n",
      "Original 'mr hänsch represented you on this occasion.', got 'herr seppänen hat sie in diesem fall bereits vertreten.', exp: 'der kollege hänsch hat sie dort vertreten.'\n",
      "Original 'we then put it to a vote.', got 'wir haben dann zustimmen.', exp: 'wir haben dann abgestimmt.'\n",
      "Original 'there was a vote on this matter.', got 'es gab eine abstimmung.', exp: 'es gab eine abstimmung zu diesem punkt.'\n",
      "Original 'all of the others were of a different opinion.', got 'alle anderen waren anderer meinung.', exp: 'alle anderen waren anderer meinung.'\n",
      "Original 'that was the decision.', got 'das war die entscheidung.', exp: 'das war der beschluß.'\n",
      "Original 'i should now like to comment on the issue itself.', got 'nun zu dem anliegen.', exp: 'jetzt möchte ich zur sache selbst etwas sagen.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:16:19.244660Z",
     "start_time": "2018-05-13T18:16:16.909173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'a lot of inspiration is needed.', got 'eine vielzahl von uns ist erforderlich.', exp: 'diese denkanstöße sind dringend nötig.'\n",
      "Original 'so what was cancún about?', got 'was ist also zu cancún?', exp: 'worum ging es in cancún?'\n",
      "Original 'i now turn to another subject.', got 'ich komme nun zu einem weiteren punkt.', exp: 'ich komme jetzt zu einem anderen punkt.'\n",
      "Original 'thank you, mr prodi.', got 'vielen dank, herr prodi.', exp: 'vielen dank, herr prodi.'\n",
      "Original 'the experience acquired varied somewhat.', got 'die erfahrung hat etwas ähnliches.', exp: 'die damit gemachten erfahrungen sind recht unterschiedlich.'\n",
      "Original 'that is what mr smith suggested.', got 'das hat sagt, herr smith.', exp: 'das hat herr smith vorgeschlagen.'\n",
      "Original 'it is stable internally and externally.', got 'das ist ein ständiges und interessanter punkt.', exp: 'er ist stabil nach innen und außen.'\n",
      "Original 'mr president, i am wearing my irish scarf today.', got 'herr präsident, ich bin der irischen ärger ausgebildete.', exp: 'herr präsident, ich trage heute mein irisches tuch.'\n",
      "Original 'it will not lead to any savings.', got 'sie wird keine übersprechendsprechen kommen.', exp: 'es werden keine einsparungen eintreten.'\n",
      "Original '(applause)', got '(beifall)', exp: '(beifall)'\n",
      "Original 'it will be noted in the minutes.', got 'das wird im protokoll aufgenommen.', exp: 'wir nehmen dies zu protokoll.'\n",
      "Original 'we need another great leap forward.', got 'wir brauchen einen weiteren schengen.', exp: 'was wir wieder brauchen, ist ein großer sprung.'\n",
      "Original 'that is the real problem.', got 'das ist das eigentliche problem.', exp: 'das ist das eigentliche problem.'\n",
      "Original '(the sitting was closed at 0.0 p.m.)', got '(die sitzung wird um 0.0 uhr geschlossen.)', exp: '(die sitzung wird um 0.0 uhr geschlossen.)0'\n",
      "Original 'how committed are you?', got 'wie stehen sie zu dieser verpflichtung?', exp: 'wie engagiert sind sie wirklich?'\n",
      "Original 'what was lacking was the necessary legal force.', got 'weil man das rechtsentwurf war.', exp: 'was fehlte, waren die gesetzlichen auflagen.'\n",
      "Original 'each of the speakers will have one minute.', got 'jede minute einer minute werden das wort ergreifen.', exp: 'jeder sprecher hat eine minute zeit.'\n",
      "Original 'but that is not enough.', got 'aber das ist nicht genug.', exp: 'aber das ist nicht genug.'\n",
      "Original 'this seems to me to be a workable solution.', got 'das scheint mir eine echte lösung zu sein.', exp: 'ich halte dieses vorgehen für angemessen.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.757495Z",
     "start_time": "2018-05-13T18:16:19.246036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a90b0a4d67f49fe8931a396c7054ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average bleu score: 0.31657723155182826\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "try:\n",
    "    from spacy.lang.de import German\n",
    "except ModuleNotFoundError:\n",
    "    spacy.cli.download('de')\n",
    "    from spacy.lang.de import German\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "parser = German()\n",
    "chencherry = SmoothingFunction()  # to handle short sequences, see also http://www.nltk.org/_modules/nltk/translate/bleu_score.html#SmoothingFunction.method3\n",
    "\n",
    "def remove_spaces_and_puncts(tokens):\n",
    "     return [token.orth_ for token in tokens if not (token.is_space or token.is_punct)]  \n",
    "\n",
    "bleu_scores = np.zeros(TEST_SIZE)\n",
    "nist_scores = np.zeros(TEST_SIZE)\n",
    "\n",
    "for i in tqdm(range(TEST_SIZE)):\n",
    "    pred_tokens = remove_spaces_and_puncts(parser(predict(df.iloc[val_ids[i]].input_texts)))\n",
    "    ref_tokens = remove_spaces_and_puncts(parser(df.iloc[val_ids[i]].target_texts))\n",
    "    bleu_scores[i] = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=chencherry.method3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.992652Z",
     "start_time": "2018-05-13T18:17:20.758836Z"
    }
   },
   "outputs": [],
   "source": [
    "name = 'bytepairencoding'\n",
    "model.save_weights(f'models/{name}_model_weights.h5')  # https://drive.google.com/open?id=1xK2QVTsIpJLmphSEUZl1Unqmz85MYeQK\n",
    "inference_encoder_model.save_weights(f'models/{name}_inference_encoder_model_weights.h5')  # https://drive.google.com/open?id=115Kp7ZIMqxu6YDvk4RhjvfYShcQdRP_o\n",
    "inference_decoder_model.save_weights(f'models/{name}_inference_decoder_model_weights.h5')  # https://drive.google.com/open?id=1_e3DE5lDw10joIb83UFbzGJyvQrMfb8w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "It is definitly an improvement to the pure char level, allthough a lot of the translations still are more funny than decent. The bleu scores correlates with this feeling with a very significant increase ($0.316 > 0.213$).\n",
    "On a side note, the training could be early stopped after a few epochs (and indeed overfits from that moment). So, this new model has a lot of power left to be filled with either more data or a better model. But the next thing missing is the Beam Search instead of the Greedy Search. I guess, this will reduce a bit some of the strangeness/funny translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 301,
   "position": {
    "height": "354px",
    "left": "569px",
    "right": "121px",
    "top": "131px",
    "width": "670px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
