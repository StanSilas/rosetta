{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple seq2seq model in keras that learns adding and subtracting integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To not get distracted, I'll start with a toy problem (I had as exercise in https://www.coursera.org/learn/language-processing). It will learn to calculate integers on char level end to end.\n",
    "I follow the standard seq2seq model with a RNN encoder and decoder (where the decoder gets as input a thought vector from the encoder).\n",
    "Design-Decisions:\n",
    "\n",
    "+ *GRUs*: The usual advice is to use LSTMs first, then implement GRUs and use the LSTM version as benchmark. If the GRU version is similiar, use GRUs as they only need half of the parameters as LSTMs. Here, it's a known problem and GRUs work fine (I also tried LSTMs, but the results were only slightly better).\n",
    "+ *Bidirectional-Encoder*: If useful, bidirectional encoders work better usually, so I use them for the encoder here (obviously they can't be used for the decoder, for at least not in an easy way).\n",
    "+ *1 Layer*: Well, working with deeper layers would be a way for improving. Here, a 2nd layer would improve the results a bit. But in the end, the training time is much longer, it overfits much easier. And of course for the toy problem, I could easily train a much simpler network of fully connected layers here - in the end we have a linear function to be solved ($\\sum_{i=0}^{d_a} 10^i a_d \\pm \\sum_{i=0}^{d_b} 10^i a_b$ for $a=a_{d_a} \\dots a_0, b=b_{d_b} \\dots b_0$). But this is not the purpose of creating a seq2seq model. The goal of course here is to show that seq2seq can learn how to calculate without it even knows itselfs that it calculates :-)\n",
    "+ *Embeddings*: Here, I use Embeddings from the start on, allthough training an one hot encoded characters would be fine here too (indeed, it should give similiar results from a theoretical point of view and I also tried it). But working with Embeddings makes it easier to later plug in Word-Embeddings or Bytepair-Encodings.\n",
    "+ *Masking*: It's very important to mask the paddings (here 0-values), so that the loss function doesn't care about the paddings. Otherwise the training process would take much longer (where it only learns padding in addition). Keras really has the nice Masking layer that does it all automatic for us, but it took me astonishing time to understand it.\n",
    "+ *START, END coding*: Results were better if there is also an END sign in the encoding strings.\n",
    "+ *Log-Uniform distributed equations*: If I'd just train on uniform distributed operands, small values are underrepresented (in the way that it might not learn how to add 1 or 2 digits numbers as most of the training examples have more digits). When working with texts, for a lot of time, we'd solve the problem by generating seq2seq models for different input lengths (like <=5, 5-10, 10-15, >15 or so). It took me some time to figure out how important it was to loguniform the training distributions here (and it was frustrating to see that $1+1=?$ was impossible to learn for the model, but $1234+4567$ was not)\n",
    "+ *Hyperparameters*: It's not a kaggle here, so I decided not to play around unless really necessary. So I take the defaults for learning rate and others, I choosed dropout to 0.5 what's always a reasonable value (and worked better than no dropout) and just some typical values for small problems (training size 100k is fine and still quick enough to run it through, batch size ~128 is usual and so on).\n",
    "+ *Decoding*: For simplicity, I take Greedy Search here, Beam Search is something for later.\n",
    "+ *OOP*: For production and reusability, it would be much better to write a Seq2Seq class. But to understand what's going on, it's a bit disturbing, so here it as simple imperative notebook without much syntactic noise or even comments.\n",
    "\n",
    "This script follows very narrow https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:29:00.753845Z",
     "start_time": "2018-05-08T11:28:59.497908Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# technical detail so that an instance (maybe running in a different window)\n",
    "# doesn't take all the GPU memory resulting in some strange error messages\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:29:00.757556Z",
     "start_time": "2018-05-08T11:29:00.755129Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.layers as L\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Fixing random state ensure reproducible results\n",
    "RANDOM_STATE=42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.set_random_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:29:00.769133Z",
     "start_time": "2018-05-08T11:29:00.758897Z"
    }
   },
   "outputs": [],
   "source": [
    "START = '^'\n",
    "END = '$'\n",
    "\n",
    "SIZE = 100_000\n",
    "LATENT_DIM = 512\n",
    "EMBEDDING_DIM = 16\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:29:00.775234Z",
     "start_time": "2018-05-08T11:29:00.770530Z"
    }
   },
   "outputs": [],
   "source": [
    "def loguniform_int(low=0, high=1, size=1):\n",
    "    offset = np.max([1 - low, 0])\n",
    "    low, high = np.log([low + offset, high + offset])\n",
    "    return (np.exp(np.random.uniform(low, high, size)) - offset).astype(int)\n",
    "\n",
    "def create_equations_df(size, min_value=0, max_value=9999, operations={'+': np.add, '-': np.subtract}):\n",
    "    df = pd.DataFrame()\n",
    "    df['a'] = loguniform_int(low=min_value, high=max_value, size=size)\n",
    "    df['b'] = loguniform_int(low=min_value, high=max_value, size=size)\n",
    "    df['op'] = np.random.choice(list(operations.keys()), size)\n",
    "    df['result'] = np.zeros(size, dtype='int')\n",
    "    for symbol, calc in operations.items():\n",
    "        df.loc[df.op == symbol, 'result'] = calc(df[df.op == symbol]['a'], df[df.op == symbol]['b'])\n",
    "        \n",
    "    df['input_texts'] = df.a.astype(str) + df.op + df.b.astype(str) + END\n",
    "    df['target_texts'] = START + df.result.astype(str) + END\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:29:01.035692Z",
     "start_time": "2018-05-08T11:29:00.776561Z"
    }
   },
   "outputs": [],
   "source": [
    "df = create_equations_df(SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:29:01.042103Z",
     "start_time": "2018-05-08T11:29:01.037105Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = pd.concat([df.input_texts, df.target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:29:01.897983Z",
     "start_time": "2018-05-08T11:29:01.043648Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=None, filters=None, char_level=True)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "df['input_sequences'] = tokenizer.texts_to_sequences(df.input_texts)\n",
    "df['target_sequences'] = tokenizer.texts_to_sequences(df.target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:29:02.646924Z",
     "start_time": "2018-05-08T11:29:01.899350Z"
    }
   },
   "outputs": [],
   "source": [
    "X = keras.preprocessing.sequence.pad_sequences(df.input_sequences, padding='post')\n",
    "y = keras.preprocessing.sequence.pad_sequences(df.target_sequences, padding='post')\n",
    "y_t_output = keras.utils.to_categorical(y[:,1:], num_classes=len(tokenizer.word_index)+1)\n",
    "x_t_input = y[:,:-1]\n",
    "\n",
    "max_len_input = X.shape[1]\n",
    "max_len_target = x_t_input.shape[1]\n",
    "nr_tokens = y_t_output.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:29:02.658009Z",
     "start_time": "2018-05-08T11:29:02.648142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$': 1,\n",
       " '1': 2,\n",
       " '2': 3,\n",
       " '^': 4,\n",
       " '3': 5,\n",
       " '4': 6,\n",
       " '-': 7,\n",
       " '5': 8,\n",
       " '0': 9,\n",
       " '6': 10,\n",
       " '7': 11,\n",
       " '8': 12,\n",
       " '9': 13,\n",
       " '+': 14}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(100000, 6, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index\n",
    "nr_tokens\n",
    "y_t_output.shape\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:29:03.898523Z",
     "start_time": "2018-05-08T11:29:02.659498Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_gru = L.Bidirectional(\n",
    "    L.GRU(LATENT_DIM // 2, dropout=DROPOUT, return_state=True, name='encoder_gru'),\n",
    "    name='encoder_bidirectional'\n",
    ")\n",
    "decoder_gru = L.GRU(LATENT_DIM, dropout=DROPOUT, return_sequences=True, return_state=True, name='decoder_gru')\n",
    "decoder_dense = L.Dense(nr_tokens, activation='softmax', name='decoder_outputs')\n",
    "\n",
    "shared_embedding = L.Embedding(nr_tokens, EMBEDDING_DIM, mask_zero=True, name='shared_embedding')\n",
    "\n",
    "encoder_inputs = L.Input(shape=(max_len_input, ), dtype='int32', name='encoder_inputs')\n",
    "encoder_embeddings = shared_embedding(encoder_inputs)\n",
    "_, encoder_state_1, encoder_state_2 = encoder_gru(encoder_embeddings)\n",
    "encoder_states = L.concatenate([encoder_state_1, encoder_state_2])\n",
    "\n",
    "decoder_inputs = L.Input(shape=(max_len_target, ), dtype='int32', name='decoder_inputs')\n",
    "decoder_mask = L.Masking(mask_value=0)(decoder_inputs)\n",
    "decoder_embeddings_inputs = shared_embedding(decoder_mask)\n",
    "decoder_embeddings_outputs, _ = decoder_gru(decoder_embeddings_inputs, initial_state=encoder_states) \n",
    "decoder_outputs = decoder_dense(decoder_embeddings_outputs)\n",
    "\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "\n",
    "inference_encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "inference_decoder_state_inputs = L.Input(shape=(LATENT_DIM, ), dtype='float32', name='inference_decoder_state_inputs')\n",
    "inference_decoder_embeddings_outputs, inference_decoder_states = decoder_gru(\n",
    "    decoder_embeddings_inputs, initial_state=inference_decoder_state_inputs\n",
    ")\n",
    "inference_decoder_outputs = decoder_dense(inference_decoder_embeddings_outputs)\n",
    "\n",
    "inference_decoder_model = Model(\n",
    "    [decoder_inputs, inference_decoder_state_inputs], \n",
    "    [inference_decoder_outputs, inference_decoder_states]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:29:03.903624Z",
     "start_time": "2018-05-08T11:29:03.899749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 6)            0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_inputs (InputLayer)     (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    multiple             240         encoder_inputs[0][0]             \n",
      "                                                                 masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bidirectional (Bidirect [(None, 512), (None, 419328      shared_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           encoder_bidirectional[0][1]      \n",
      "                                                                 encoder_bidirectional[0][2]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 6, 512), (No 812544      shared_embedding[1][0]           \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 6, 15)        7695        decoder_gru[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,239,807\n",
      "Trainable params: 1,239,807\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 6)            0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    multiple             240         masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "inference_decoder_state_inputs  (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 6, 512), (No 812544      shared_embedding[1][0]           \n",
      "                                                                 inference_decoder_state_inputs[0]\n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 6, 15)        7695        decoder_gru[1][0]                \n",
      "==================================================================================================\n",
      "Total params: 820,479\n",
      "Trainable params: 820,479\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "inference_decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:29:03.944119Z",
     "start_time": "2018-05-08T11:29:03.905593Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(clipnorm=1.), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:34:50.894200Z",
     "start_time": "2018-05-08T11:29:03.945835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "90000/90000 [==============================] - 19s 206us/step - loss: 1.4554 - val_loss: 1.2196\n",
      "Epoch 2/20\n",
      "90000/90000 [==============================] - 17s 187us/step - loss: 0.9733 - val_loss: 0.7145\n",
      "Epoch 3/20\n",
      "90000/90000 [==============================] - 17s 189us/step - loss: 0.5888 - val_loss: 0.4185\n",
      "Epoch 4/20\n",
      "90000/90000 [==============================] - 17s 187us/step - loss: 0.3579 - val_loss: 0.2536\n",
      "Epoch 5/20\n",
      "90000/90000 [==============================] - 17s 189us/step - loss: 0.2269 - val_loss: 0.1678\n",
      "Epoch 6/20\n",
      "90000/90000 [==============================] - 17s 190us/step - loss: 0.1649 - val_loss: 0.1360\n",
      "Epoch 7/20\n",
      "90000/90000 [==============================] - 17s 190us/step - loss: 0.1315 - val_loss: 0.1027\n",
      "Epoch 8/20\n",
      "90000/90000 [==============================] - 18s 197us/step - loss: 0.1119 - val_loss: 0.0948\n",
      "Epoch 9/20\n",
      "90000/90000 [==============================] - 18s 195us/step - loss: 0.0980 - val_loss: 0.0772\n",
      "Epoch 10/20\n",
      "90000/90000 [==============================] - 17s 194us/step - loss: 0.0851 - val_loss: 0.0738\n",
      "Epoch 11/20\n",
      "90000/90000 [==============================] - 17s 191us/step - loss: 0.0767 - val_loss: 0.0759\n",
      "Epoch 12/20\n",
      "90000/90000 [==============================] - 17s 188us/step - loss: 0.0708 - val_loss: 0.0653\n",
      "Epoch 13/20\n",
      "90000/90000 [==============================] - 17s 190us/step - loss: 0.0657 - val_loss: 0.0574\n",
      "Epoch 14/20\n",
      "90000/90000 [==============================] - 17s 189us/step - loss: 0.0630 - val_loss: 0.0534\n",
      "Epoch 15/20\n",
      "90000/90000 [==============================] - 17s 189us/step - loss: 0.0579 - val_loss: 0.0487\n",
      "Epoch 16/20\n",
      "90000/90000 [==============================] - 17s 188us/step - loss: 0.0527 - val_loss: 0.0565\n",
      "Epoch 17/20\n",
      "90000/90000 [==============================] - 17s 189us/step - loss: 0.0514 - val_loss: 0.0491\n",
      "Epoch 18/20\n",
      "90000/90000 [==============================] - 17s 188us/step - loss: 0.0480 - val_loss: 0.0516\n",
      "Epoch 19/20\n",
      "90000/90000 [==============================] - 18s 195us/step - loss: 0.0466 - val_loss: 0.0409\n",
      "Epoch 20/20\n",
      "90000/90000 [==============================] - 18s 199us/step - loss: 0.0440 - val_loss: 0.0396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f53f81c9ba8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, x_t_input], y_t_output, validation_split=0.1, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:34:50.899016Z",
     "start_time": "2018-05-08T11:34:50.895440Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = inference_encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, max_len_target))\n",
    "    target_seq[0, 0] = tokenizer.word_index[START]\n",
    "    \n",
    "    tokens = {idx: token for (token, idx) in tokenizer.word_index.items()}\n",
    "    \n",
    "    decoded_sequence = ''\n",
    "    for i in range(max_len_target):\n",
    "        output_tokens, output_states = inference_decoder_model.predict(\n",
    "            [target_seq, states_value]\n",
    "        )\n",
    "        \n",
    "        # greedy search\n",
    "        sampled_token_idx = np.argmax(output_tokens[0, 0, :])\n",
    "        sampled_token = tokens.get(sampled_token_idx, '.')\n",
    "        if sampled_token == END:\n",
    "            break\n",
    "        decoded_sequence += sampled_token\n",
    "            \n",
    "        target_seq[0, 0] = sampled_token_idx\n",
    "        states_value = output_states\n",
    "    \n",
    "    return decoded_sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:34:50.908353Z",
     "start_time": "2018-05-08T11:34:50.900462Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(equation):\n",
    "    return decode_sequence(keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences([equation]),\n",
    "        padding='post',\n",
    "        maxlen=X.shape[1]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:34:51.358076Z",
     "start_time": "2018-05-08T11:34:50.910269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1$=got: 2, exp: 2\n",
      "9+11$=got: 20, exp: 20\n",
      "21+34$=got: 55, exp: 55\n",
      "359+468$=got: 827, exp: 827\n",
      "1359+468$=got: 1827, exp: 1827\n",
      "1-1$=got: 0, exp: 0\n",
      "19-1$=got: 18, exp: 18\n",
      "34-359$=got: -325, exp: -325\n",
      "11359-1468$=got: -19, exp: 9891\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "for calc in [eq + '$' for eq in ['1+1', '9+11', '21+34', '359+468', '1359+468', '1-1', '19-1', '34-359', '11359-1468']]:\n",
    "    print(f\"{calc}=got: {predict(calc)}, exp: {eval(calc[:-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:34:51.536121Z",
     "start_time": "2018-05-08T11:34:51.359390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30-209$=got: -179, exp: -179\n",
      "6350+127$=got: 6477, exp: 6477\n",
      "846-24$=got: 822, exp: 822\n",
      "247-92$=got: 155, exp: 155\n",
      "3+27$=got: 30, exp: 30\n",
      "3-427$=got: -424, exp: -424\n",
      "0-2187$=got: -2187, exp: -2187\n",
      "2914-1403$=got: 1511, exp: 1511\n",
      "252+22$=got: 274, exp: 274\n",
      "678-108$=got: 570, exp: 570\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for calc in df.input_texts[:10].tolist():\n",
    "    print(f\"{calc}=got: {predict(calc)}, exp: {eval(calc[:-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T11:35:05.967529Z",
     "start_time": "2018-05-08T11:34:51.537482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 10.071\n"
     ]
    }
   ],
   "source": [
    "# Mean average error on a test set\n",
    "test_df = create_equations_df(size=1000)\n",
    "test_df['y_pred'] = test_df.input_texts.apply(predict).astype(int)\n",
    "test_df['y_true'] = test_df.result\n",
    "print(\"MAE\", np.mean(np.abs(test_df.y_pred - test_df.y_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It doesn't work perfect, but fine enough to show that seq2seq works in some way. I wouldn't be surprised if the mean average error is better than average human bias for calculating without any tools.\n",
    "For improvements and further discussions I'll move to a real problem (translating) and main steps will be:\n",
    "* Bytepairencoding/Word embeddings\n",
    "* Beam Search\n",
    "* Attention models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
