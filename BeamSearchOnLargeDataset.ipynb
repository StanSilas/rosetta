{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on large dataset with already implemented BeamSearch but without attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After [implementing Beamsearch](BeamSearchForMachineTranslation.ipynb), I'll now train it on a large dataset. The goal is beside a better translation quality also to show problems arising without attention model (that is needed for larger texts). \n",
    "As trainings set I use the [European Parliament Proceedings Parallel Corpus 1996-2011](http://statmt.org/europarl/) German-English corpus with medium sized sentences.\n",
    "\n",
    "Again, I'll refactor the code a bit, putting most of the implementation details into modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:01:39.461769Z",
     "start_time": "2018-05-24T09:01:37.425219Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    " \n",
    "import bytepairencoding as bpe\n",
    "import seq2seq\n",
    "from utils.download import download_and_extract_resources\n",
    "from utils.linguistic import bleu_scores_europarl, read_europarl, preprocess_input_europarl as preprocess\n",
    "\n",
    "\n",
    "# Fixing random state ensure reproducible results\n",
    "RANDOM_STATE=42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "pd.set_option('max_colwidth', 60)  # easier to read texts in e.g. df.head()\n",
    "\n",
    "# technical detail so that an instance (maybe running in a different window)\n",
    "# doesn't take all the GPU memory resulting in some strange error messages\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:01:39.465384Z",
     "start_time": "2018-05-24T09:01:39.462999Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 100  # was 50\n",
    "MAX_TARGET_LENGTH = 125  # was 65\n",
    "LATENT_DIM = 512\n",
    "EMBEDDING_DIM = 300\n",
    "BPE_MERGE_OPERATIONS = 5_000  # I'd love to use 10_000 x 300, but this one is broken: https://github.com/bheinzerling/bpemb/issues/6\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32  # was 64, but need to reduced so a batch still fits in GPU memory\n",
    "DROPOUT = 0.5\n",
    "TEST_SIZE = 2_500  # was 500\n",
    "EMBEDDING_TRAINABLE = True  # Improves results significant and for at least it's not the most dominant training time factor (that's the output softmax layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:01:39.473087Z",
     "start_time": "2018-05-24T09:01:39.466737Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = 'data'\n",
    "INPUT_LANG = 'en'\n",
    "TARGET_LANG = 'de'\n",
    "LANGUAGES = [INPUT_LANG, TARGET_LANG]\n",
    "BPE_URL = {lang: f'http://cosyne.h-its.org/bpemb/data/{lang}/' for lang in LANGUAGES}\n",
    "BPE_MODEL_NAME = {lang: f'{lang}.wiki.bpe.op{BPE_MERGE_OPERATIONS}.model' for lang in LANGUAGES}\n",
    "BPE_WORD2VEC_NAME = {lang: f'{lang}.wiki.bpe.op{BPE_MERGE_OPERATIONS}.d{EMBEDDING_DIM}.w2v.bin' for lang in LANGUAGES}\n",
    "\n",
    "EXTERNAL_RESOURCES = {\n",
    "    # Europarl Corpus\n",
    "    'de-en.tgz': 'http://statmt.org/europarl/v7/de-en.tgz',\n",
    "    \n",
    "    # Bytepairencoding subwords (_MODEL_) and pretrained embeddings (_WORD2VEC_)\n",
    "    BPE_MODEL_NAME[INPUT_LANG]: f'{BPE_URL[INPUT_LANG]}/{BPE_MODEL_NAME[INPUT_LANG]}',\n",
    "    BPE_WORD2VEC_NAME[INPUT_LANG] + '.tar.gz': f'{BPE_URL[INPUT_LANG]}/{BPE_WORD2VEC_NAME[INPUT_LANG]}' + '.tar.gz',\n",
    "    BPE_MODEL_NAME[TARGET_LANG]: f'{BPE_URL[TARGET_LANG]}/{BPE_MODEL_NAME[TARGET_LANG]}',\n",
    "    BPE_WORD2VEC_NAME[TARGET_LANG] + '.tar.gz': f'{BPE_URL[TARGET_LANG]}/{BPE_WORD2VEC_NAME[TARGET_LANG]}' + '.tar.gz',\n",
    "}\n",
    "\n",
    "download_and_extract_resources(fnames_and_urls=EXTERNAL_RESOURCES, dest_path=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:01:41.462690Z",
     "start_time": "2018-05-24T09:01:39.474327Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\n",
    "    'input_texts': read_europarl(INPUT_LANG),\n",
    "    'target_texts': read_europarl(TARGET_LANG)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:01:42.368262Z",
     "start_time": "2018-05-24T09:01:41.465504Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Nr total input:\", len(df))\n",
    "df['input_length'] = df.input_texts.apply(len)\n",
    "df['target_length'] = df.target_texts.apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter translations (only sentences shorter than a given length)\n",
    "\n",
    "With a full working machine translation system, it's of course better to train on all data (plus maybe some augmented data). Without attention (and maybe copy mechanism, dynamic memory, ...) there's no point anyway in it, but it also reduces training time (a full training on ~2 Mio translations might take days, even with a good GPU).\n",
    "I use different length for input (english) than target (german) language as german is more verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:01:42.512261Z",
     "start_time": "2018-05-24T09:01:42.370425Z"
    }
   },
   "outputs": [],
   "source": [
    "non_empty = (df.input_length > 1) & (df.target_length > 1)  # there are empty phrases like '\\n' --> 'Frau Präsidentin\\n'\n",
    "short_inputs = (df.input_length < MAX_INPUT_LENGTH) & (df.target_length < MAX_TARGET_LENGTH)\n",
    "print(f'Sentences with length between (1, input={MAX_INPUT_LENGTH}/target={MAX_TARGET_LENGTH}) characters:', sum(non_empty & short_inputs))\n",
    "df = df[non_empty & short_inputs]\n",
    "gc.collect();  # df with filtered sentences is significant smaller, so time to garbage collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load (pretrained) Bytepairs\n",
    "\n",
    "I need the subwords dictionary (in `BPE_WORD2VEC_NAME`), the pretrained embeddings (in `BPE_MODEL_NAME`) and a [sentencepiece](https://github.com/google/sentencepiece) handler that can encode/decode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:01:42.635198Z",
     "start_time": "2018-05-24T09:01:42.513941Z"
    }
   },
   "outputs": [],
   "source": [
    "bpe_input, bpe_target = [bpe.Bytepairencoding(\n",
    "    word2vec_fname=os.path.join(PATH, BPE_WORD2VEC_NAME[lang]),\n",
    "    sentencepiece_fname=os.path.join(PATH, BPE_MODEL_NAME[lang]),\n",
    ") for lang in [INPUT_LANG, TARGET_LANG]] \n",
    "print(\"English subwords\", bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", bpe_input.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:01:42.731815Z",
     "start_time": "2018-05-24T09:01:42.636400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now encode the texts into sequences of indexes of bytepairs\n",
    "df['input_sequences'] = df.input_texts.apply(bpe_input.subword_indices)\n",
    "df['target_sequences'] = df.target_texts.apply(bpe_target.subword_indices)\n",
    "df[['input_sequences', 'target_sequences']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:01:42.742141Z",
     "start_time": "2018-05-24T09:01:42.733923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Those will be the inputs for the seq2seq model (that needs to know how long the sequences can get)\n",
    "max_len_input = df.input_sequences.apply(len).max()\n",
    "max_len_target = df.target_sequences.apply(len).max()\n",
    "(max_len_input, max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:01:42.752108Z",
     "start_time": "2018-05-24T09:01:42.744193Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(df.shape[0]), test_size=0.1, random_state=RANDOM_STATE)  # fixed random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:02:35.764190Z",
     "start_time": "2018-05-24T09:01:42.753872Z"
    }
   },
   "outputs": [],
   "source": [
    "s2s = seq2seq.Seq2SeqWithBPE(\n",
    "    bpe_input=bpe_input,\n",
    "    bpe_target=bpe_target,\n",
    "    max_len_input=max_len_input,\n",
    "    max_len_target=max_len_target\n",
    ")\n",
    "s2s.model.compile(optimizer=keras.optimizers.Adam(clipnorm=1., clipvalue=.5), loss='categorical_crossentropy')\n",
    "train_generator = s2s.create_batch_generator(train_ids, df.input_sequences, df.target_sequences, BATCH_SIZE)\n",
    "val_generator = s2s.create_batch_generator(val_ids, df.input_sequences, df.target_sequences, BATCH_SIZE)\n",
    "\n",
    "s2s.model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(len(train_ids) / BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=np.ceil(len(val_ids) / BATCH_SIZE),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:02:36.013155Z",
     "start_time": "2018-05-24T09:02:35.767613Z"
    }
   },
   "outputs": [],
   "source": [
    "name = 'beamsearchlarge'\n",
    "s2s.model.save_weights(f'data/{name}_model_weights.h5')\n",
    "s2s.inference_encoder_model.save_weights(f'data/{name}_inference_encoder_model_weights.h5')\n",
    "s2s.inference_decoder_model.save_weights(f'data/{name}_inference_decoder_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:02:36.019776Z",
     "start_time": "2018-05-24T09:02:36.015570Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence, beam_width=5):\n",
    "    return s2s.decode_beam_search(pad_sequences(\n",
    "        [bpe_input.subword_indices(preprocess(sentence))],\n",
    "        padding='post',\n",
    "        maxlen=max_len_input,\n",
    "    ), beam_width=beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:02:37.355602Z",
     "start_time": "2018-05-24T09:02:36.021893Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:02:38.598590Z",
     "start_time": "2018-05-24T09:02:37.356850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performance on training set:\n",
    "for en, de in df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:02:39.628279Z",
     "start_time": "2018-05-24T09:02:38.599919Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performance on validation set\n",
    "val_df = df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T09:02:52.944442Z",
     "start_time": "2018-05-24T09:02:39.629448Z"
    }
   },
   "outputs": [],
   "source": [
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    target_texts=df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The texts feel more readable, allthough the BLEU score rises up only a bit ($0.328 > $0.316$).\n",
    "A lot of the problems in the translations certainly depend on the still small training set (~200k), so as next step, I'll train on a bigger sub-corpus of longer texts. This will also make the need to use an attention model more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 294,
   "position": {
    "height": "40px",
    "left": "553px",
    "right": "192px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
