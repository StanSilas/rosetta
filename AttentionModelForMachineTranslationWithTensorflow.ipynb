{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on large dataset with attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing [Beamsearch on a large dataset](BeamSearchOnLargeDataset.ipynb), I'll now add an attention model.\n",
    "As trainings set I use the [European Parliament Proceedings Parallel Corpus 1996-2011](http://statmt.org/europarl/).\n",
    "\n",
    "I first intented to implement it also with `Keras`. First of all, there is no built-in implementation of an attention layer or an attention decoder (it's planned atm). There are several projects like [keras-attention](https://github.com/datalogue/keras-attention) or a bit modified [monitonic-keras-attention](https://github.com/andreyzharkov/keras-monotonic-attention) (that works really better). Also there is [seq2seq project](https://github.com/farizrahman4u/seq2seq) that as lot of issues open. And a promising looking [NMT Keras](https://nmt-keras.readthedocs.io/en/latest/) that failed to install all dependency. I wouldn't mind a reimplemetation on my own (or improving one of these), just as I do the project anyway for learning purposes. After a while I really found this approach disturbing. I don't like switching around between the real high level layers of `Keras` down to `keras.backend` when I pretty much have to low level implement everything (in a different way to usual `Keras`) and in addition everything in object oriented extension of Layers, Cell and so on (with passing all parameters along, as GRU/LSTM have to be changed they also have to be reimplementend, vectorization with own time distributed layers, the masking layer won't work with further inputs like the weighted context vector, so we'll implement also Masking, ...). Just look into the projects and there is a lot of noisy code inside detracting from the original algorithm. \n",
    "\n",
    "Attention as basic idea is pretty simple: While decoding, we'll look back the weighted encoded states that depend of the current decoding position, the previous (or current) hidden state of the decoder, and maybe to the previous alignment. We create a contect vector of them and use it also as an input (beside the last generated token) to the decoder. For performance we might only look to local hidden states around an alignment prediction (that can linear in the simplest form or usually als learnable). That's not so tough to represent as direct computation, but as we never work in Keras with the computation graph directly, it's harder than it should be. \n",
    "\n",
    "With tensorflow we're closer to research and as I anyway intended to use multiple frameworks, I'll follow now the [seq2seq tutorial from tensorflow](https://www.tensorflow.org/tutorials/seq2seq). So, in this notebook there will be also a tensorflow implementation of the raw seq2seq model and Beam Search. I also refactored all the preparation work into a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T08:56:00.779511Z",
     "start_time": "2018-06-16T08:55:59.493902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4477692514215284661\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7793544397\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6673719624239460398\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T08:56:00.798558Z",
     "start_time": "2018-06-16T08:56:00.780789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T08:56:01.503138Z",
     "start_time": "2018-06-16T08:56:00.800234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed random seed to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils.download import download_and_extract_resources\n",
    "from utils.linguistic import bleu_scores_europarl, preprocess_input_europarl as preprocess\n",
    "from utils.preparation import Europarl, RANDOM_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T09:51:27.783694Z",
     "start_time": "2018-06-16T09:51:27.775324Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 20 #100  # was 50\n",
    "MAX_TARGET_LENGTH = 25 # 125  # was 65\n",
    "LATENT_DIM =  512 # 256  # was 512, but we should be able to use a smaller hidden representation as we are looking back anyway as needed\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "DROPOUT = 0.5\n",
    "TEST_SIZE = 2500 # 2_500  \n",
    "BEAM_WIDTH = 5\n",
    "EMBEDDING_TRAINABLE = True  # Improves results significant and for at least it's not the most dominant training time factor (that's the output softmax layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T08:56:01.512398Z",
     "start_time": "2018-06-16T08:56:01.507923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n"
     ]
    }
   ],
   "source": [
    "europarl = Europarl()\n",
    "download_and_extract_resources(fnames_and_urls=europarl.external_resources, dest_path=europarl.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T08:56:05.461932Z",
     "start_time": "2018-06-16T08:56:01.513594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unfiltered translations 1920209\n",
      "Filtered translations with length between (1, input=20/target=25) characters: 14943\n"
     ]
    }
   ],
   "source": [
    "europarl.load_and_preprocess(max_input_length=MAX_INPUT_LENGTH, max_target_length=MAX_TARGET_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T08:56:05.482929Z",
     "start_time": "2018-06-16T08:56:05.463252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "      <th>input_sequences</th>\n",
       "      <th>target_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>agenda</td>\n",
       "      <td>arbeitsplan</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>[1, 631, 222, 34, 2]</td>\n",
       "      <td>[1, 941, 197, 3454, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>what is the result?</td>\n",
       "      <td>was sind die folgen?</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>[1, 781, 14, 3, 714, 2426, 2]</td>\n",
       "      <td>[1, 748, 126, 6, 2374, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>with what aim?</td>\n",
       "      <td>zu welchem zweck?</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>[1, 23, 781, 2973, 2426, 2]</td>\n",
       "      <td>[1, 26, 2740, 156, 155, 142, 359, 188, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>why?</td>\n",
       "      <td>wieso?</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>[1, 958, 38, 2426, 2]</td>\n",
       "      <td>[1, 167, 1659, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>no.</td>\n",
       "      <td>nein.</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, 220, 5, 2]</td>\n",
       "      <td>[1, 124, 191, 3, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              input_texts          target_texts  input_length  target_length  \\\n",
       "67                 agenda           arbeitsplan             6             11   \n",
       "704   what is the result?  was sind die folgen?            19             20   \n",
       "1261       with what aim?     zu welchem zweck?            14             17   \n",
       "1401                 why?                wieso?             4              6   \n",
       "1403                  no.                 nein.             3              5   \n",
       "\n",
       "                    input_sequences  \\\n",
       "67             [1, 631, 222, 34, 2]   \n",
       "704   [1, 781, 14, 3, 714, 2426, 2]   \n",
       "1261    [1, 23, 781, 2973, 2426, 2]   \n",
       "1401          [1, 958, 38, 2426, 2]   \n",
       "1403                 [1, 220, 5, 2]   \n",
       "\n",
       "                                     target_sequences  \n",
       "67                             [1, 941, 197, 3454, 2]  \n",
       "704                   [1, 748, 126, 6, 2374, 3720, 2]  \n",
       "1261  [1, 26, 2740, 156, 155, 142, 359, 188, 3720, 2]  \n",
       "1401                          [1, 167, 1659, 3720, 2]  \n",
       "1403                              [1, 124, 191, 3, 2]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T08:56:05.489927Z",
     "start_time": "2018-06-16T08:56:05.484993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English subwords ['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n",
      "German subwords ['▁das', '▁ist', '▁ein', '▁test', '▁für', '▁v', 'ort', 'rain', 'ierte', '▁zeich', 'eng', 'ruppen']\n"
     ]
    }
   ],
   "source": [
    "print(\"English subwords\", europarl.bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", europarl.bpe_target.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T08:56:05.504749Z",
     "start_time": "2018-06-16T08:56:05.491489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Those will be the inputs for the seq2seq model (that needs to know how long the sequences can get)\n",
    "max_len_input = europarl.df.input_sequences.apply(len).max()\n",
    "max_len_target = europarl.df.target_sequences.apply(len).max()\n",
    "(max_len_input, max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T09:37:15.821549Z",
     "start_time": "2018-06-16T09:37:15.818047Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(europarl.df.shape[0]), test_size=0.1, random_state=RANDOM_STATE)  # fixed random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T10:09:41.130712Z",
     "start_time": "2018-06-16T10:09:39.129396Z"
    }
   },
   "outputs": [],
   "source": [
    "TIME_MAJOR = False\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    encoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_input\n",
    "        dtype=tf.int32,\n",
    "        name='encoder_inputs' \n",
    "    )\n",
    "    batch_size = tf.shape(encoder_inputs)[0]\n",
    "    \n",
    "    dropout = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", \n",
    "        initializer=tf.constant(europarl.bpe_input.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder,\n",
    "        encoder_inputs,\n",
    "        name=\"encoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    input_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='input_sequence_length'\n",
    "    )\n",
    "    \n",
    "    rnn_cell_type = tf.nn.rnn_cell.GRUCell\n",
    "    encoder_forward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name='encoder_forward_cell'),\n",
    "        input_keep_prob=dropout,\n",
    "        output_keep_prob=dropout,\n",
    "        state_keep_prob=dropout,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    encoder_backward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name='encoder_backward_cell'),\n",
    "        input_keep_prob=dropout,\n",
    "        output_keep_prob=dropout,\n",
    "        state_keep_prob=dropout,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    encoder_bi_outputs, encoder_bi_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        encoder_forward_cell, encoder_backward_cell,\n",
    "        inputs=encoder_emb_inp,\n",
    "        sequence_length=input_sequence_length,\n",
    "        time_major=TIME_MAJOR,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    encoder_outputs = tf.concat(encoder_bi_outputs, -1)\n",
    "    encoder_state = tf.concat(encoder_bi_state, -1)\n",
    "    \n",
    "    # Regarding time_major:\n",
    "    # If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "    # If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "    # Using `time_major = True` is a bit more efficient because it avoids\n",
    "    # transposes at the beginning and end of the RNN calculation.  However,\n",
    "    # most TensorFlow data is batch-major, so by default this function\n",
    "    # accepts input and emits output in batch-major form.\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_inputs' \n",
    "    )\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", \n",
    "        initializer=tf.constant(europarl.bpe_target.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder,\n",
    "        decoder_inputs,\n",
    "        name=\"decoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='target_sequence_length'\n",
    "    )\n",
    "    decoder_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM, name='decoder_cell'),\n",
    "        input_keep_prob=dropout,\n",
    "        output_keep_prob=dropout,\n",
    "        state_keep_prob=dropout,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        inputs=decoder_emb_inp, \n",
    "        sequence_length=target_sequence_length,\n",
    "        time_major=TIME_MAJOR,\n",
    "        name=\"decoder_training_helper\",\n",
    "    )\n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        units=len(europarl.bpe_target.tokens),\n",
    "        use_bias=False,\n",
    "        name='projection_layer',\n",
    "    )\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=decoder_cell,\n",
    "        helper=training_helper,\n",
    "        initial_state=encoder_state,\n",
    "        output_layer=projection_layer,\n",
    "    )\n",
    "    outputs, _final_state, _final_sequence_length = tf.contrib.seq2seq.dynamic_decode(  \n",
    "        decoder,\n",
    "        output_time_major=TIME_MAJOR,\n",
    "        impute_finished=True,\n",
    "        # swap_memory=True,\n",
    "    )\n",
    "    logits = outputs.rnn_output\n",
    "    \n",
    "    decoder_outputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_outputs',\n",
    "    )\n",
    "    # crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    #     labels=decoder_outputs,\n",
    "    #     logits=logits\n",
    "    # )\n",
    "    target_weights = tf.cast(tf.sequence_mask(target_sequence_length), dtype=tf.float32)\n",
    "    #train_loss = (tf.reduce_sum(crossent * target_weights) / BATCH_SIZE)\n",
    "    train_loss = tf.contrib.seq2seq.sequence_loss(logits, decoder_outputs, target_weights)\n",
    "\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "        t_list=gradients,\n",
    "        clip_norm=1.,\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "    \n",
    "    # inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "    #     embedding=embedding_decoder,\n",
    "    #     start_tokens=tf.fill([batch_size], europarl.bpe_target.start_token_idx),\n",
    "    #     end_token=europarl.bpe_target.stop_token_idx\n",
    "    # )\n",
    "\n",
    "    # inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    #     cell=decoder_cell,\n",
    "    #     helper=inference_helper,\n",
    "    #     initial_state=encoder_state,\n",
    "    #     output_layer=projection_layer,\n",
    "    # )\n",
    "    \n",
    "    inference_decoder_initial_state = tf.contrib.seq2seq.tile_batch(\n",
    "        encoder_state,\n",
    "        multiplier=BEAM_WIDTH,\n",
    "        name='inference_decoder_initital_state',\n",
    "    )\n",
    "\n",
    "    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "        cell=decoder_cell,\n",
    "        embedding=embedding_decoder,\n",
    "        start_tokens=tf.fill([batch_size], europarl.bpe_target.start_token_idx),\n",
    "        end_token=europarl.bpe_target.stop_token_idx,\n",
    "        initial_state=inference_decoder_initial_state,\n",
    "        beam_width=BEAM_WIDTH,\n",
    "        output_layer=projection_layer,\n",
    "        length_penalty_weight=0.0,  # TODO: check hyperparameter tuning\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    inference_outputs, _inference_final_state, _inference_final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inference_decoder,\n",
    "        maximum_iterations=tf.round(tf.reduce_max(input_sequence_length) * 2),  # a bit more flexible than max_len_target\n",
    "        swap_memory=True,\n",
    "    )\n",
    "    # translations = inference_outputs.sample_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T10:09:41.499388Z",
     "start_time": "2018-06-16T10:09:41.489481Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_train_batch(batch_ids):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    pred, loss, _ = sess.run(\n",
    "        fetches=[\n",
    "            outputs, train_loss, update_step\n",
    "        ],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "            dropout: DROPOUT,\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def run_val_batch(batch_ids):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    loss = sess.run(\n",
    "        fetches=[train_loss],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def run_validation_loss():\n",
    "    return np.mean([\n",
    "        run_val_batch(ids)\n",
    "        for ids \n",
    "        in np.array_split(val_ids, np.ceil(len(val_ids) / BATCH_SIZE))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T10:12:28.159974Z",
     "start_time": "2018-06-16T10:09:41.875203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fb6d2d2b604878a925a1f21d4adead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 4.0990767 val_loss 2.716789\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06713e9f0f184c4b89e82c86aef66b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.3895214 val_loss 2.1803892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f3d6081ad143d988e89085c5d4d811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.9932975 val_loss 1.9526802\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f97e12ff5c643588df623866cefeed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.7791237 val_loss 1.8376094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b250b24e03464e10bfb00a5e2ae067f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.6382072 val_loss 1.7498192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa255be49a640b29cd96832e2003611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.5293925 val_loss 1.7048975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a358afda369466c9d64de7c3738e5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.4433974 val_loss 1.6530296\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59fc79ea09b4d21b05e453feb590b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.3728479 val_loss 1.633344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee659df3157453893fafb0592890321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.318255 val_loss 1.6181208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e0edbea6074e6ca10aa725f9fa9088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 10', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.2689469 val_loss 1.6091264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8183d1e0764e679568fafc0a842f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 11', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.2280858 val_loss 1.5922742\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d8343980e34a2bbf6f9114c7f197dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 12', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.2007769 val_loss 1.5943135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4338e010b14d6a81f0e3dec452e515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 13', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.1689807 val_loss 1.5994959\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac2de4e49b54cc1a3febbb0bb7459a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 14', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.1382523 val_loss 1.5958112\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb59ed17259c4fe5b9acf663624e3f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 15', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.1142709 val_loss 1.5867953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d96fc4f5724862ab09a4bdb71259fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 16', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.0952272 val_loss 1.6047292\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae2bf911187462d80519d428fb3ab26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 17', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.0773029 val_loss 1.6094815\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953a8a9649f446b1b6bd0ce96fd34679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 18', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.0617497 val_loss 1.6123999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb14b8b3bbb148f7994c962ea8e9fe86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 19', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.0457625 val_loss 1.6057591\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866d2ad09133406e9a2825945b57bda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 20', max=211), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.0315187 val_loss 1.6124529\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(\n",
    "    allow_soft_placement=True,  # needed as recommendation from https://github.com/tensorflow/tensorflow/issues/2292\n",
    "    log_device_placement=True,\n",
    ")\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batches_per_epoch = np.ceil(len(train_ids) / BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    shuffled_ids = np.random.permutation(train_ids)\n",
    "    batch_splits = np.array_split(shuffled_ids, batches_per_epoch)\n",
    "    train_losses = []\n",
    "    N = len(batch_splits)\n",
    "    with tqdm(batch_splits, desc=f\"Epoch {epoch+1}\") as t:\n",
    "        for train_batch_ids in t:\n",
    "            batch_loss = run_train_batch(train_batch_ids)\n",
    "            train_losses.append(batch_loss)\n",
    "            t.set_postfix(train_loss=np.mean(train_losses))\n",
    "        print(\"train_loss\", np.mean(train_losses), \"val_loss\", run_validation_loss())\n",
    "        \n",
    "validation_input_sequences = europarl.df.input_sequences.iloc[val_ids[:BATCH_SIZE]]\n",
    "validation_input_lengths = validation_input_sequences.apply(len)\n",
    "\n",
    "validation_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    validation_input_sequences,\n",
    "    maxlen=max_len_input,\n",
    "    dtype=int,\n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "# translations = sess.run(\n",
    "#     fetches=[inference_outputs.sample_id],\n",
    "#     feed_dict={\n",
    "#         encoder_inputs: validation_input_padded,\n",
    "#         input_sequence_length: np.array(validation_input_lengths),\n",
    "#     },\n",
    "# )\n",
    "# for input_text, target_text, translation_token_indices in zip(\n",
    "#     europarl.df.input_texts.iloc[val_ids[:BATCH_SIZE]],\n",
    "#     europarl.df.target_texts.iloc[val_ids[:BATCH_SIZE]],\n",
    "#     translations[0]\n",
    "# ):\n",
    "#     translation = europarl.bpe_target.sentencepiece.DecodePieces([\n",
    "#         europarl.bpe_target.tokens[idx] for idx in translation_token_indices\n",
    "#     ])\n",
    "#     print(input_text, target_text, translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T10:22:07.333289Z",
     "start_time": "2018-06-16T10:22:07.310046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1161, 2266, 52, 2584, 2, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'frau präsidentin!'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(sentence):\n",
    "    sequenced = europarl.bpe_input.subword_indices(preprocess(sentence))\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [sequenced],\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    \n",
    "    beam_search_output = sess.run(\n",
    "        fetches=[inference_outputs],\n",
    "        feed_dict={\n",
    "            encoder_inputs: padded,\n",
    "            input_sequence_length: [len(sequenced)],\n",
    "        }\n",
    "    )[0]\n",
    "    \n",
    "    return europarl.bpe_target.sentencepiece.DecodePieces([\n",
    "        europarl.bpe_target.tokens[idx] for idx in beam_search_output.predicted_ids[0, :, 0].tolist()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T09:39:50.274102Z",
     "start_time": "2018-06-16T09:39:50.267871Z"
    }
   },
   "outputs": [],
   "source": [
    "# def create_batch_generator(\n",
    "#     samples_ids, input_sequences, target_sequences, batch_size\n",
    "# ):\n",
    "# \n",
    "#     def batch_generator():\n",
    "#         nr_batches = np.ceil(len(samples_ids) / batch_size)\n",
    "#         while True:\n",
    "#             shuffled_ids = np.random.permutation(samples_ids)\n",
    "#             batch_splits = np.array_split(shuffled_ids, nr_batches)\n",
    "#             for batch_ids in batch_splits:\n",
    "#                 batch_X = pad_sequences(\n",
    "#                     input_sequences.iloc[batch_ids],\n",
    "#                     padding='post',\n",
    "#                     maxlen=max_len_input\n",
    "#                 )\n",
    "#                 batch_y = pad_sequences(\n",
    "#                     target_sequences.iloc[batch_ids],\n",
    "#                     padding='post',\n",
    "#                     maxlen=max_len_target\n",
    "#                 )\n",
    "#                 batch_y_t_output = keras.utils.to_categorical(\n",
    "#                     batch_y[:, 1:],\n",
    "#                     num_classes=len(europarl.bpe_target.tokens)\n",
    "#                 )\n",
    "#                 batch_x_t_input = batch_y[:, :-1]\n",
    "#                 #yield ([batch_X, batch_x_t_input], batch_y_t_output)\n",
    "#                 yield(batch_X, batch_y_t_output)\n",
    "#         \n",
    "#     return batch_generator()\n",
    "# \n",
    "# train_generator = create_batch_generator(\n",
    "#     train_ids, europarl.df.input_sequences, europarl.df.target_sequences, BATCH_SIZE\n",
    "# )\n",
    "# val_generator = create_batch_generator(\n",
    "#     val_ids, europarl.df.input_sequences, europarl.df.target_sequences, BATCH_SIZE\n",
    "# )\n",
    "# \n",
    "# model.fit_generator(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=np.ceil(len(train_ids) / BATCH_SIZE),\n",
    "#     epochs=EPOCHS,\n",
    "#     validation_data=val_generator,\n",
    "#     validation_steps=np.ceil(len(val_ids) / BATCH_SIZE),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T09:54:31.480804Z",
     "start_time": "2018-06-16T09:54:30.782882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/tfattentionmodel.ckpt'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'tfattentionmodel'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, f\"data/{name}.ckpt\")\n",
    "\n",
    "# model.save_weights(f'data/{name}_model_weights.h5') \n",
    "# s2s.model.save_weights(f'data/{name}_model_weights.h5')  # https://drive.google.com/open?id=10Sv-JnAiUT_fvU_cw1_H7mkcTAipC5aA\n",
    "# s2s.inference_encoder_model.save_weights(f'data/{name}_inference_encoder_model_weights.h5')  # https://drive.google.com/open?id=1gNBrn_Wij0PyeE-jJsEnlv7aHXkYuAup\n",
    "# s2s.inference_decoder_model.save_weights(f'data/{name}_inference_decoder_model_weights.h5')  # https://drive.google.com/open?id=1LCU53Hnb4m42QO3qsZTAkyYyroqz2vbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T09:54:33.765408Z",
     "start_time": "2018-06-16T09:54:33.745355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "      <th>input_sequences</th>\n",
       "      <th>target_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>agenda</td>\n",
       "      <td>arbeitsplan</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>[1, 631, 222, 34, 2]</td>\n",
       "      <td>[1, 941, 197, 3454, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>what is the result?</td>\n",
       "      <td>was sind die folgen?</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>[1, 781, 14, 3, 714, 2426, 2]</td>\n",
       "      <td>[1, 748, 126, 6, 2374, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>with what aim?</td>\n",
       "      <td>zu welchem zweck?</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>[1, 23, 781, 2973, 2426, 2]</td>\n",
       "      <td>[1, 26, 2740, 156, 155, 142, 359, 188, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>why?</td>\n",
       "      <td>wieso?</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>[1, 958, 38, 2426, 2]</td>\n",
       "      <td>[1, 167, 1659, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>no.</td>\n",
       "      <td>nein.</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, 220, 5, 2]</td>\n",
       "      <td>[1, 124, 191, 3, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              input_texts          target_texts  input_length  target_length  \\\n",
       "67                 agenda           arbeitsplan             6             11   \n",
       "704   what is the result?  was sind die folgen?            19             20   \n",
       "1261       with what aim?     zu welchem zweck?            14             17   \n",
       "1401                 why?                wieso?             4              6   \n",
       "1403                  no.                 nein.             3              5   \n",
       "\n",
       "                    input_sequences  \\\n",
       "67             [1, 631, 222, 34, 2]   \n",
       "704   [1, 781, 14, 3, 714, 2426, 2]   \n",
       "1261    [1, 23, 781, 2973, 2426, 2]   \n",
       "1401          [1, 958, 38, 2426, 2]   \n",
       "1403                 [1, 220, 5, 2]   \n",
       "\n",
       "                                     target_sequences  \n",
       "67                             [1, 941, 197, 3454, 2]  \n",
       "704                   [1, 748, 126, 6, 2374, 3720, 2]  \n",
       "1261  [1, 26, 2740, 156, 155, 142, 359, 188, 3720, 2]  \n",
       "1401                          [1, 167, 1659, 3720, 2]  \n",
       "1403                              [1, 124, 191, 3, 2]  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T09:57:53.765534Z",
     "start_time": "2018-06-16T09:57:53.560862Z"
    }
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 512 values, but the requested shape has 2560\n\t [[Node: Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat_1, concat_2)]]\n\t [[Node: decoder_1/while/BeamSearchDecoderStep/truediv_1/_527 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_572_decoder_1/while/BeamSearchDecoderStep/truediv_1\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopConstantFolding/decoder/while/BasicDecoderStep/projection_layer/Tensordot/ListDiff-folded-0/_413)]]\n\nCaused by op 'Reshape', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1426, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-90-fdef2bd977f4>\", line 164, in <module>\n    length_penalty_weight=0.0,  # TODO: check hyperparameter tuning\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\", line 334, in __init__\n    self._maybe_split_batch_beams, initial_state, self._cell.state_size)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 375, in map_structure\n    structure[0], [func(*x) for x in entries])\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 375, in <listcomp>\n    structure[0], [func(*x) for x in entries])\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\", line 555, in _maybe_split_batch_beams\n    return self._split_batch_beams(t, s)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\", line 519, in _split_batch_beams\n    0))\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6113, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 512 values, but the requested shape has 2560\n\t [[Node: Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat_1, concat_2)]]\n\t [[Node: decoder_1/while/BeamSearchDecoderStep/truediv_1/_527 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_572_decoder_1/while/BeamSearchDecoderStep/truediv_1\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopConstantFolding/decoder/while/BasicDecoderStep/projection_layer/Tensordot/ListDiff-folded-0/_413)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 512 values, but the requested shape has 2560\n\t [[Node: Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat_1, concat_2)]]\n\t [[Node: decoder_1/while/BeamSearchDecoderStep/truediv_1/_527 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_572_decoder_1/while/BeamSearchDecoderStep/truediv_1\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopConstantFolding/decoder/while/BasicDecoderStep/projection_layer/Tensordot/ListDiff-folded-0/_413)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-6d83917613c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m ]\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0men\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mEXAMPLES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{preprocess(en)!r} --> {predict(en)!r}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-96-faabc69df708>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     12\u001b[0m         feed_dict={\n\u001b[1;32m     13\u001b[0m             \u001b[0mencoder_inputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0minput_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequenced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         }\n\u001b[1;32m     16\u001b[0m     )\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 512 values, but the requested shape has 2560\n\t [[Node: Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat_1, concat_2)]]\n\t [[Node: decoder_1/while/BeamSearchDecoderStep/truediv_1/_527 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_572_decoder_1/while/BeamSearchDecoderStep/truediv_1\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopConstantFolding/decoder/while/BasicDecoderStep/projection_layer/Tensordot/ListDiff-folded-0/_413)]]\n\nCaused by op 'Reshape', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1426, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-90-fdef2bd977f4>\", line 164, in <module>\n    length_penalty_weight=0.0,  # TODO: check hyperparameter tuning\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\", line 334, in __init__\n    self._maybe_split_batch_beams, initial_state, self._cell.state_size)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 375, in map_structure\n    structure[0], [func(*x) for x in entries])\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 375, in <listcomp>\n    structure[0], [func(*x) for x in entries])\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\", line 555, in _maybe_split_batch_beams\n    return self._split_batch_beams(t, s)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\", line 519, in _split_batch_beams\n    0))\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6113, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 512 values, but the requested shape has 2560\n\t [[Node: Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat_1, concat_2)]]\n\t [[Node: decoder_1/while/BeamSearchDecoderStep/truediv_1/_527 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_572_decoder_1/while/BeamSearchDecoderStep/truediv_1\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopConstantFolding/decoder/while/BasicDecoderStep/projection_layer/Tensordot/ListDiff-folded-0/_413)]]\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T09:39:51.259736Z",
     "start_time": "2018-06-16T09:39:51.140123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'what is the result?', got 'was ist das ergebnis?', exp: 'was sind die folgen?'\n",
      "Original 'with what aim?', got 'zu welchem zweck?', exp: 'zu welchem zweck?'\n",
      "Original 'why?', got 'warum?', exp: 'wieso?'\n",
      "Original 'no.', got 'nein.', exp: 'nein.'\n",
      "Original 'just like europol.', got 'genau wie europol.', exp: 'genau wie europol.'\n",
      "Original 'vote', got 'abstimmungen', exp: 'abstimmungen'\n",
      "Original 'why not?', got 'warum nicht?', exp: 'warum?'\n",
      "Original 'and now the erika.', got 'und und es geht um.', exp: 'und nun erika.'\n",
      "Original 'they want answers.', got 'sie wollen antworten.', exp: 'sie wollen antworten.'\n",
      "Original 'storms in europe', got 'stürme in europa', exp: 'stürme in europa'\n",
      "Original 'food safety', got 'lebensmittelsicherheit', exp: 'lebensmittelsicherheit'\n",
      "Original 'first part', got 'teil i', exp: 'teil i'\n",
      "Original 'if not, why not?', got 'wenn nein, warum nicht?', exp: 'wenn nicht, warum nicht?'\n",
      "Original 'second part', got 'teil ii', exp: 'teil ii'\n",
      "Original '0 discharge', got 'entlastungen 0', exp: 'entlastung 0'\n",
      "Original 'that is important.', got 'das ist wichtig.', exp: 'das ist wichtig.'\n",
      "Original 'is this possible?', got 'ist das möglich?', exp: 'kann das sein?'\n",
      "Original 'let us be honest.', got 'seien wir doch ehrlich.', exp: 'seien wir doch ehrlich.'\n",
      "Original 'where is the beef?', got 'wo ist die eib?', exp: 'where is the beef?'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in europarl.df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T09:39:51.384417Z",
     "start_time": "2018-06-16T09:39:51.261133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'yes.', got 'ja.', exp: 'ja.'\n",
      "Original 'why?', got 'warum?', exp: 'warum?'\n",
      "Original '(loud applause)', got '(lebhafter beifall)', exp: '(lebhafter beifall)'\n",
      "Original 'loud applause', got 'lebhafter beifall', exp: 'lebhafter beifall'\n",
      "Original 'why?', got 'warum?', exp: 'warum?'\n",
      "Original 'president.', got 'die präsidentin.', exp: 'die präsidentin.'\n",
      "Original 'consumer protection', got 'verbraucherschutz', exp: 'verbraucherschutz'\n",
      "Original 'biocidal products', got 'biozid-produkte', exp: 'biozid-produkte'\n",
      "Original '(applause)', got '(beifall)', exp: '(beifall)'\n",
      "Original 'applause', got 'beifall', exp: 'beifall'\n",
      "Original 'tempus fugit!', got 'faketien!', exp: 'die zeit drängt!'\n",
      "Original 'hence this debate.', got 'das war unser ziel.', exp: 'deshalb diese debatte.'\n",
      "Original '(applause)', got '(beifall)', exp: '(beifall)'\n",
      "Original 'maes (verts/ale).', got 'maes (verts/ale).', exp: 'maes (verts/ale).'\n",
      "Original '(applause)', got '(beifall)', exp: '(beifall)'\n",
      "Original 'how can this be?', got 'wie ist das möglich?', exp: 'aus welchem grund?'\n",
      "Original '(applause)', got '(beifall)', exp: '(beifall)'\n",
      "Original 'thank you.', got 'vielen dank.', exp: 'danke.'\n",
      "Original 'we were deceived.', got 'wir sind solidarisch.', exp: 'wir wurden betrogen.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = europarl.df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T09:40:01.575226Z",
     "start_time": "2018-06-16T09:39:51.385621Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3668bc8d442742ae990cf346bab4812b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1495), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.3406622219670425\n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 294,
   "position": {
    "height": "40px",
    "left": "553px",
    "right": "192px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
