{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on large dataset with attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing [Beamsearch on a large dataset](BeamSearchOnLargeDataset.ipynb), I'll now add an attention model.\n",
    "As trainings set I use the [European Parliament Proceedings Parallel Corpus 1996-2011](http://statmt.org/europarl/).\n",
    "\n",
    "I first intented to implement it also with `Keras`. First of all, there is no built-in implementation of an attention layer or an attention decoder (it's planned atm). There are several projects like [keras-attention](https://github.com/datalogue/keras-attention) or a bit modified [monitonic-keras-attention](https://github.com/andreyzharkov/keras-monotonic-attention) (that works really better). Also there is [seq2seq project](https://github.com/farizrahman4u/seq2seq) that as lot of issues open. And a promising looking [NMT Keras](https://nmt-keras.readthedocs.io/en/latest/) that failed to install all dependency. I wouldn't mind a reimplemetation on my own (or improving one of these), just as I do the project anyway for learning purposes. After a while I really found this approach disturbing. I don't like switching around between the real high level layers of `Keras` down to `keras.backend` when I pretty much have to low level implement everything (in a different way to usual `Keras`) and in addition everything in object oriented extension of Layers, Cell and so on (with passing all parameters along, as GRU/LSTM have to be changed they also have to be reimplementend, vectorization with own time distributed layers, the masking layer won't work with further inputs like the weighted context vector, so we'll implement also Masking, ...). Just look into the projects and there is a lot of noisy code inside detracting from the original algorithm. \n",
    "\n",
    "Attention as basic idea is pretty simple: While decoding, we'll look back the weighted encoded states that depend of the current decoding position, the previous (or current) hidden state of the decoder, and maybe to the previous alignment. We create a contect vector of them and use it also as an input (beside the last generated token) to the decoder. For performance we might only look to local hidden states around an alignment prediction (that can linear in the simplest form or usually als learnable). That's not so tough to represent as direct computation, but as we never work in Keras with the computation graph directly, it's harder than it should be. \n",
    "\n",
    "With tensorflow we're closer to research and as I anyway intended to use multiple frameworks, I'll follow now the [seq2seq tutorial from tensorflow](https://www.tensorflow.org/tutorials/seq2seq). So, in this notebook there will be also a tensorflow implementation of the raw seq2seq model and Beam Search. I also refactored all the preparation work into a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T09:21:25.945500Z",
     "start_time": "2018-06-15T09:21:24.299649Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed random seed to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "# import seq2seq\n",
    "from utils.download import download_and_extract_resources\n",
    "from utils.linguistic import bleu_scores_europarl, preprocess_input_europarl as preprocess\n",
    "from utils.preparation import Europarl, RANDOM_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T09:21:25.950133Z",
     "start_time": "2018-06-15T09:21:25.947285Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 20 #100  # was 50\n",
    "MAX_TARGET_LENGTH = 25 # 125  # was 65\n",
    "LATENT_DIM = 256  # was 512, but we should be able to use a smaller hidden representation as we are looking back anyway as needed\n",
    "EPOCHS = 5 #20\n",
    "BATCH_SIZE = 32\n",
    "DROPOUT = 0.5\n",
    "TEST_SIZE = 25 # 2_500  \n",
    "EMBEDDING_TRAINABLE = True  # Improves results significant and for at least it's not the most dominant training time factor (that's the output softmax layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T09:21:25.959875Z",
     "start_time": "2018-06-15T09:21:25.951838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n"
     ]
    }
   ],
   "source": [
    "europarl = Europarl()\n",
    "download_and_extract_resources(fnames_and_urls=europarl.external_resources, dest_path=europarl.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T09:21:29.798882Z",
     "start_time": "2018-06-15T09:21:25.961741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unfiltered translations 1920209\n",
      "Filtered translations with length between (1, input=20/target=25) characters: 14943\n"
     ]
    }
   ],
   "source": [
    "europarl.load_and_preprocess(max_input_length=MAX_INPUT_LENGTH, max_target_length=MAX_TARGET_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T09:21:29.823382Z",
     "start_time": "2018-06-15T09:21:29.800384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "      <th>input_sequences</th>\n",
       "      <th>target_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>agenda</td>\n",
       "      <td>arbeitsplan</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>[1, 631, 222, 34, 2]</td>\n",
       "      <td>[1, 941, 197, 3454, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>what is the result?</td>\n",
       "      <td>was sind die folgen?</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>[1, 781, 14, 3, 714, 2426, 2]</td>\n",
       "      <td>[1, 748, 126, 6, 2374, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>with what aim?</td>\n",
       "      <td>zu welchem zweck?</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>[1, 23, 781, 2973, 2426, 2]</td>\n",
       "      <td>[1, 26, 2740, 156, 155, 142, 359, 188, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>why?</td>\n",
       "      <td>wieso?</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>[1, 958, 38, 2426, 2]</td>\n",
       "      <td>[1, 167, 1659, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>no.</td>\n",
       "      <td>nein.</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, 220, 5, 2]</td>\n",
       "      <td>[1, 124, 191, 3, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              input_texts          target_texts  input_length  target_length  \\\n",
       "67                 agenda           arbeitsplan             6             11   \n",
       "704   what is the result?  was sind die folgen?            19             20   \n",
       "1261       with what aim?     zu welchem zweck?            14             17   \n",
       "1401                 why?                wieso?             4              6   \n",
       "1403                  no.                 nein.             3              5   \n",
       "\n",
       "                    input_sequences  \\\n",
       "67             [1, 631, 222, 34, 2]   \n",
       "704   [1, 781, 14, 3, 714, 2426, 2]   \n",
       "1261    [1, 23, 781, 2973, 2426, 2]   \n",
       "1401          [1, 958, 38, 2426, 2]   \n",
       "1403                 [1, 220, 5, 2]   \n",
       "\n",
       "                                     target_sequences  \n",
       "67                             [1, 941, 197, 3454, 2]  \n",
       "704                   [1, 748, 126, 6, 2374, 3720, 2]  \n",
       "1261  [1, 26, 2740, 156, 155, 142, 359, 188, 3720, 2]  \n",
       "1401                          [1, 167, 1659, 3720, 2]  \n",
       "1403                              [1, 124, 191, 3, 2]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T09:21:29.828587Z",
     "start_time": "2018-06-15T09:21:29.824888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English subwords ['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n",
      "German subwords ['▁das', '▁ist', '▁ein', '▁test', '▁für', '▁v', 'ort', 'rain', 'ierte', '▁zeich', 'eng', 'ruppen']\n"
     ]
    }
   ],
   "source": [
    "print(\"English subwords\", europarl.bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", europarl.bpe_target.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T09:21:29.842677Z",
     "start_time": "2018-06-15T09:21:29.830093Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Those will be the inputs for the seq2seq model (that needs to know how long the sequences can get)\n",
    "max_len_input = europarl.df.input_sequences.apply(len).max()\n",
    "max_len_target = europarl.df.target_sequences.apply(len).max()\n",
    "(max_len_input, max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T09:21:29.847949Z",
     "start_time": "2018-06-15T09:21:29.844393Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(europarl.df.shape[0]), test_size=0.1, random_state=RANDOM_STATE)  # fixed random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T09:21:33.852168Z",
     "start_time": "2018-06-15T09:21:29.850034Z"
    }
   },
   "outputs": [],
   "source": [
    "TIME_MAJOR = False\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    encoder_inputs = tf.placeholder(\n",
    "        #shape=(None, max_len_input), \n",
    "        shape=(None, None),  # batch_size x max_len_input\n",
    "        dtype=tf.int32,\n",
    "        name='encoder_inputs' \n",
    "    )\n",
    "\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", \n",
    "        initializer=tf.constant(europarl.bpe_input.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder,\n",
    "        encoder_inputs,\n",
    "        name=\"encoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    input_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='input_sequence_length'\n",
    "    )\n",
    "    encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=LATENT_DIM, name='encoder_cell')\n",
    "    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "        cell=encoder_cell,\n",
    "        inputs=encoder_emb_inp,\n",
    "        sequence_length=input_sequence_length,\n",
    "        time_major=TIME_MAJOR,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    # Regarding time_major:\n",
    "    # If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "    # If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "    # Using `time_major = True` is a bit more efficient because it avoids\n",
    "    # transposes at the beginning and end of the RNN calculation.  However,\n",
    "    # most TensorFlow data is batch-major, so by default this function\n",
    "    # accepts input and emits output in batch-major form.\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(\n",
    "        # shape=(None, max_len_target), \n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_inputs' \n",
    "    )\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", \n",
    "        initializer=tf.constant(europarl.bpe_target.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder,\n",
    "        decoder_inputs,\n",
    "        name=\"decoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='target_sequence_length'\n",
    "    )\n",
    "    decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(\n",
    "        num_units=LATENT_DIM, \n",
    "        name=\"decoder_cell\"\n",
    "    )\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        inputs=decoder_emb_inp, \n",
    "        sequence_length=target_sequence_length,\n",
    "        time_major=TIME_MAJOR,\n",
    "        name=\"decoder_training_helper\",\n",
    "    )\n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        units=len(europarl.bpe_target.tokens),\n",
    "        use_bias=False,\n",
    "        name='projection_layer',\n",
    "    )\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=decoder_cell,\n",
    "        helper=training_helper,\n",
    "        initial_state=encoder_state,\n",
    "        output_layer=projection_layer,\n",
    "    )\n",
    "    \n",
    "    outputs, _final_state, _final_sequence_length = tf.contrib.seq2seq.dynamic_decode(  \n",
    "        decoder,\n",
    "        output_time_major=TIME_MAJOR,\n",
    "        impute_finished=True,\n",
    "        # swap_memory=True,\n",
    "    )\n",
    "    logits = outputs.rnn_output\n",
    "    \n",
    "    decoder_outputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_outputs',\n",
    "    )\n",
    "    # crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    #     labels=decoder_outputs,\n",
    "    #     logits=logits\n",
    "    # )\n",
    "    target_weights = tf.cast(tf.sequence_mask(target_sequence_length), dtype=tf.float32)\n",
    "    #train_loss = (tf.reduce_sum(crossent * target_weights) / BATCH_SIZE)\n",
    "    train_loss = tf.contrib.seq2seq.sequence_loss(logits, decoder_outputs, target_weights)\n",
    "\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "        t_list=gradients,\n",
    "        clip_norm=1.,\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "        embedding=embedding_decoder,\n",
    "        start_tokens=tf.fill([BATCH_SIZE], europarl.bpe_target.start_token_idx),\n",
    "        end_token=europarl.bpe_target.stop_token_idx\n",
    "    )\n",
    "\n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=decoder_cell,\n",
    "        helper=inference_helper,\n",
    "        initial_state=encoder_state,\n",
    "        output_layer=projection_layer,\n",
    "    )\n",
    "    \n",
    "    inference_outputs, _inference_final_state, _inference_final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inference_decoder,\n",
    "        maximum_iterations=tf.round(tf.reduce_max(input_sequence_length) * 2),  # a bit more flexible than max_len_target\n",
    "    )\n",
    "    # translations = inference_outputs.sample_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T09:21:33.862439Z",
     "start_time": "2018-06-15T09:21:33.853498Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_input_sequences = europarl.df.input_sequences[:BATCH_SIZE]\n",
    "batch_input_lengths = batch_input_sequences.apply(len)\n",
    "batch_target_sequences = europarl.df.target_sequences[:BATCH_SIZE]\n",
    "batch_target_lengths = batch_target_sequences.apply(len)-1\n",
    "\n",
    "input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    batch_input_sequences,\n",
    "    maxlen=max_len_input,\n",
    "    dtype=int,\n",
    "    padding='post'\n",
    ")\n",
    "target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    batch_target_sequences,\n",
    "    maxlen=max_len_target,\n",
    "    dtype=int,\n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-15T09:28:20.245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 7.2217135\n",
      "<class 'tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoderOutput'>\n",
      "len translations 1\n",
      "eren\n",
      "agenda arbeitsplan \n",
      "what is the result? was sind die folgen? \n",
      "with what aim? zu welchem zweck? \n",
      "why? wieso? \n",
      "no. nein. \n",
      "just like europol. genau wie europol. \n",
      "vote abstimmungen \n",
      "why not? warum? \n",
      "and now the erika. und nun erika. \n",
      "they want answers. sie wollen antworten. \n",
      "storms in europe stürme in europa \n",
      "food safety lebensmittelsicherheit \n",
      "first part teil i \n",
      "if not, why not? wenn nicht, warum nicht? \n",
      "second part teil ii \n",
      "0 discharge entlastung 0 \n",
      "that is important. das ist wichtig. \n",
      "is this possible? kann das sein? \n",
      "let us be honest. seien wir doch ehrlich. \n",
      "where is the beef? where is the beef? \n",
      "'long-winded' . \"weitschweifig \". \n",
      "such is life. so ist das leben. \n",
      "(applause) (beifall) (\n",
      "i refer to eurodac. es geht um eurodac. \n",
      "capital tax kapitalsteuer \n",
      "we all know that! das ist klar geworden! \n",
      "agenda tagesordnung \n",
      "(applause) (beifall) (\n",
      "if you ratify ... wenn sie ... \n",
      "do forgive me. entschuldigen sie. \n",
      "(applause) (beifall) (\n",
      "(applause) (beifall) (\n",
      "20 4.098965\n",
      "<class 'tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoderOutput'>\n",
      "len translations 1\n",
      "eren\n",
      "agenda arbeitsplan \n",
      "what is the result? was sind die folgen? \n",
      "with what aim? zu welchem zweck? \n",
      "why? wieso? \n",
      "no. nein. \n",
      "just like europol. genau wie europol. \n",
      "vote abstimmungen \n",
      "why not? warum? \n",
      "and now the erika. und nun erika. \n",
      "they want answers. sie wollen antworten. \n",
      "storms in europe stürme in europa \n",
      "food safety lebensmittelsicherheit \n",
      "first part teil i \n",
      "if not, why not? wenn nicht, warum nicht? \n",
      "second part teil ii \n",
      "0 discharge entlastung 0 \n",
      "that is important. das ist wichtig. \n",
      "is this possible? kann das sein? \n",
      "let us be honest. seien wir doch ehrlich. \n",
      "where is the beef? where is the beef? \n",
      "'long-winded' . \"weitschweifig \". \n",
      "such is life. so ist das leben. \n",
      "(applause) (beifall) \n",
      "i refer to eurodac. es geht um eurodac. \n",
      "capital tax kapitalsteuer \n",
      "we all know that! das ist klar geworden! \n",
      "agenda tagesordnung \n",
      "(applause) (beifall) \n",
      "if you ratify ... wenn sie ... \n",
      "do forgive me. entschuldigen sie. \n",
      "(applause) (beifall) \n",
      "(applause) (beifall) \n",
      "30 3.7896094\n",
      "<class 'tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoderOutput'>\n",
      "len translations 1\n",
      "eren\n",
      "agenda arbeitsplan \n",
      "what is the result? was sind die folgen? \n",
      "with what aim? zu welchem zweck? \n",
      "why? wieso? \n",
      "no. nein. \n",
      "just like europol. genau wie europol. \n",
      "vote abstimmungen \n",
      "why not? warum? \n",
      "and now the erika. und nun erika. \n",
      "they want answers. sie wollen antworten. \n",
      "storms in europe stürme in europa \n",
      "food safety lebensmittelsicherheit \n",
      "first part teil i \n",
      "if not, why not? wenn nicht, warum nicht? \n",
      "second part teil ii \n",
      "0 discharge entlastung 0 \n",
      "that is important. das ist wichtig. \n",
      "is this possible? kann das sein? \n",
      "let us be honest. seien wir doch ehrlich. \n",
      "where is the beef? where is the beef? \n",
      "'long-winded' . \"weitschweifig \". \n",
      "such is life. so ist das leben. \n",
      "(applause) (beifall) (\n",
      "i refer to eurodac. es geht um eurodac. \n",
      "capital tax kapitalsteuer \n",
      "we all know that! das ist klar geworden! \n",
      "agenda tagesordnung \n",
      "(applause) (beifall) (\n",
      "if you ratify ... wenn sie ... \n",
      "do forgive me. entschuldigen sie. \n",
      "(applause) (beifall) (\n",
      "(applause) (beifall) (\n",
      "40 3.545245\n",
      "<class 'tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoderOutput'>\n",
      "len translations 1\n",
      "eren\n",
      "agenda arbeitsplan (\n",
      "what is the result? was sind die folgen? das das\n",
      "with what aim? zu welchem zweck? das das\n",
      "why? wieso? das\n",
      "no. nein. \n",
      "just like europol. genau wie europol. das das\n",
      "vote abstimmungen \n",
      "why not? warum? das das\n",
      "and now the erika. und nun erika. das das\n",
      "they want answers. sie wollen antworten. das das das\n",
      "storms in europe stürme in europa (\n",
      "food safety lebensmittelsicherheit (\n",
      "first part teil i \n",
      "if not, why not? wenn nicht, warum nicht? das das das\n",
      "second part teil ii \n",
      "0 discharge entlastung 0 (\n",
      "that is important. das ist wichtig. das das\n",
      "is this possible? kann das sein? das das\n",
      "let us be honest. seien wir doch ehrlich. das das das\n",
      "where is the beef? where is the beef? das das das\n",
      "'long-winded' . \"weitschweifig \". ( ( (\n",
      "such is life. so ist das leben. das das\n",
      "(applause) (beifall) ( (\n",
      "i refer to eurodac. es geht um eurodac. ( ( das\n",
      "capital tax kapitalsteuer \n",
      "we all know that! das ist klar geworden! das das das\n",
      "agenda tagesordnung (\n",
      "(applause) (beifall) ( (\n",
      "if you ratify ... wenn sie ... das das das\n",
      "do forgive me. entschuldigen sie. das das das\n",
      "(applause) (beifall) ( (\n",
      "(applause) (beifall) ( (\n",
      "50 3.2728486\n",
      "<class 'tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoderOutput'>\n",
      "len translations 1\n",
      "eren\n",
      "agenda arbeitsplan (\n",
      "what is the result? was sind die folgen? das das\n",
      "with what aim? zu welchem zweck? das das\n",
      "why? wieso? das\n",
      "no. nein. \n",
      "just like europol. genau wie europol. das das\n",
      "vote abstimmungen \n",
      "why not? warum? das\n",
      "and now the erika. und nun erika. das das\n",
      "they want answers. sie wollen antworten. das das das\n",
      "storms in europe stürme in europa (\n",
      "food safety lebensmittelsicherheit (\n",
      "first part teil i \n",
      "if not, why not? wenn nicht, warum nicht? das das das\n",
      "second part teil ii \n",
      "0 discharge entlastung 0 (\n",
      "that is important. das ist wichtig. das das\n",
      "is this possible? kann das sein? das das\n",
      "let us be honest. seien wir doch ehrlich. das das das\n",
      "where is the beef? where is the beef? das das\n",
      "'long-winded' . \"weitschweifig \". ( (bei\n",
      "such is life. so ist das leben. das das\n",
      "(applause) (beifall) (beifall\n",
      "i refer to eurodac. es geht um eurodac. ( ( (\n",
      "capital tax kapitalsteuer \n",
      "we all know that! das ist klar geworden! das das\n",
      "agenda tagesordnung (\n",
      "(applause) (beifall) (beifall\n",
      "if you ratify ... wenn sie ... das das das\n",
      "do forgive me. entschuldigen sie. ( das das\n",
      "(applause) (beifall) (beifall\n",
      "(applause) (beifall) (beifall\n",
      "60 2.9591331\n",
      "<class 'tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoderOutput'>\n",
      "len translations 1\n",
      "eren\n",
      "agenda arbeitsplan (\n",
      "what is the result? was sind die folgen? das das das.\n",
      "with what aim? zu welchem zweck? das das.\n",
      "why? wieso? das\n",
      "no. nein. das\n",
      "just like europol. genau wie europol. das das.\n",
      "vote abstimmungen \n",
      "why not? warum? das das\n",
      "and now the erika. und nun erika. das das.\n",
      "they want answers. sie wollen antworten. das das das.\n",
      "storms in europe stürme in europa (\n",
      "food safety lebensmittelsicherheit lebensmittel\n",
      "first part teil i \n",
      "if not, why not? wenn nicht, warum nicht? das das das das.\n",
      "second part teil ii \n",
      "0 discharge entlastung 0 ent ent\n",
      "that is important. das ist wichtig. das das.\n",
      "is this possible? kann das sein? das das.\n",
      "let us be honest. seien wir doch ehrlich. das das das.\n",
      "where is the beef? where is the beef? das das das.\n",
      "'long-winded' . \"weitschweifig \". ent ent (bei\n",
      "such is life. so ist das leben. das das.\n",
      "(applause) (beifall) (beifall)\n",
      "i refer to eurodac. es geht um eurodac. ent ent ent das\n",
      "capital tax kapitalsteuer \n",
      "we all know that! das ist klar geworden! das das das.\n",
      "agenda tagesordnung (\n",
      "(applause) (beifall) (beifall)\n",
      "if you ratify ... wenn sie ... das das das.\n",
      "do forgive me. entschuldigen sie. ent das das.\n",
      "(applause) (beifall) (beifall)\n",
      "(applause) (beifall) (beifall)\n",
      "70 2.606614\n",
      "<class 'tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoderOutput'>\n",
      "len translations 1\n",
      "eren\n",
      "agenda arbeitsplan arbeit\n",
      "what is the result? was sind die folgen? das das ist.\n",
      "with what aim? zu welchem zweck? das das?\n",
      "why? wieso? das?\n",
      "no. nein. das\n",
      "just like europol. genau wie europol. ent das wie.\n",
      "vote abstimmungen ab\n",
      "why not? warum? dasum?\n",
      "and now the erika. und nun erika. das das.\n",
      "they want answers. sie wollen antworten. das das ist.\n",
      "storms in europe stürme in europa stür\n",
      "food safety lebensmittelsicherheit lebensmittel\n",
      "first part teil i teil\n",
      "if not, why not? wenn nicht, warum nicht? das das das ist.\n",
      "second part teil ii teil\n",
      "0 discharge entlastung 0 entla\n",
      "that is important. das ist wichtig. das das.\n",
      "is this possible? kann das sein? das das.\n",
      "let us be honest. seien wir doch ehrlich. das das ist.\n",
      "where is the beef? where is the beef? das das..\n",
      "'long-winded' . \"weitschweifig \". ent ent \" \"\n",
      "such is life. so ist das leben. das ist.\n",
      "(applause) (beifall) (beifall)\n",
      "i refer to eurodac. es geht um eurodac. ent ent ent ent\n",
      "capital tax kapitalsteuer kap\n",
      "we all know that! das ist klar geworden! das das ist.\n",
      "agenda tagesordnung arbeit\n",
      "(applause) (beifall) (beifall)\n",
      "if you ratify ... wenn sie ... das das das.\n",
      "do forgive me. entschuldigen sie. ent ent ent sie.\n",
      "(applause) (beifall) (beifall)\n",
      "(applause) (beifall) (beifall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 2.2149692\n",
      "<class 'tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoderOutput'>\n",
      "len translations 1\n",
      "eren\n",
      "agenda arbeitsplan arbeitsp\n",
      "what is the result? was sind die folgen? das das das?\n",
      "with what aim? zu welchem zweck? das das?\n",
      "why? wieso? wie?\n",
      "no. nein. n.\n",
      "just like europol. genau wie europol. gen wie wie.\n",
      "vote abstimmungen ab\n",
      "why not? warum? dasum?\n",
      "and now the erika. und nun erika. und nun.\n",
      "they want answers. sie wollen antworten. das das sie.\n",
      "storms in europe stürme in europa stürme\n",
      "food safety lebensmittelsicherheit lebensmittels\n",
      "first part teil i teil\n",
      "if not, why not? wenn nicht, warum nicht? das das das war?\n",
      "second part teil ii teil\n",
      "0 discharge entlastung 0 entlastung\n",
      "that is important. das ist wichtig. das das.\n",
      "is this possible? kann das sein? das das?\n",
      "let us be honest. seien wir doch ehrlich. das das ist.\n",
      "where is the beef? where is the beef? das das..\n",
      "'long-winded' . \"weitschweifig \". \"weit \" \" \"\n",
      "such is life. so ist das leben. das ist das.\n",
      "(applause) (beifall) (beifall)\n",
      "i refer to eurodac. es geht um eurodac. ent ent ent ent sie.\n",
      "capital tax kapitalsteuer kapital\n",
      "we all know that! das ist klar geworden! das das ist.\n",
      "agenda tagesordnung arbeitsp\n",
      "(applause) (beifall) (beifall)\n",
      "if you ratify ... wenn sie ... das das sie.\n",
      "do forgive me. entschuldigen sie. ent ent ent sie.\n",
      "(applause) (beifall) (beifall)\n",
      "(applause) (beifall) (beifall)\n",
      "90 1.8198322\n",
      "<class 'tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoderOutput'>\n",
      "len translations 1\n",
      "eren\n",
      "agenda arbeitsplan arbeitsp\n",
      "what is the result? was sind die folgen? das das ist?\n",
      "with what aim? zu welchem zweck? zu das?\n",
      "why? wieso? wie?\n",
      "no. nein. nein\n",
      "just like europol. genau wie europol. genau wie europ.\n",
      "vote abstimmungen abstimmungen\n",
      "why not? warum? warum?\n",
      "and now the erika. und nun erika. und nun er.\n",
      "they want answers. sie wollen antworten. w w sie..\n",
      "storms in europe stürme in europa stürme in\n",
      "food safety lebensmittelsicherheit lebensmittelsicher\n",
      "first part teil i teil\n",
      "if not, why not? wenn nicht, warum nicht? wenn das warum nicht?\n",
      "second part teil ii teil teil\n",
      "0 discharge entlastung 0 entlastung\n",
      "that is important. das ist wichtig. das ist.\n",
      "is this possible? kann das sein? das das?\n",
      "let us be honest. seien wir doch ehrlich. das ist das.\n",
      "where is the beef? where is the beef? w we is.\n",
      "'long-winded' . \"weitschweifig \". \"weit \" \" \"ig\n",
      "such is life. so ist das leben. das ist das.\n",
      "(applause) (beifall) (beifall)\n",
      "i refer to eurodac. es geht um eurodac. ent ent ent geht eur.\n",
      "capital tax kapitalsteuer kapitalital\n",
      "we all know that! das ist klar geworden! das ist das.\n",
      "agenda tagesordnung arbeitsp\n",
      "(applause) (beifall) (beifall)\n",
      "if you ratify ... wenn sie ... wenn sie sie.\n",
      "do forgive me. entschuldigen sie. ent ent entigen sie.\n",
      "(applause) (beifall) (beifall)\n",
      "(applause) (beifall) (beifall)\n",
      "100 1.4379731\n",
      "<class 'tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoderOutput'>\n",
      "len translations 1\n",
      "eren\n",
      "agenda arbeitsplan arbeitsplan\n",
      "what is the result? was sind die folgen? das ist das?\n",
      "with what aim? zu welchem zweck? zu welch?\n",
      "why? wieso? wie?\n",
      "no. nein. nein.\n",
      "just like europol. genau wie europol. genau wie europ.\n",
      "vote abstimmungen abstimmungen\n",
      "why not? warum? warum?\n",
      "and now the erika. und nun erika. und nun erika.\n",
      "they want answers. sie wollen antworten. w wollen antw.\n",
      "storms in europe stürme in europa stürme in\n",
      "food safety lebensmittelsicherheit lebensmittelsicherheit\n",
      "first part teil i teil ii\n",
      "if not, why not? wenn nicht, warum nicht? wenn nicht warum nicht?\n",
      "second part teil ii teil ii\n",
      "0 discharge entlastung 0 entlastung 0\n",
      "that is important. das ist wichtig. das ist wichtig.\n",
      "is this possible? kann das sein? das ist?\n",
      "let us be honest. seien wir doch ehrlich. das ist das..\n",
      "where is the beef? where is the beef? where is the.\n",
      "'long-winded' . \"weitschweifig \". \"weit \"fig \".\n",
      "such is life. so ist das leben. das ist das.\n",
      "(applause) (beifall) (beifall)\n",
      "i refer to eurodac. es geht um eurodac. es geht um eurod.\n",
      "capital tax kapitalsteuer kapitalsteuer\n",
      "we all know that! das ist klar geworden! das ist klar?\n",
      "agenda tagesordnung arbeitsplan\n",
      "(applause) (beifall) (beifall)\n",
      "if you ratify ... wenn sie ... wenn sie sie.\n",
      "do forgive me. entschuldigen sie. entschuldigen sie.\n",
      "(applause) (beifall) (beifall)\n",
      "(applause) (beifall) (beifall)\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True)  # needed as recommendation from https://github.com/tensorflow/tensorflow/issues/2292\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(1, 200):\n",
    "    pred, loss, _ = sess.run(\n",
    "        fetches=[\n",
    "            outputs, train_loss, update_step\n",
    "        ],\n",
    "        feed_dict={\n",
    "            encoder_inputs: input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if i % 10 != 0:\n",
    "        continue\n",
    "\n",
    "    print(i, loss)\n",
    "    \n",
    "    translations = sess.run(\n",
    "        fetches=[inference_outputs.sample_id],\n",
    "        feed_dict={\n",
    "            encoder_inputs: input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "        },\n",
    "    )\n",
    "    print(type(inference_outputs))\n",
    "    print(\"len translations\", len(translations))\n",
    "    print(europarl.bpe_target.sentencepiece.DecodeIds([1, 3, 4, 2]))\n",
    "    for input_text, target_text, translation_token_indices in zip(\n",
    "        europarl.df.input_texts[:BATCH_SIZE],\n",
    "        europarl.df.target_texts[:BATCH_SIZE],\n",
    "        translations[0]\n",
    "    ):\n",
    "        translation = europarl.bpe_target.sentencepiece.DecodePieces([\n",
    "            europarl.bpe_target.tokens[idx] for idx in translation_token_indices\n",
    "        ])\n",
    "        print(input_text, target_text, translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T09:10:26.499185Z",
     "start_time": "2018-06-14T09:10:26.495447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10233727731472308209\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7741495706\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5175185302746556953\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T13:01:57.005573Z",
     "start_time": "2018-06-03T12:36:03.612907Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_batch_generator(\n",
    "    samples_ids, input_sequences, target_sequences, batch_size\n",
    "):\n",
    "\n",
    "    def batch_generator():\n",
    "        nr_batches = np.ceil(len(samples_ids) / batch_size)\n",
    "        while True:\n",
    "            shuffled_ids = np.random.permutation(samples_ids)\n",
    "            batch_splits = np.array_split(shuffled_ids, nr_batches)\n",
    "            for batch_ids in batch_splits:\n",
    "                batch_X = pad_sequences(\n",
    "                    input_sequences.iloc[batch_ids],\n",
    "                    padding='post',\n",
    "                    maxlen=max_len_input\n",
    "                )\n",
    "                batch_y = pad_sequences(\n",
    "                    target_sequences.iloc[batch_ids],\n",
    "                    padding='post',\n",
    "                    maxlen=max_len_target\n",
    "                )\n",
    "                batch_y_t_output = keras.utils.to_categorical(\n",
    "                    batch_y[:, 1:],\n",
    "                    num_classes=len(europarl.bpe_target.tokens)\n",
    "                )\n",
    "                batch_x_t_input = batch_y[:, :-1]\n",
    "                #yield ([batch_X, batch_x_t_input], batch_y_t_output)\n",
    "                yield(batch_X, batch_y_t_output)\n",
    "        \n",
    "    return batch_generator()\n",
    "\n",
    "train_generator = create_batch_generator(\n",
    "    train_ids, europarl.df.input_sequences, europarl.df.target_sequences, BATCH_SIZE\n",
    ")\n",
    "val_generator = create_batch_generator(\n",
    "    val_ids, europarl.df.input_sequences, europarl.df.target_sequences, BATCH_SIZE\n",
    ")\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(len(train_ids) / BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=np.ceil(len(val_ids) / BATCH_SIZE),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T13:01:57.258461Z",
     "start_time": "2018-06-03T13:01:57.007278Z"
    }
   },
   "outputs": [],
   "source": [
    "name = 'attentionmodel'\n",
    "# model.save_weights(f'data/{name}_model_weights.h5') \n",
    "s2s.model.save_weights(f'data/{name}_model_weights.h5')  # https://drive.google.com/open?id=10Sv-JnAiUT_fvU_cw1_H7mkcTAipC5aA\n",
    "s2s.inference_encoder_model.save_weights(f'data/{name}_inference_encoder_model_weights.h5')  # https://drive.google.com/open?id=1gNBrn_Wij0PyeE-jJsEnlv7aHXkYuAup\n",
    "s2s.inference_decoder_model.save_weights(f'data/{name}_inference_decoder_model_weights.h5')  # https://drive.google.com/open?id=1LCU53Hnb4m42QO3qsZTAkyYyroqz2vbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T13:07:25.484397Z",
     "start_time": "2018-06-03T13:07:25.457844Z"
    }
   },
   "outputs": [],
   "source": [
    "europarl.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T13:08:49.891047Z",
     "start_time": "2018-06-03T13:08:49.834469Z"
    }
   },
   "outputs": [],
   "source": [
    "p = model.predict(pad_sequences(\n",
    "    [europarl.bpe_input.subword_indices(preprocess('agenda'))],\n",
    "    padding='post',\n",
    "    maxlen=max_len_input\n",
    "), verbose=True)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T13:08:05.439022Z",
     "start_time": "2018-06-03T13:08:05.434562Z"
    }
   },
   "outputs": [],
   "source": [
    "s2s.bpe_target.tokens[np.argmax(p[0, 3, :])]\n",
    "model.call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T08:36:35.781710Z",
     "start_time": "2018-06-03T08:36:29.538Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence, beam_width=5):\n",
    "    return s2s.decode_beam_search(pad_sequences(\n",
    "        [europarl.bpe_input.subword_indices(preprocess(sentence))],\n",
    "        padding='post',\n",
    "        maxlen=max_len_input,\n",
    "    ), beam_width=beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T08:36:35.782283Z",
     "start_time": "2018-06-03T08:36:29.540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T08:36:35.782904Z",
     "start_time": "2018-06-03T08:36:29.542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performance on training set:\n",
    "for en, de in europarl.df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T08:36:35.783442Z",
     "start_time": "2018-06-03T08:36:29.544Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performance on validation set\n",
    "val_df = europarl.df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T08:36:35.783971Z",
     "start_time": "2018-06-03T08:36:29.548Z"
    }
   },
   "outputs": [],
   "source": [
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Translations for short sentences are looking decent. But it's also obvious that for longer sentences the translation gets lost somehow in the sentence and alltough the translated sentence is related to a real translation, it's also confusing and self-repeating.\n",
    "It's worth to notice that the sentences are not too long for a LSTM/GRU model (52, 71) bytepairs for encoding/decoding network. LSTM/GRUs are known to handle sequences up to 100 elements and start decreasing performance at around 60 (for at least that's what the Stanford courses say). So, it could be that a long enough training (we can see here that the training progresses epoch for epoch what's really nice to see for large data) would solve the problem for the choosen sentence lengths here. But of course, it's better to do what humans do also and applicate an attention model instead of trying to keep everything condensed in 512 float32 embedding while also generating bytepair for bytepair.\n",
    "This model is also already a realistic model in terms of training time. I needed around 18h on a GTX1080. Beside implementing attention model, it is tempting to see how a convolutional network might improve the runtime performance (and also quality). But let's get first to Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 294,
   "position": {
    "height": "40px",
    "left": "553px",
    "right": "192px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
