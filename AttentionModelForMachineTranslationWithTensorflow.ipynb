{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on large dataset with attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing [Beamsearch on a large dataset](BeamSearchOnLargeDataset.ipynb), I'll now add an attention model.\n",
    "As trainings set I use the [European Parliament Proceedings Parallel Corpus 1996-2011](http://statmt.org/europarl/).\n",
    "\n",
    "I first intented to implement it also with `Keras`. First of all, there is no built-in implementation of an attention layer or an attention decoder (it's planned atm). There are several projects like [keras-attention](https://github.com/datalogue/keras-attention) or a bit modified [monitonic-keras-attention](https://github.com/andreyzharkov/keras-monotonic-attention) (that works really better). Also there is [seq2seq project](https://github.com/farizrahman4u/seq2seq) that as lot of issues open. And a promising looking [NMT Keras](https://nmt-keras.readthedocs.io/en/latest/) that failed to install all dependency. I wouldn't mind a reimplemetation on my own (or improving one of these), just as I do the project anyway for learning purposes. After a while I really found this approach disturbing. I don't like switching around between the real high level layers of `Keras` down to `keras.backend` when I pretty much have to low level implement everything (in a different way to usual `Keras`) and in addition everything in object oriented extension of Layers, Cell and so on (with passing all parameters along, as GRU/LSTM have to be changed they also have to be reimplementend, vectorization with own time distributed layers, the masking layer won't work with further inputs like the weighted context vector, so we'll implement also Masking, ...). Just look into the projects and there is a lot of noisy code inside detracting from the original algorithm. \n",
    "\n",
    "Attention as basic idea is pretty simple: While decoding, we'll look back the weighted encoded states that depend of the current decoding position, the previous (or current) hidden state of the decoder, and maybe to the previous alignment. We create a contect vector of them and use it also as an input (beside the last generated token) to the decoder. For performance we might only look to local hidden states around an alignment prediction (that can linear in the simplest form or usually als learnable). That's not so tough to represent as direct computation, but as we never work in Keras with the computation graph directly, it's harder than it should be. \n",
    "\n",
    "With tensorflow we're closer to research and as I anyway intended to use multiple frameworks, I'll follow now the [seq2seq tutorial from tensorflow](https://www.tensorflow.org/tutorials/seq2seq). So, in this notebook there will be also a tensorflow implementation of the raw seq2seq model and Beam Search. I also refactored all the preparation work into a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:02:23.364779Z",
     "start_time": "2018-06-16T21:02:22.050054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13627177616749298348\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7808362087\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 18341031548040615031\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:02:23.385141Z",
     "start_time": "2018-06-16T21:02:23.366417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:02:24.085838Z",
     "start_time": "2018-06-16T21:02:23.386780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed random seed to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils.download import download_and_extract_resources\n",
    "from utils.linguistic import bleu_scores_europarl, preprocess_input_europarl as preprocess\n",
    "from utils.preparation import Europarl, RANDOM_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:02:24.089559Z",
     "start_time": "2018-06-16T21:02:24.087122Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 50 #100  # was 50\n",
    "MAX_TARGET_LENGTH = 65 # 125  # was 65\n",
    "LATENT_DIM =  512 # 256  # was 512, but we should be able to use a smaller hidden representation as we are looking back anyway as needed\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT = 0.25\n",
    "TEST_SIZE = 2500\n",
    "BEAM_WIDTH = 5\n",
    "EMBEDDING_TRAINABLE = True  # Improves results significant and for at least it's not the most dominant training time factor (that's the output softmax layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:02:24.095907Z",
     "start_time": "2018-06-16T21:02:24.091075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n"
     ]
    }
   ],
   "source": [
    "europarl = Europarl()\n",
    "download_and_extract_resources(fnames_and_urls=europarl.external_resources, dest_path=europarl.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:02:34.751053Z",
     "start_time": "2018-06-16T21:02:24.097372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unfiltered translations 1920209\n",
      "Filtered translations with length between (1, input=50/target=65) characters: 167211\n"
     ]
    }
   ],
   "source": [
    "europarl.load_and_preprocess(max_input_length=MAX_INPUT_LENGTH, max_target_length=MAX_TARGET_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:02:34.778204Z",
     "start_time": "2018-06-16T21:02:34.752656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "      <th>input_sequences</th>\n",
       "      <th>target_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resumption of the session</td>\n",
       "      <td>wiederaufnahme der sitzungsperiode</td>\n",
       "      <td>25</td>\n",
       "      <td>34</td>\n",
       "      <td>[1, 344, 146, 498, 90, 6, 3, 3235, 90, 2]</td>\n",
       "      <td>[1, 247, 351, 750, 5, 934, 43, 3158, 4762, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>please rise, then, for this minute' s silence.</td>\n",
       "      <td>ich bitte sie, sich zu einer schweigeminute zu...</td>\n",
       "      <td>46</td>\n",
       "      <td>55</td>\n",
       "      <td>[1, 3005, 416, 77, 359, 4, 241, 4, 17, 76, 451...</td>\n",
       "      <td>[1, 241, 156, 72, 3112, 54, 4, 39, 26, 95, 473...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(the house rose and observed a minute' s silence)</td>\n",
       "      <td>(das parlament erhebt sich zu einer schweigemi...</td>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "      <td>[1, 29, 140, 414, 3231, 8, 3106, 2484, 9, 451,...</td>\n",
       "      <td>[1, 35, 2444, 2269, 2109, 625, 39, 26, 95, 473...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>madam president, on a point of order.</td>\n",
       "      <td>frau präsidentin, zur geschäftsordnung.</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>[1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]</td>\n",
       "      <td>[1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>madam president, on a point of order.</td>\n",
       "      <td>frau präsidentin, zur geschäftsordnung.</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>[1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]</td>\n",
       "      <td>[1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_texts  \\\n",
       "0                           resumption of the session   \n",
       "5      please rise, then, for this minute' s silence.   \n",
       "6   (the house rose and observed a minute' s silence)   \n",
       "7               madam president, on a point of order.   \n",
       "13              madam president, on a point of order.   \n",
       "\n",
       "                                         target_texts  input_length  \\\n",
       "0                  wiederaufnahme der sitzungsperiode            25   \n",
       "5   ich bitte sie, sich zu einer schweigeminute zu...            46   \n",
       "6   (das parlament erhebt sich zu einer schweigemi...            49   \n",
       "7             frau präsidentin, zur geschäftsordnung.            37   \n",
       "13            frau präsidentin, zur geschäftsordnung.            37   \n",
       "\n",
       "    target_length                                    input_sequences  \\\n",
       "0              34          [1, 344, 146, 498, 90, 6, 3, 3235, 90, 2]   \n",
       "5              55  [1, 3005, 416, 77, 359, 4, 241, 4, 17, 76, 451...   \n",
       "6              52  [1, 29, 140, 414, 3231, 8, 3106, 2484, 9, 451,...   \n",
       "7              39   [1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]   \n",
       "13             39   [1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]   \n",
       "\n",
       "                                     target_sequences  \n",
       "0       [1, 247, 351, 750, 5, 934, 43, 3158, 4762, 2]  \n",
       "5   [1, 241, 156, 72, 3112, 54, 4, 39, 26, 95, 473...  \n",
       "6   [1, 35, 2444, 2269, 2109, 625, 39, 26, 95, 473...  \n",
       "7       [1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]  \n",
       "13      [1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:02:34.784912Z",
     "start_time": "2018-06-16T21:02:34.779635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English subwords ['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n",
      "German subwords ['▁das', '▁ist', '▁ein', '▁test', '▁für', '▁v', 'ort', 'rain', 'ierte', '▁zeich', 'eng', 'ruppen']\n"
     ]
    }
   ],
   "source": [
    "print(\"English subwords\", europarl.bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", europarl.bpe_target.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:02:34.879798Z",
     "start_time": "2018-06-16T21:02:34.786708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 40)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Those will be the inputs for the seq2seq model (that needs to know how long the sequences can get)\n",
    "max_len_input = europarl.df.input_sequences.apply(len).max()\n",
    "max_len_target = europarl.df.target_sequences.apply(len).max()\n",
    "(max_len_input, max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:02:34.890290Z",
     "start_time": "2018-06-16T21:02:34.881728Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(europarl.df.shape[0]), test_size=0.1, random_state=RANDOM_STATE)  # fixed random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:02:39.605925Z",
     "start_time": "2018-06-16T21:02:34.892407Z"
    }
   },
   "outputs": [],
   "source": [
    "TIME_MAJOR = False\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    encoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_input\n",
    "        dtype=tf.int32,\n",
    "        name='encoder_inputs' \n",
    "    )\n",
    "    batch_size = tf.shape(encoder_inputs)[0]\n",
    "    \n",
    "    dropout = tf.placeholder_with_default(tf.cast(0.0, tf.float32), shape=[])\n",
    "    keep_prob = tf.cast(1.0, tf.float32) - dropout\n",
    "\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", \n",
    "        initializer=tf.constant(europarl.bpe_input.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder,\n",
    "        encoder_inputs,\n",
    "        name=\"encoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    input_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='input_sequence_length'\n",
    "    )\n",
    "    \n",
    "    rnn_cell_type = tf.nn.rnn_cell.GRUCell\n",
    "    encoder_forward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name='encoder_forward_cell'),\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        #state_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    encoder_backward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name='encoder_backward_cell'),\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        #state_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    encoder_bi_outputs, encoder_bi_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        encoder_forward_cell, encoder_backward_cell,\n",
    "        inputs=encoder_emb_inp,\n",
    "        sequence_length=input_sequence_length,\n",
    "        time_major=TIME_MAJOR,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    encoder_outputs = tf.concat(encoder_bi_outputs, -1)\n",
    "    encoder_state = tf.concat(encoder_bi_state, -1)\n",
    "    \n",
    "    # Regarding time_major:\n",
    "    # If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "    # If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "    # Using `time_major = True` is a bit more efficient because it avoids\n",
    "    # transposes at the beginning and end of the RNN calculation.  However,\n",
    "    # most TensorFlow data is batch-major, so by default this function\n",
    "    # accepts input and emits output in batch-major form.\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_inputs' \n",
    "    )\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", \n",
    "        initializer=tf.constant(europarl.bpe_target.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder,\n",
    "        decoder_inputs,\n",
    "        name=\"decoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='target_sequence_length'\n",
    "    )\n",
    "    decoder_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM, name='decoder_cell'),\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        #state_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        inputs=decoder_emb_inp, \n",
    "        sequence_length=target_sequence_length,\n",
    "        time_major=TIME_MAJOR,\n",
    "        name=\"decoder_training_helper\",\n",
    "    )\n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        units=len(europarl.bpe_target.tokens),\n",
    "        use_bias=False,\n",
    "        name='projection_layer',\n",
    "    )\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=decoder_cell,\n",
    "        helper=training_helper,\n",
    "        initial_state=encoder_state,\n",
    "        output_layer=projection_layer,\n",
    "    )\n",
    "    outputs, _final_state, _final_sequence_length = tf.contrib.seq2seq.dynamic_decode(  \n",
    "        decoder,\n",
    "        output_time_major=TIME_MAJOR,\n",
    "        impute_finished=True,\n",
    "        # swap_memory=True,\n",
    "    )\n",
    "    logits = outputs.rnn_output\n",
    "    \n",
    "    decoder_outputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_outputs',\n",
    "    )\n",
    "    target_weights = tf.cast(tf.sequence_mask(target_sequence_length), dtype=tf.float32)\n",
    "    train_loss = tf.contrib.seq2seq.sequence_loss(logits, decoder_outputs, target_weights)\n",
    "\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "        t_list=gradients,\n",
    "        clip_norm=1.,\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "    \n",
    "    inference_decoder_initial_state = tf.contrib.seq2seq.tile_batch(\n",
    "        encoder_state,\n",
    "        multiplier=BEAM_WIDTH,\n",
    "        name='inference_decoder_initital_state',\n",
    "    )\n",
    "\n",
    "    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "        cell=decoder_cell,\n",
    "        embedding=embedding_decoder,\n",
    "        start_tokens=tf.fill([batch_size], europarl.bpe_target.start_token_idx),\n",
    "        end_token=europarl.bpe_target.stop_token_idx,\n",
    "        initial_state=inference_decoder_initial_state,\n",
    "        beam_width=BEAM_WIDTH,\n",
    "        output_layer=projection_layer,\n",
    "        length_penalty_weight=1.0,  # TODO: check hyperparameter tuning\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    inference_outputs, _inference_final_state, _inference_final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inference_decoder,\n",
    "        maximum_iterations=tf.round(tf.reduce_max(input_sequence_length) * 2),  # a bit more flexible than max_len_target\n",
    "        swap_memory=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:02:39.620065Z",
     "start_time": "2018-06-16T21:02:39.607374Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_train_batch(batch_ids):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    pred, loss, _ = sess.run(\n",
    "        fetches=[\n",
    "            outputs, train_loss, update_step\n",
    "        ],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "            dropout: DROPOUT,\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def run_val_batch(batch_ids):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    loss = sess.run(\n",
    "        fetches=[train_loss],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def run_validation_loss():\n",
    "    return np.mean([\n",
    "        run_val_batch(ids)\n",
    "        for ids \n",
    "        in np.array_split(val_ids, np.ceil(len(val_ids) / BATCH_SIZE))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:35:58.660635Z",
     "start_time": "2018-06-16T21:02:39.621972Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9d40f28d0a45a1bd552662e1f08617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 3.3044024 val_loss 2.2765489\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d19a1c9996245f7ad57eb91d26feb95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.1493225 val_loss 1.9203267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6ab8edaa5e4da1b90be7c9cdcc7efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8672987 val_loss 1.7638649\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c287117196ef4c9c97bc6d57279dcaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.7052873 val_loss 1.6825069\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5282214c9e45ce9b468b0d6c177821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.5943111 val_loss 1.6305454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5deafba7fa41f695134e4456c58fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.5122467 val_loss 1.6029016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab69a3cec86a4417931ba839a285cbb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.4481808 val_loss 1.5801067\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2771d99047c8400cafc2137ccb2106aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.397142 val_loss 1.5679839\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21f476261ce4625b56e505aeab1e8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.3544468 val_loss 1.5588113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e66ced814f47bd8dd11853efa330cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 10', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.3182902 val_loss 1.5606792\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e93a4a22a334253ab7d0992ae9cc000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 11', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.2866696 val_loss 1.5568115\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3c944f3c44410781b3dbb3d01b1e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 12', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.2595927 val_loss 1.5548369\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1e98c1d30748a98eeac92db78eba9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 13', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.2369063 val_loss 1.5530577\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bce236e65564e4b9e289173228fe8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 14', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.2158996 val_loss 1.556557\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e041b281944aa8bbb3532ab3e7fe5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 15', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.1947303 val_loss 1.5570173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0a00a78d294a79a63c241af2946871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 16', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.1787221 val_loss 1.5603123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7a109deb934afcac6492741806c7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 17', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.1633192 val_loss 1.565825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95112a43f8a04f2ca02ade981969c419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 18', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.1501997 val_loss 1.5637382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac47b7108fe74a689740ff46d77ea514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 19', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.1345676 val_loss 1.568669\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59366a7f73945709ee9b8d2f175c561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 20', max=1176), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.1238617 val_loss 1.5717236\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(\n",
    "    allow_soft_placement=True,  # needed as recommendation from https://github.com/tensorflow/tensorflow/issues/2292\n",
    "    log_device_placement=True,\n",
    ")\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batches_per_epoch = np.ceil(len(train_ids) / BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    shuffled_ids = np.random.permutation(train_ids)\n",
    "    batch_splits = np.array_split(shuffled_ids, batches_per_epoch)\n",
    "    train_losses = []\n",
    "    N = len(batch_splits)\n",
    "    with tqdm(batch_splits, desc=f\"Epoch {epoch+1}\") as t:\n",
    "        for train_batch_ids in t:\n",
    "            batch_loss = run_train_batch(train_batch_ids)\n",
    "            train_losses.append(batch_loss)\n",
    "            t.set_postfix(train_loss=np.mean(train_losses))\n",
    "        print(\"train_loss\", np.mean(train_losses), \"val_loss\", run_validation_loss())\n",
    "        \n",
    "validation_input_sequences = europarl.df.input_sequences.iloc[val_ids[:BATCH_SIZE]]\n",
    "validation_input_lengths = validation_input_sequences.apply(len)\n",
    "\n",
    "validation_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    validation_input_sequences,\n",
    "    maxlen=max_len_input,\n",
    "    dtype=int,\n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "# translations = sess.run(\n",
    "#     fetches=[inference_outputs.sample_id],\n",
    "#     feed_dict={\n",
    "#         encoder_inputs: validation_input_padded,\n",
    "#         input_sequence_length: np.array(validation_input_lengths),\n",
    "#     },\n",
    "# )\n",
    "# for input_text, target_text, translation_token_indices in zip(\n",
    "#     europarl.df.input_texts.iloc[val_ids[:BATCH_SIZE]],\n",
    "#     europarl.df.target_texts.iloc[val_ids[:BATCH_SIZE]],\n",
    "#     translations[0]\n",
    "# ):\n",
    "#     translation = europarl.bpe_target.sentencepiece.DecodePieces([\n",
    "#         europarl.bpe_target.tokens[idx] for idx in translation_token_indices\n",
    "#     ])\n",
    "#     print(input_text, target_text, translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:35:58.666348Z",
     "start_time": "2018-06-16T21:35:58.662115Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    sequenced = europarl.bpe_input.subword_indices(preprocess(sentence))\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [sequenced],\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    \n",
    "    beam_search_output = sess.run(\n",
    "        fetches=[inference_outputs],\n",
    "        feed_dict={\n",
    "            encoder_inputs: padded,\n",
    "            input_sequence_length: [len(sequenced)],\n",
    "        }\n",
    "    )[0]\n",
    "    \n",
    "    return europarl.bpe_target.sentencepiece.DecodePieces([\n",
    "        europarl.bpe_target.tokens[idx] for idx in beam_search_output.predicted_ids[0, :, 0].tolist()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:35:59.387584Z",
     "start_time": "2018-06-16T21:35:58.667873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/tfattentionmodel.ckpt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'tfattentionmodel'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, f\"data/{name}.ckpt\")\n",
    "\n",
    "# model.save_weights(f'data/{name}_model_weights.h5') \n",
    "# s2s.model.save_weights(f'data/{name}_model_weights.h5')  # https://drive.google.com/open?id=10Sv-JnAiUT_fvU_cw1_H7mkcTAipC5aA\n",
    "# s2s.inference_encoder_model.save_weights(f'data/{name}_inference_encoder_model_weights.h5')  # https://drive.google.com/open?id=1gNBrn_Wij0PyeE-jJsEnlv7aHXkYuAup\n",
    "# s2s.inference_decoder_model.save_weights(f'data/{name}_inference_decoder_model_weights.h5')  # https://drive.google.com/open?id=1LCU53Hnb4m42QO3qsZTAkyYyroqz2vbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:35:59.403414Z",
     "start_time": "2018-06-16T21:35:59.389383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "      <th>input_sequences</th>\n",
       "      <th>target_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resumption of the session</td>\n",
       "      <td>wiederaufnahme der sitzungsperiode</td>\n",
       "      <td>25</td>\n",
       "      <td>34</td>\n",
       "      <td>[1, 344, 146, 498, 90, 6, 3, 3235, 90, 2]</td>\n",
       "      <td>[1, 247, 351, 750, 5, 934, 43, 3158, 4762, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>please rise, then, for this minute' s silence.</td>\n",
       "      <td>ich bitte sie, sich zu einer schweigeminute zu...</td>\n",
       "      <td>46</td>\n",
       "      <td>55</td>\n",
       "      <td>[1, 3005, 416, 77, 359, 4, 241, 4, 17, 76, 451...</td>\n",
       "      <td>[1, 241, 156, 72, 3112, 54, 4, 39, 26, 95, 473...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(the house rose and observed a minute' s silence)</td>\n",
       "      <td>(das parlament erhebt sich zu einer schweigemi...</td>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "      <td>[1, 29, 140, 414, 3231, 8, 3106, 2484, 9, 451,...</td>\n",
       "      <td>[1, 35, 2444, 2269, 2109, 625, 39, 26, 95, 473...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>madam president, on a point of order.</td>\n",
       "      <td>frau präsidentin, zur geschäftsordnung.</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>[1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]</td>\n",
       "      <td>[1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>madam president, on a point of order.</td>\n",
       "      <td>frau präsidentin, zur geschäftsordnung.</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>[1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]</td>\n",
       "      <td>[1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_texts  \\\n",
       "0                           resumption of the session   \n",
       "5      please rise, then, for this minute' s silence.   \n",
       "6   (the house rose and observed a minute' s silence)   \n",
       "7               madam president, on a point of order.   \n",
       "13              madam president, on a point of order.   \n",
       "\n",
       "                                         target_texts  input_length  \\\n",
       "0                  wiederaufnahme der sitzungsperiode            25   \n",
       "5   ich bitte sie, sich zu einer schweigeminute zu...            46   \n",
       "6   (das parlament erhebt sich zu einer schweigemi...            49   \n",
       "7             frau präsidentin, zur geschäftsordnung.            37   \n",
       "13            frau präsidentin, zur geschäftsordnung.            37   \n",
       "\n",
       "    target_length                                    input_sequences  \\\n",
       "0              34          [1, 344, 146, 498, 90, 6, 3, 3235, 90, 2]   \n",
       "5              55  [1, 3005, 416, 77, 359, 4, 241, 4, 17, 76, 451...   \n",
       "6              52  [1, 29, 140, 414, 3231, 8, 3106, 2484, 9, 451,...   \n",
       "7              39   [1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]   \n",
       "13             39   [1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]   \n",
       "\n",
       "                                     target_sequences  \n",
       "0       [1, 247, 351, 750, 5, 934, 43, 3158, 4762, 2]  \n",
       "5   [1, 241, 156, 72, 3112, 54, 4, 39, 26, 95, 473...  \n",
       "6   [1, 35, 2444, 2269, 2109, 625, 39, 26, 95, 473...  \n",
       "7       [1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]  \n",
       "13      [1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:35:59.803079Z",
     "start_time": "2018-06-16T21:35:59.405619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello.' --> 'hallo!'\n",
      "'you are welcome.' --> 'sie sind zu begrüßen.'\n",
      "'how do you do?' --> 'wie tun sie es?'\n",
      "'i hate mondays.' --> 'ich habe gesprochen.'\n",
      "'i am a programmer.' --> 'ich bin ein programm.'\n",
      "'data is the new oil.' --> 'vors sind die neuen ölländerung.'\n",
      "'it could be worse.' --> 'das könnte bedenklich werden.'\n",
      "'i am on top of it.' --> 'ich bin schon davon.'\n",
      "'n° uno' --> 'neino'\n",
      "'awesome!' --> 'das ist eine schande!'\n",
      "'put your feet up!' --> 'fassen sie ihren füßen!'\n",
      "'from the start till the end!' --> 'beginnen sie am ende!'\n",
      "'from dusk till dawn.' --> 'aus der mouskampagne.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:36:00.375601Z",
     "start_time": "2018-06-16T21:35:59.804273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \"please rise, then, for this minute' s silence.\", got 'ich bitte sie, sich zu einer schweigeminute zu erheben.', exp: 'ich bitte sie, sich zu einer schweigeminute zu erheben.'\n",
      "Original \"(the house rose and observed a minute' s silence)\", got '(das parlament erhebt sich zu einer schweigeminute.)', exp: '(das parlament erhebt sich zu einer schweigeminute.)'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'thank you, mr segni, i shall do so gladly.', got 'vielen dank, herr segni, ich werde dies tun.', exp: 'vielen dank, herr segni, das will ich gerne tun.'\n",
      "Original 'it is the case of alexander nikitin.', got 'das ist der fall von alexander nikitin.', exp: 'das ist der fall von alexander nikitin.'\n",
      "Original 'it will, i hope, be examined in a positive light.', got 'ich hoffe, daß ich auf einige positiven bericht eingehen kann.', exp: 'ich hoffe, daß dort in ihrem sinne entschieden wird.'\n",
      "Original 'why are there no fire instructions?', got 'warum gibt es keine streitigkeiten?', exp: 'warum finden keine brandschutzbelehrungen statt?'\n",
      "Original 'mr berenguer fuster, we shall check all this.', got 'herr berichterstatter, das werden wir prüfen.', exp: 'lieber kollege, wir werden das prüfen.'\n",
      "Original 'we do not know what is happening.', got 'wir wissen nicht, was passiert ist.', exp: 'wir wissen nicht, was passiert.'\n",
      "Original 'agenda', got 'arbeitsplan', exp: 'arbeitsplan'\n",
      "Original 'relating to wednesday:', got 'zum mittwoch:', exp: 'zum mittwoch:'\n",
      "Original '(applause from the pse group)', got '(beifall von der pse-fraktion)', exp: '(beifall der pse-fraktion)'\n",
      "Original 'mr hänsch represented you on this occasion.', got 'herr hänsch hat sie in diesem fall vertreten.', exp: 'der kollege hänsch hat sie dort vertreten.'\n",
      "Original 'we then put it to a vote.', got 'wir kommen nun zur abstimmung.', exp: 'wir haben dann abgestimmt.'\n",
      "Original 'there was a vote on this matter.', got 'es gab eine abstimmung über diese angelegenheit.', exp: 'es gab eine abstimmung zu diesem punkt.'\n",
      "Original 'all of the others were of a different opinion.', got 'alle anderen waren anders.', exp: 'alle anderen waren anderer meinung.'\n",
      "Original 'that was the decision.', got 'das war die entscheidung.', exp: 'das war der beschluß.'\n",
      "Original 'i should now like to comment on the issue itself.', got 'ich möchte nun auf diese frage eingehen.', exp: 'jetzt möchte ich zur sache selbst etwas sagen.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in europarl.df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:36:00.833742Z",
     "start_time": "2018-06-16T21:36:00.377404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'perhaps there is a connection.', got 'vielleicht gibt es hier einen zusammenhang.', exp: 'vielleicht besteht da ein zusammenhang.'\n",
      "Original '(applause)', got '(beifall)', exp: '(beifall)'\n",
      "Original 'mr president, what is going on?', got 'herr präsident, was wird hier geschehen?', exp: 'herr präsident! was steckt dahinter?'\n",
      "Original 'it is something that we must do ourselves.', got 'das müssen wir uns tun.', exp: 'es ist etwas, das wir selbst tun müssen.'\n",
      "Original \"no reform'.\", got 'keine reform.', exp: 'keine reform.\"'\n",
      "Original 'there must be no more lockerbies.', got 'es darf keine vorbedingungen mehr tun.', exp: 'es darf keine lockerbies mehr geben.'\n",
      "Original 'the european union must lead by example.', got 'die europäische union muss mit gutem beispiel vorangehen.', exp: 'die europäische union muss mit gutem beispiel vorangehen.'\n",
      "Original 'that is the point.', got 'das ist der punkt.', exp: 'darum geht es.'\n",
      "Original 'it is as simple as that.', got 'so einfach ist das.', exp: 'so einfach ist der grund.'\n",
      "Original 'how does he see those conflicts being resolved?', got 'wie sieht die konflikte aus?', exp: 'wie sollen diese konflikte seiner ansicht nach gelöst werden?'\n",
      "Original 'not even rio could agree on that.', got 'das kann auch nicht so richtig sein.', exp: 'darauf konnte sich rio noch nicht einmal einigen.'\n",
      "Original 'situation in iraq', got 'lage im irak', exp: 'lage im irak'\n",
      "Original 'quisthoudt-rowohl report (a0-0/0)', got 'bericht quisthoudt-rowohl (a0-0/0)', exp: 'bericht quisthoudt-rowohl (a0-0/0)'\n",
      "Original 'the debate is closed.', got 'die aussprache ist geschlossen.', exp: 'die aussprache ist geschlossen.'\n",
      "Original 'the vote will take place at 0.0 p.m. tomorrow.', got 'die abstimmung findet morgen um 0.0 uhr statt.', exp: 'die abstimmung findet morgen um 0.0 uhr statt.'\n",
      "Original 'that has already been mentioned too.', got 'das wurde schon angesprochen.', exp: 'das wurde schon angesprochen.'\n",
      "Original 'is this the example we wish to follow?', got 'ist das ein beispiel, das wir festgehen?', exp: 'ist dies das beispiel, dem wir folgen wollen?'\n",
      "Original 'the debate is closed.', got 'die aussprache ist geschlossen.', exp: 'die aussprache ist geschlossen.'\n",
      "Original 'my next point concerns liquids.', got 'meine nächste frage betrifft die solidarität.', exp: 'nächster punkt: liquids.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = europarl.df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T21:37:04.466374Z",
     "start_time": "2018-06-16T21:36:00.835487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b384580db284fed85de89da69c1a6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.32828439168305806\n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 294,
   "position": {
    "height": "40px",
    "left": "553px",
    "right": "192px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
