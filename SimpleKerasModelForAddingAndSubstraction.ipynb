{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:17:18.709459Z",
     "start_time": "2018-05-08T09:17:17.505899Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:17:18.713428Z",
     "start_time": "2018-05-08T09:17:18.710754Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.layers as L\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_STATE=42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.set_random_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:17:18.720133Z",
     "start_time": "2018-05-08T09:17:18.714863Z"
    }
   },
   "outputs": [],
   "source": [
    "START = '^'\n",
    "END = '$'\n",
    "\n",
    "SIZE = 100_000\n",
    "LATENT_DIM = 512\n",
    "EMBEDDING_DIM = 16\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:17:18.725901Z",
     "start_time": "2018-05-08T09:17:18.721483Z"
    }
   },
   "outputs": [],
   "source": [
    "def loguniform_int(low=0, high=1, size=1):\n",
    "    offset = np.max([1 - low, 0])\n",
    "    low, high = np.log([low + offset, high + offset])\n",
    "    return (np.exp(np.random.uniform(low, high, size)) - offset).astype(int)\n",
    "\n",
    "def create_equations_df(size, min_value=0, max_value=9999, operations={'+': np.add, '-': np.subtract}):\n",
    "    df = pd.DataFrame()\n",
    "    df['a'] = loguniform_int(low=min_value, high=max_value, size=size)\n",
    "    df['b'] = loguniform_int(low=min_value, high=max_value, size=size)\n",
    "    df['op'] = np.random.choice(list(operations.keys()), size)\n",
    "    df['result'] = np.zeros(size, dtype='int')\n",
    "    for symbol, calc in operations.items():\n",
    "        df.loc[df.op == symbol, 'result'] = calc(df[df.op == symbol]['a'], df[df.op == symbol]['b'])\n",
    "        \n",
    "    df['input_texts'] = df.a.astype(str) + df.op + df.b.astype(str) + END\n",
    "    df['target_texts'] = START + df.result.astype(str) + END\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:17:18.973642Z",
     "start_time": "2018-05-08T09:17:18.727106Z"
    }
   },
   "outputs": [],
   "source": [
    "df = create_equations_df(SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:17:18.980803Z",
     "start_time": "2018-05-08T09:17:18.974999Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = pd.concat([df.input_texts, df.target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:17:19.838850Z",
     "start_time": "2018-05-08T09:17:18.982374Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=None, filters=None, char_level=True)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "df['input_sequences'] = tokenizer.texts_to_sequences(df.input_texts)\n",
    "df['target_sequences'] = tokenizer.texts_to_sequences(df.target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:17:20.606923Z",
     "start_time": "2018-05-08T09:17:19.840398Z"
    }
   },
   "outputs": [],
   "source": [
    "X = keras.preprocessing.sequence.pad_sequences(df.input_sequences, padding='post')\n",
    "y = keras.preprocessing.sequence.pad_sequences(df.target_sequences, padding='post')\n",
    "y_t_output = keras.utils.to_categorical(y[:,1:], num_classes=len(tokenizer.word_index)+1)\n",
    "x_t_input = y[:,:-1]\n",
    "\n",
    "max_len_input = X.shape[1]\n",
    "max_len_target = x_t_input.shape[1]\n",
    "nr_tokens = y_t_output.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:17:20.617445Z",
     "start_time": "2018-05-08T09:17:20.608279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$': 1,\n",
       " '1': 2,\n",
       " '2': 3,\n",
       " '^': 4,\n",
       " '3': 5,\n",
       " '4': 6,\n",
       " '-': 7,\n",
       " '5': 8,\n",
       " '0': 9,\n",
       " '6': 10,\n",
       " '7': 11,\n",
       " '8': 12,\n",
       " '9': 13,\n",
       " '+': 14}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(100000, 6, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index\n",
    "nr_tokens\n",
    "y_t_output.shape\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:17:21.694744Z",
     "start_time": "2018-05-08T09:17:20.619084Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_gru = L.Bidirectional(\n",
    "    L.GRU(LATENT_DIM // 2, dropout=DROPOUT, return_state=True, name='encoder_gru'),\n",
    "    name='encoder_bidirectional'\n",
    ")\n",
    "decoder_gru = L.GRU(LATENT_DIM, dropout=DROPOUT, return_sequences=True, return_state=True, name='decoder_gru')\n",
    "decoder_dense = L.Dense(nr_tokens, activation='softmax', name='decoder_outputs')\n",
    "\n",
    "shared_embedding = L.Embedding(nr_tokens, EMBEDDING_DIM, mask_zero=True, name='shared_embedding')\n",
    "\n",
    "encoder_inputs = L.Input(shape=(max_len_input, ), dtype='int32', name='encoder_inputs')\n",
    "encoder_embeddings = shared_embedding(encoder_inputs)\n",
    "_, encoder_state_1, encoder_state_2 = encoder_gru(encoder_embeddings)\n",
    "encoder_states = L.concatenate([encoder_state_1, encoder_state_2])\n",
    "\n",
    "decoder_inputs = L.Input(shape=(max_len_target, ), dtype='int32', name='decoder_inputs')\n",
    "decoder_mask = L.Masking(mask_value=0)(decoder_inputs)\n",
    "decoder_embeddings_inputs = shared_embedding(decoder_mask)\n",
    "decoder_embeddings_outputs, _ = decoder_gru(decoder_embeddings_inputs, initial_state=encoder_states) \n",
    "decoder_outputs = decoder_dense(decoder_embeddings_outputs)\n",
    "\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "\n",
    "inference_encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "inference_decoder_state_inputs = L.Input(shape=(LATENT_DIM, ), dtype='float32', name='inference_decoder_state_inputs')\n",
    "inference_decoder_embeddings_outputs, inference_decoder_states = decoder_gru(\n",
    "    decoder_embeddings_inputs, initial_state=inference_decoder_state_inputs\n",
    ")\n",
    "inference_decoder_outputs = decoder_dense(inference_decoder_embeddings_outputs)\n",
    "\n",
    "inference_decoder_model = Model(\n",
    "    [decoder_inputs, inference_decoder_state_inputs], \n",
    "    [inference_decoder_outputs, inference_decoder_states]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:17:21.700374Z",
     "start_time": "2018-05-08T09:17:21.696172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 6)            0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_inputs (InputLayer)     (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    multiple             240         encoder_inputs[0][0]             \n",
      "                                                                 masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bidirectional (Bidirect [(None, 512), (None, 419328      shared_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           encoder_bidirectional[0][1]      \n",
      "                                                                 encoder_bidirectional[0][2]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 6, 512), (No 812544      shared_embedding[1][0]           \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 6, 15)        7695        decoder_gru[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,239,807\n",
      "Trainable params: 1,239,807\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 6)            0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    multiple             240         masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "inference_decoder_state_inputs  (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 6, 512), (No 812544      shared_embedding[1][0]           \n",
      "                                                                 inference_decoder_state_inputs[0]\n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 6, 15)        7695        decoder_gru[1][0]                \n",
      "==================================================================================================\n",
      "Total params: 820,479\n",
      "Trainable params: 820,479\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "inference_decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:17:21.732381Z",
     "start_time": "2018-05-08T09:17:21.701708Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(clipnorm=1.), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:22:58.305534Z",
     "start_time": "2018-05-08T09:17:21.733947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "90000/90000 [==============================] - 19s 208us/step - loss: 1.4554 - val_loss: 1.2199\n",
      "Epoch 2/20\n",
      "90000/90000 [==============================] - 17s 189us/step - loss: 0.9731 - val_loss: 0.7166\n",
      "Epoch 3/20\n",
      "90000/90000 [==============================] - 16s 183us/step - loss: 0.5876 - val_loss: 0.4270\n",
      "Epoch 4/20\n",
      "90000/90000 [==============================] - 17s 183us/step - loss: 0.3584 - val_loss: 0.2481\n",
      "Epoch 5/20\n",
      "90000/90000 [==============================] - 16s 183us/step - loss: 0.2270 - val_loss: 0.1729\n",
      "Epoch 6/20\n",
      "90000/90000 [==============================] - 17s 184us/step - loss: 0.1653 - val_loss: 0.1450\n",
      "Epoch 7/20\n",
      "90000/90000 [==============================] - 17s 184us/step - loss: 0.1348 - val_loss: 0.1009\n",
      "Epoch 8/20\n",
      "90000/90000 [==============================] - 17s 184us/step - loss: 0.1136 - val_loss: 0.0855\n",
      "Epoch 9/20\n",
      "90000/90000 [==============================] - 17s 185us/step - loss: 0.0992 - val_loss: 0.0875\n",
      "Epoch 10/20\n",
      "90000/90000 [==============================] - 17s 184us/step - loss: 0.0890 - val_loss: 0.0782\n",
      "Epoch 11/20\n",
      "90000/90000 [==============================] - 17s 184us/step - loss: 0.0773 - val_loss: 0.0644\n",
      "Epoch 12/20\n",
      "90000/90000 [==============================] - 17s 184us/step - loss: 0.0723 - val_loss: 0.0605\n",
      "Epoch 13/20\n",
      "90000/90000 [==============================] - 17s 184us/step - loss: 0.0659 - val_loss: 0.0618\n",
      "Epoch 14/20\n",
      "90000/90000 [==============================] - 17s 184us/step - loss: 0.0626 - val_loss: 0.0619\n",
      "Epoch 15/20\n",
      "90000/90000 [==============================] - 17s 185us/step - loss: 0.0588 - val_loss: 0.0574\n",
      "Epoch 16/20\n",
      "90000/90000 [==============================] - 17s 186us/step - loss: 0.0551 - val_loss: 0.0473\n",
      "Epoch 17/20\n",
      "90000/90000 [==============================] - 17s 185us/step - loss: 0.0497 - val_loss: 0.0502\n",
      "Epoch 18/20\n",
      "90000/90000 [==============================] - 17s 185us/step - loss: 0.0474 - val_loss: 0.0476\n",
      "Epoch 19/20\n",
      "90000/90000 [==============================] - 17s 185us/step - loss: 0.0452 - val_loss: 0.0446\n",
      "Epoch 20/20\n",
      "90000/90000 [==============================] - 17s 190us/step - loss: 0.0441 - val_loss: 0.0410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f977471d7f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, x_t_input], y_t_output, validation_split=0.1, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:22:58.310628Z",
     "start_time": "2018-05-08T09:22:58.307000Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = inference_encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, max_len_target))\n",
    "    target_seq[0, 0] = tokenizer.word_index[START]\n",
    "    \n",
    "    tokens = {idx: token for (token, idx) in tokenizer.word_index.items()}\n",
    "    \n",
    "    decoded_sequence = ''\n",
    "    for i in range(max_len_target):\n",
    "        output_tokens, output_states = inference_decoder_model.predict(\n",
    "            [target_seq, states_value]\n",
    "        )\n",
    "        \n",
    "        sampled_token_idx = np.argmax(output_tokens[0, 0, :])\n",
    "        sampled_token = tokens.get(sampled_token_idx, '.')\n",
    "        if sampled_token == END:\n",
    "            break\n",
    "        decoded_sequence += sampled_token\n",
    "            \n",
    "        target_seq[0, 0] = sampled_token_idx\n",
    "        states_value = output_states\n",
    "    \n",
    "    return decoded_sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:22:58.778923Z",
     "start_time": "2018-05-08T09:22:58.312669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1$=got: 2, exp: 2\n",
      "9+11$=got: 20, exp: 20\n",
      "21+34$=got: 55, exp: 55\n",
      "359+468$=got: 827, exp: 827\n",
      "1359+468$=got: 1827, exp: 1827\n",
      "1-1$=got: 0, exp: 0\n",
      "19-1$=got: 18, exp: 18\n",
      "34-359$=got: -325, exp: -325\n",
      "1359-468$=got: 891, exp: 891\n"
     ]
    }
   ],
   "source": [
    "for calc in [eq + '$' for eq in ['1+1', '9+11', '21+34', '359+468', '1359+468', '1-1', '19-1', '34-359', '1359-468']]:\n",
    "    input_seq = keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences([calc]), \n",
    "        padding='post', \n",
    "        maxlen=X.shape[1]\n",
    "    )\n",
    "    print(f\"{calc}=got: {decode_sequence(input_seq)}, exp: {eval(calc[:-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:22:58.946185Z",
     "start_time": "2018-05-08T09:22:58.780064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30-209$=got: -179, exp: -179\n",
      "6350+127$=got: 6477, exp: 6477\n",
      "846-24$=got: 822, exp: 822\n",
      "247-92$=got: 155, exp: 155\n",
      "3+27$=got: 30, exp: 30\n",
      "3-427$=got: -424, exp: -424\n",
      "0-2187$=got: -2187, exp: -2187\n",
      "2914-1403$=got: 1511, exp: 1511\n",
      "252+22$=got: 274, exp: 274\n",
      "678-108$=got: 560, exp: 570\n"
     ]
    }
   ],
   "source": [
    "for calc in df.input_texts[:10].tolist():\n",
    "    input_seq = keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences([calc]), \n",
    "        padding='post', \n",
    "        maxlen=X.shape[1]\n",
    "    )\n",
    "    print(f\"{calc}=got: {decode_sequence(input_seq)}, exp: {eval(calc[:-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
