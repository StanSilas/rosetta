{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple seq2seq model in keras that translates english <-> german"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step I take the model used for the [toy problem of adding/subtracting numbers](SimpleKerasModelForAddingAndSubstraction.ipynb) and train it with english/german data for machine translation.\n",
    "\n",
    "As trainings set I use the [European Parliament Proceedings Parallel Corpus 1996-2011](http://statmt.org/europarl/) German-English corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:39.133776Z",
     "start_time": "2018-05-09T12:14:37.825408Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# technical detail so that an instance (maybe running in a different window)\n",
    "# doesn't take all the GPU memory resulting in some strange error messages\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:39.404583Z",
     "start_time": "2018-05-09T12:14:39.135186Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "import keras\n",
    "import keras.layers as L\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Fixing random state ensure reproducible results\n",
    "RANDOM_STATE=42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.set_random_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:39.408287Z",
     "start_time": "2018-05-09T12:14:39.405808Z"
    }
   },
   "outputs": [],
   "source": [
    "START = '^'\n",
    "END = '\\n'\n",
    "\n",
    "MAX_INPUT_LENGTH = 50\n",
    "MAX_TARGET_LENGTH = 65\n",
    "LATENT_DIM = 512\n",
    "EMBEDDING_DIM = 64\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "TEST_SIZE=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:39.421889Z",
     "start_time": "2018-05-09T12:14:39.410066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n"
     ]
    }
   ],
   "source": [
    "def download_file(fname, url):\n",
    "    print(f\"Downloading {fname} from {url} ...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    total_size = int(response.headers.get('content-length', 0)); \n",
    "    block_size = 1024\n",
    "\n",
    "    download = tqdm(\n",
    "        response.iter_content(block_size),\n",
    "        total=math.ceil(total_size // block_size),\n",
    "        unit='KB',\n",
    "        unit_scale=True\n",
    "    )\n",
    "    with open(f\"{fname}\", \"wb\") as handle:\n",
    "        for data in download:\n",
    "            handle.write(data)\n",
    "\n",
    "PATH = 'data'\n",
    "FILES = {\n",
    "    'de-en.tgz': 'http://statmt.org/europarl/v7/de-en.tgz',  # incredible: really only http, not https :-o\n",
    "}\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "for name, url in FILES.items():\n",
    "    fname = os.path.join(PATH, name)\n",
    "    exists = os.path.exists(fname)\n",
    "    size = os.path.getsize(fname) if exists else -1\n",
    "    if exists and size > 0:\n",
    "        print(f'{name} already downloaded ({size / 2**20:3.1f} MB)')\n",
    "        continue\n",
    "    download_file(fname, url)\n",
    "    if (fname.endswith(\".tgz\")):\n",
    "        tar = tarfile.open(fname, \"r:gz\")\n",
    "        tar.extractall(path=PATH)\n",
    "        tar.close()\n",
    "        print(f'Extracted {fname} ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:40.986482Z",
     "start_time": "2018-05-09T12:14:39.423573Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 60)\n",
    "df = pd.DataFrame(data={\n",
    "    'input_texts': open(f'{PATH}/europarl-v7.de-en.en', 'r').readlines(),\n",
    "    'target_texts': open(f'{PATH}/europarl-v7.de-en.de', 'r').readlines(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:42.369417Z",
     "start_time": "2018-05-09T12:14:40.989270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920209"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session\\n</td>\n",
       "      <td>^Wiederaufnahme der Sitzungsperiode\\n</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European Parliament...</td>\n",
       "      <td>^Ich erkläre die am Freitag, dem 17. Dezember unterbroch...</td>\n",
       "      <td>208</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded 'millennium...</td>\n",
       "      <td>^Wie Sie feststellen konnten, ist der gefürchtete \"Mille...</td>\n",
       "      <td>192</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in the cours...</td>\n",
       "      <td>^Im Parlament besteht der Wunsch nach einer Aussprache i...</td>\n",
       "      <td>106</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a minute' s si...</td>\n",
       "      <td>^Heute möchte ich Sie bitten - das ist auch der Wunsch e...</td>\n",
       "      <td>233</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input_texts  \\\n",
       "0                                  Resumption of the session\\n   \n",
       "1  I declare resumed the session of the European Parliament...   \n",
       "2  Although, as you will have seen, the dreaded 'millennium...   \n",
       "3  You have requested a debate on this subject in the cours...   \n",
       "4  In the meantime, I should like to observe a minute' s si...   \n",
       "\n",
       "                                                  target_texts  input_length  \\\n",
       "0                        ^Wiederaufnahme der Sitzungsperiode\\n            26   \n",
       "1  ^Ich erkläre die am Freitag, dem 17. Dezember unterbroch...           208   \n",
       "2  ^Wie Sie feststellen konnten, ist der gefürchtete \"Mille...           192   \n",
       "3  ^Im Parlament besteht der Wunsch nach einer Aussprache i...           106   \n",
       "4  ^Heute möchte ich Sie bitten - das ist auch der Wunsch e...           233   \n",
       "\n",
       "   target_length  \n",
       "0             36  \n",
       "1            220  \n",
       "2            187  \n",
       "3            112  \n",
       "4            219  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)\n",
    "df.target_texts = '^' + df.target_texts  # encode a start symbol (doesn't occur in texts)\n",
    "df['input_length'] = df.input_texts.apply(len)\n",
    "df['target_length'] = df.target_texts.apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only use short translations right now\n",
    "\n",
    "So, first I plot sentence length on a logarithmic scale, \n",
    "then I only choose short input texts (and a bit longer target texts as german is more verbose than english)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:42.830575Z",
     "start_time": "2018-05-09T12:14:42.371099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = df.input_length\n",
    "logbins = np.logspace(1,5,20)\n",
    "plt.hist(x, bins=logbins)\n",
    "plt.xscale('log')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:42.926691Z",
     "start_time": "2018-05-09T12:14:42.831767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170266"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_empty = (df.input_length > 2) & (df.target_length > 2)  # there are empty phrases like '\\n' --> 'Frau Präsidentin\\n'\n",
    "short_inputs = (df.input_length < MAX_INPUT_LENGTH) & (df.target_length < MAX_TARGET_LENGTH)\n",
    "sum(short_inputs)\n",
    "df = df[non_empty & short_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:42.936722Z",
     "start_time": "2018-05-09T12:14:42.928409Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = pd.concat([df.input_texts, df.target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:48.467771Z",
     "start_time": "2018-05-09T12:14:42.938980Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=100, filters=None, char_level=True, oov_token='~')\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "df['input_sequences'] = tokenizer.texts_to_sequences(df.input_texts)\n",
    "df['target_sequences'] = tokenizer.texts_to_sequences(df.target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:48.481301Z",
     "start_time": "2018-05-09T12:14:48.469212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 1541244),\n",
       " ('e', 1260453),\n",
       " ('i', 715699),\n",
       " ('n', 711947),\n",
       " ('t', 697801),\n",
       " ('s', 630599),\n",
       " ('r', 586692),\n",
       " ('a', 556070),\n",
       " ('o', 447123),\n",
       " ('h', 429870),\n",
       " ('\\n', 324200),\n",
       " ('l', 316464),\n",
       " ('d', 290315),\n",
       " ('u', 284932),\n",
       " ('.', 258158),\n",
       " ('c', 253867),\n",
       " ('m', 224347),\n",
       " ('g', 199194),\n",
       " ('^', 162100),\n",
       " ('w', 131188),\n",
       " ('f', 128928),\n",
       " ('b', 127399),\n",
       " ('p', 118190),\n",
       " ('k', 89809),\n",
       " ('v', 72478),\n",
       " ('y', 71107),\n",
       " ('D', 65914),\n",
       " ('T', 62802),\n",
       " ('I', 55213),\n",
       " ('A', 49270),\n",
       " ('W', 48462),\n",
       " (',', 46096),\n",
       " ('z', 45294),\n",
       " ('S', 36079),\n",
       " ('ü', 32015),\n",
       " ('E', 31773),\n",
       " ('P', 30841),\n",
       " ('ä', 28169),\n",
       " ('(', 27523),\n",
       " (')', 26649),\n",
       " ('0', 26272),\n",
       " ('M', 25695),\n",
       " ('B', 25483),\n",
       " ('?', 25114),\n",
       " ('H', 23918),\n",
       " ('\\xa0', 23046),\n",
       " ('1', 18507),\n",
       " ('F', 17077),\n",
       " ('V', 15151),\n",
       " ('-', 14427),\n",
       " ('!', 13678),\n",
       " ('R', 13379),\n",
       " ('U', 13068),\n",
       " ('2', 13056),\n",
       " ('N', 13009),\n",
       " ('K', 12837),\n",
       " ('G', 12719),\n",
       " ('ö', 12593),\n",
       " ('C', 12035),\n",
       " (':', 11333),\n",
       " ('j', 10676),\n",
       " ('L', 10673),\n",
       " ('ß', 10091),\n",
       " ('9', 8549),\n",
       " ('x', 8496),\n",
       " ('Z', 7314),\n",
       " ('4', 6950),\n",
       " ('O', 6281),\n",
       " ('3', 6148),\n",
       " ('5', 6143),\n",
       " ('/', 5771),\n",
       " ('q', 5489),\n",
       " ('7', 3853),\n",
       " ('J', 3765),\n",
       " ('8', 3758),\n",
       " ('6', 3633),\n",
       " (\"'\", 3429),\n",
       " ('Y', 2371),\n",
       " ('Q', 2299),\n",
       " ('Ä', 1899),\n",
       " ('–', 1726),\n",
       " ('\"', 1647),\n",
       " ('%', 872),\n",
       " ('Ü', 681),\n",
       " ('’', 357),\n",
       " ('é', 355),\n",
       " (';', 320),\n",
       " ('á', 230),\n",
       " ('í', 197),\n",
       " ('Ö', 185),\n",
       " ('ó', 165),\n",
       " ('‘', 120),\n",
       " ('“', 116),\n",
       " ('„', 110),\n",
       " ('è', 104),\n",
       " ('X', 75),\n",
       " ('…', 54),\n",
       " (']', 52),\n",
       " ('[', 51),\n",
       " ('à', 49),\n",
       " ('ł', 38),\n",
       " ('æ', 37),\n",
       " ('č', 29),\n",
       " ('ú', 29),\n",
       " ('ç', 28),\n",
       " ('ï', 27),\n",
       " ('°', 26),\n",
       " ('ñ', 25),\n",
       " ('š', 21),\n",
       " ('ã', 21),\n",
       " ('ò', 21),\n",
       " ('+', 20),\n",
       " ('ø', 20),\n",
       " ('ô', 19),\n",
       " ('ê', 19),\n",
       " ('ő', 18),\n",
       " ('ă', 18),\n",
       " ('Š', 18),\n",
       " ('&', 17),\n",
       " ('”', 16),\n",
       " ('Μ', 16),\n",
       " ('*', 15),\n",
       " ('ş', 15),\n",
       " ('σ', 14),\n",
       " ('•', 14),\n",
       " ('ρ', 13),\n",
       " ('\\xad', 13),\n",
       " ('ń', 12),\n",
       " ('Τ', 12),\n",
       " ('η', 11),\n",
       " ('τ', 11),\n",
       " ('ε', 10),\n",
       " ('α', 10),\n",
       " ('ė', 10),\n",
       " ('ς', 9),\n",
       " ('ι', 9),\n",
       " ('ą', 9),\n",
       " ('ί', 8),\n",
       " ('ο', 8),\n",
       " ('ý', 8),\n",
       " ('Ó', 8),\n",
       " ('π', 7),\n",
       " ('¡', 7),\n",
       " ('ę', 7),\n",
       " ('Ţ', 7),\n",
       " ('Å', 7),\n",
       " ('Á', 7),\n",
       " ('μ', 6),\n",
       " ('ν', 6),\n",
       " ('#', 6),\n",
       " ('â', 6),\n",
       " ('·', 6),\n",
       " ('δ', 5),\n",
       " ('ή', 5),\n",
       " ('ì', 5),\n",
       " ('´', 5),\n",
       " ('λ', 4),\n",
       " ('υ', 4),\n",
       " ('å', 4),\n",
       " ('ň', 4),\n",
       " ('ž', 4),\n",
       " ('Α', 4),\n",
       " ('о', 4),\n",
       " ('‟', 3),\n",
       " ('<', 3),\n",
       " ('γ', 3),\n",
       " ('ż', 3),\n",
       " ('χ', 3),\n",
       " ('î', 3),\n",
       " ('Í', 3),\n",
       " ('φ', 2),\n",
       " ('ό', 2),\n",
       " ('Π', 2),\n",
       " ('ć', 2),\n",
       " ('ś', 2),\n",
       " ('‚', 2),\n",
       " ('Ç', 2),\n",
       " ('ř', 2),\n",
       " ('ά', 2),\n",
       " ('Κ', 2),\n",
       " ('έ', 2),\n",
       " ('Υ', 2),\n",
       " ('\\u200b', 2),\n",
       " ('ū', 2),\n",
       " ('ǎ', 2),\n",
       " ('É', 2),\n",
       " ('œ', 2),\n",
       " ('ë', 2),\n",
       " ('¹', 2),\n",
       " ('Ο', 2),\n",
       " ('Ν', 2),\n",
       " ('и', 2),\n",
       " ('л', 2),\n",
       " ('ш', 2),\n",
       " ('д', 2),\n",
       " ('е', 2),\n",
       " ('р', 2),\n",
       " ('б', 2),\n",
       " ('Д', 2),\n",
       " ('ţ', 2),\n",
       " ('ğ', 2),\n",
       " ('ı', 2),\n",
       " ('Ž', 2),\n",
       " ('ź', 2),\n",
       " ('û', 2),\n",
       " ('ξ', 1),\n",
       " ('ύ', 1),\n",
       " ('κ', 1),\n",
       " ('ψ', 1),\n",
       " ('ζ', 1),\n",
       " ('À', 1),\n",
       " ('ŏ', 1),\n",
       " ('ľ', 1),\n",
       " ('`', 1),\n",
       " ('º', 1),\n",
       " ('>', 1),\n",
       " ('£', 1),\n",
       " ('Ø', 1),\n",
       " ('ī', 1),\n",
       " ('Ρ', 1),\n",
       " ('Ï', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(sorted(tokenizer.word_counts.items(), key=lambda d: d[1])))\n",
    "sum(1 for w, count in tokenizer.word_counts.items() if count > 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:48.489763Z",
     "start_time": "2018-05-09T12:14:48.482935Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len_input = df.input_length.max()\n",
    "max_len_target = df.target_length.max()\n",
    "nr_tokens = len(tokenizer.word_index) + 1  # add 0 padding not in word_index contained\n",
    "\n",
    "# one hot encoded y_t_output wouldn't fit into memory any longer\n",
    "# so need to train/validate on batches generated on the fly\n",
    "def create_batch_generator(samples_ids):\n",
    "    \n",
    "    def batch_generator():\n",
    "        nr_batches = np.ceil(len(samples_ids) / BATCH_SIZE)\n",
    "        while True:\n",
    "            shuffled_ids = np.random.permutation(samples_ids)\n",
    "            batch_splits = np.array_split(shuffled_ids, nr_batches)\n",
    "            for batch_ids in batch_splits:\n",
    "                batch_X = pad_sequences(df.iloc[batch_ids].input_sequences, padding='post', maxlen=max_len_input)\n",
    "                batch_y = pad_sequences(df.iloc[batch_ids].target_sequences, padding='post', maxlen=max_len_target)\n",
    "                batch_y_t_output = keras.utils.to_categorical(batch_y[:,1:], num_classes=len(tokenizer.word_index)+1)\n",
    "                batch_x_t_input = batch_y[:,:-1]\n",
    "                yield ([batch_X, batch_x_t_input], batch_y_t_output)\n",
    "    \n",
    "    return batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:48.501644Z",
     "start_time": "2018-05-09T12:14:48.491223Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(df.shape[0]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:48.509938Z",
     "start_time": "2018-05-09T12:14:48.503566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(145890, 16210)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_tokens\n",
    "len(tokenizer.word_index)\n",
    "len(train_ids), len(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:49.738121Z",
     "start_time": "2018-05-09T12:14:48.512082Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_gru = L.Bidirectional(\n",
    "    L.GRU(LATENT_DIM // 2, dropout=DROPOUT, return_state=True, name='encoder_gru'),\n",
    "    name='encoder_bidirectional'\n",
    ")\n",
    "decoder_gru = L.GRU(LATENT_DIM, dropout=DROPOUT, return_sequences=True, return_state=True, name='decoder_gru')\n",
    "decoder_dense = L.Dense(nr_tokens, activation='softmax', name='decoder_outputs')\n",
    "\n",
    "shared_embedding = L.Embedding(nr_tokens, EMBEDDING_DIM, mask_zero=True, name='shared_embedding')\n",
    "\n",
    "encoder_inputs = L.Input(shape=(max_len_input, ), dtype='int32', name='encoder_inputs')\n",
    "encoder_embeddings = shared_embedding(encoder_inputs)\n",
    "_, encoder_state_1, encoder_state_2 = encoder_gru(encoder_embeddings)\n",
    "encoder_states = L.concatenate([encoder_state_1, encoder_state_2])\n",
    "\n",
    "decoder_inputs = L.Input(shape=(max_len_target-1, ), dtype='int32', name='decoder_inputs')\n",
    "decoder_mask = L.Masking(mask_value=0)(decoder_inputs)\n",
    "decoder_embeddings_inputs = shared_embedding(decoder_mask)\n",
    "decoder_embeddings_outputs, _ = decoder_gru(decoder_embeddings_inputs, initial_state=encoder_states) \n",
    "decoder_outputs = decoder_dense(decoder_embeddings_outputs)\n",
    "\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "\n",
    "inference_encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "inference_decoder_state_inputs = L.Input(shape=(LATENT_DIM, ), dtype='float32', name='inference_decoder_state_inputs')\n",
    "inference_decoder_embeddings_outputs, inference_decoder_states = decoder_gru(\n",
    "    decoder_embeddings_inputs, initial_state=inference_decoder_state_inputs\n",
    ")\n",
    "inference_decoder_outputs = decoder_dense(inference_decoder_embeddings_outputs)\n",
    "\n",
    "inference_decoder_model = Model(\n",
    "    [decoder_inputs, inference_decoder_state_inputs], \n",
    "    [inference_decoder_outputs, inference_decoder_states]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:49.743487Z",
     "start_time": "2018-05-09T12:14:49.739412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, 63)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 63)           0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_inputs (InputLayer)     (None, 49)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    multiple             14272       encoder_inputs[0][0]             \n",
      "                                                                 masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bidirectional (Bidirect [(None, 512), (None, 493056      shared_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           encoder_bidirectional[0][1]      \n",
      "                                                                 encoder_bidirectional[0][2]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 63, 512), (N 886272      shared_embedding[1][0]           \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 63, 223)      114399      decoder_gru[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,507,999\n",
      "Trainable params: 1,507,999\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, 63)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 63)           0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    multiple             14272       masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "inference_decoder_state_inputs  (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 63, 512), (N 886272      shared_embedding[1][0]           \n",
      "                                                                 inference_decoder_state_inputs[0]\n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 63, 223)      114399      decoder_gru[1][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,014,943\n",
      "Trainable params: 1,014,943\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "inference_decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:14:49.776524Z",
     "start_time": "2018-05-09T12:14:49.744977Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(clipnorm=1.), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T13:05:34.423436Z",
     "start_time": "2018-05-09T12:14:49.778072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1140/1140 [==============================] - 150s 132ms/step - loss: 1.6295 - val_loss: 1.0509\n",
      "Epoch 2/20\n",
      "1140/1140 [==============================] - 149s 131ms/step - loss: 0.9899 - val_loss: 0.8707\n",
      "Epoch 3/20\n",
      "1140/1140 [==============================] - 150s 131ms/step - loss: 0.8693 - val_loss: 0.8034\n",
      "Epoch 4/20\n",
      "1140/1140 [==============================] - 152s 134ms/step - loss: 0.8083 - val_loss: 0.7608\n",
      "Epoch 5/20\n",
      "1140/1140 [==============================] - 149s 131ms/step - loss: 0.7702 - val_loss: 0.7329\n",
      "Epoch 6/20\n",
      "1140/1140 [==============================] - 149s 131ms/step - loss: 0.7431 - val_loss: 0.7152\n",
      "Epoch 7/20\n",
      "1140/1140 [==============================] - 149s 131ms/step - loss: 0.7227 - val_loss: 0.7028\n",
      "Epoch 8/20\n",
      "1140/1140 [==============================] - 149s 131ms/step - loss: 0.7060 - val_loss: 0.6874\n",
      "Epoch 9/20\n",
      "1140/1140 [==============================] - 149s 131ms/step - loss: 0.6929 - val_loss: 0.6860\n",
      "Epoch 10/20\n",
      "1140/1140 [==============================] - 150s 131ms/step - loss: 0.6820 - val_loss: 0.6753\n",
      "Epoch 11/20\n",
      "1140/1140 [==============================] - 151s 133ms/step - loss: 0.6720 - val_loss: 0.6724\n",
      "Epoch 12/20\n",
      "1140/1140 [==============================] - 150s 132ms/step - loss: 0.6640 - val_loss: 0.6654\n",
      "Epoch 13/20\n",
      "1140/1140 [==============================] - 150s 131ms/step - loss: 0.6569 - val_loss: 0.6613\n",
      "Epoch 14/20\n",
      "1140/1140 [==============================] - 150s 132ms/step - loss: 0.6501 - val_loss: 0.6564\n",
      "Epoch 15/20\n",
      "1140/1140 [==============================] - 161s 141ms/step - loss: 0.6445 - val_loss: 0.6574\n",
      "Epoch 16/20\n",
      "1140/1140 [==============================] - 158s 139ms/step - loss: 0.6389 - val_loss: 0.6527\n",
      "Epoch 17/20\n",
      "1140/1140 [==============================] - 150s 132ms/step - loss: 0.6344 - val_loss: 0.6447\n",
      "Epoch 18/20\n",
      "1140/1140 [==============================] - 156s 137ms/step - loss: 0.6298 - val_loss: 0.6462\n",
      "Epoch 19/20\n",
      "1140/1140 [==============================] - 160s 140ms/step - loss: 0.6258 - val_loss: 0.6467\n",
      "Epoch 20/20\n",
      "1140/1140 [==============================] - 160s 141ms/step - loss: 0.6220 - val_loss: 0.6405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb991098080>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = create_batch_generator(train_ids)\n",
    "val_generator = create_batch_generator(val_ids)\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(len(train_ids) / BATCH_SIZE),\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=np.ceil(len(val_ids) / BATCH_SIZE),\n",
    ")\n",
    "#model.fit([X, x_t_input], y_t_output, validation_split=0.1, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T13:05:34.429711Z",
     "start_time": "2018-05-09T13:05:34.424990Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = inference_encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, max_len_target-1))\n",
    "    target_seq[0, 0] = tokenizer.word_index[START]\n",
    "    \n",
    "    tokens = {idx: token for (token, idx) in tokenizer.word_index.items()}\n",
    "    \n",
    "    decoded_sequence = ''\n",
    "    for i in range(max_len_target):\n",
    "        output_tokens, output_states = inference_decoder_model.predict(\n",
    "            [target_seq, states_value]\n",
    "        )\n",
    "        \n",
    "        # greedy search\n",
    "        sampled_token_idx = np.argmax(output_tokens[0, 0, :])\n",
    "        sampled_token = tokens.get(sampled_token_idx, '.')\n",
    "        if sampled_token == END:\n",
    "            break\n",
    "        decoded_sequence += sampled_token\n",
    "            \n",
    "        target_seq[0, 0] = sampled_token_idx\n",
    "        states_value = output_states\n",
    "    \n",
    "    return decoded_sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T13:05:34.436767Z",
     "start_time": "2018-05-09T13:05:34.431105Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    return decode_sequence(keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences([sentence]),\n",
    "        padding='post',\n",
    "        maxlen=max_len_input,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T13:47:27.988241Z",
     "start_time": "2018-05-09T13:47:23.341287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hello.\\n' --> 'Halles.'\n",
      "'You are welcome.\\n' --> 'Sie sind bereits gesagt.'\n",
      "'How do you do?\\n' --> 'Wie viel zu tun?'\n",
      "'I hate mondays.\\n' --> 'Ich habe das gesagt.'\n",
      "'I am a programmer.\\n' --> 'Ich bin ein Problem dar.'\n",
      "'Data is the new oil.\\n' --> 'Das ist nicht der Fall.'\n",
      "'It could be worse.\\n' --> 'Sie könnte geschehen.'\n",
      "'I am on top of it.\\n' --> 'Ich bin sehr dankbar.'\n",
      "'N° Uno\\n' --> '\"Abstimmungen'\n",
      "'Awesome!\\n' --> 'Alles!'\n",
      "'Put your feet up!\\n' --> 'Last Sie sich darüber hinaus!'\n",
      "'From the start till the end!\\n' --> 'Freiheit ist der Grund für die Welt!'\n",
      "'From dusk till dawn.\\n' --> 'Freiheit kann die Liste geben.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{en!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T13:47:53.244725Z",
     "start_time": "2018-05-09T13:47:43.722593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \"Please rise, then, for this minute' s silence.\\n\", got 'Bitte stehen Sie das auch für die Frage.', exp: 'Ich bitte Sie, sich zu einer Schweigeminute zu erheben.\\n'\n",
      "Original 'Madam President, on a point of order.\\n', got 'Frau Präsidentin, zur Geschäftsordnung.', exp: 'Frau Präsidentin, zur Geschäftsordnung.\\n'\n",
      "Original 'Madam President, on a point of order.\\n', got 'Frau Präsidentin, zur Geschäftsordnung.', exp: 'Frau Präsidentin, zur Geschäftsordnung.\\n'\n",
      "Original 'Thank you, Mr Segni, I shall do so gladly.\\n', got 'Danke, Herr Schulz, das ich zu schätzen kann.', exp: 'Vielen Dank, Herr Segni, das will ich gerne tun.\\n'\n",
      "Original 'It is the case of Alexander Nikitin.\\n', got 'Das ist der Haushalt des Parlaments.', exp: 'Das ist der Fall von Alexander Nikitin.\\n'\n",
      "Original 'Why are there no fire instructions?\\n', got 'Warum schlagen Sie nicht aus der Welt?', exp: 'Warum finden keine Brandschutzbelehrungen statt?\\n'\n",
      "Original 'Mr Berenguer Fuster, we shall check all this.\\n', got 'Herr Barroso, wir werden das auch sagen.', exp: 'Lieber Kollege, wir werden das prüfen.\\n'\n",
      "Original 'We do not know what is happening.\\n', got 'Wir wissen nicht, was das geschieht.', exp: 'Wir wissen nicht, was passiert.\\n'\n",
      "Original 'Agenda\\n', got 'Tagesordnung', exp: 'Arbeitsplan\\n'\n",
      "Original 'Relating to Wednesday:\\n', got 'Zum Bericht: Herrn Schulz', exp: 'Zum Mittwoch:\\n'\n",
      "Original '(Applause from the PSE Group)\\n', got '(Beifall von der PPE-DE-Fraktion)', exp: '(Beifall der PSE-Fraktion)\\n'\n",
      "Original 'Mr Hänsch represented you on this occasion.\\n', got 'Herr Schulz hat uns das Wort gebeten.', exp: 'Der Kollege Hänsch hat Sie dort vertreten.\\n'\n",
      "Original 'We then put it to a vote.\\n', got 'Wir haben das auch gestimmt.', exp: 'Wir haben dann abgestimmt.\\n'\n",
      "Original 'There was a vote on this matter.\\n', got 'Es gab eine Antwort auf diesen Antrag.', exp: 'Es gab eine Abstimmung zu diesem Punkt.\\n'\n",
      "Original 'All of the others were of a different opinion.\\n', got 'Alle diese Frage wurden noch viel erreicht.', exp: 'Alle anderen waren anderer Meinung.\\n'\n",
      "Original 'That was the decision.\\n', got 'Das war der Antrag.', exp: 'Das war der Beschluß.\\n'\n",
      "Original 'There is no such document!\\n', got 'Es gibt keine Diskussion!', exp: 'Ein solches Dokument gibt es nicht!\\n'\n",
      "Original 'We have agreed to this.\\n', got 'Das haben wir getan.', exp: 'Wir haben dem zugestimmt.\\n'\n",
      "Original '(Applause from the PPE-DE Group)\\n', got '(Beifall von der PPE-DE-Fraktion)', exp: '(Beifall von der PPE-DE-Fraktion)\\n'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de[1:]!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T13:48:57.485205Z",
     "start_time": "2018-05-09T13:48:46.254643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'The vote will be taken tomorrow at 12 noon.\\n', got 'Die Abstimmung findet morgen um 12.00 Uhr statt.', exp: 'Die Abstimmung findet morgen um 12.00 Uhr statt.\\n'\n",
      "Original 'Composition of Parliament: see Minutes\\n', got 'Zusammensetzung des Parlaments: siehe Protokoll', exp: 'Zusammensetzung des Parlaments: siehe Protokoll\\n'\n",
      "Original 'Has this idea worked?\\n', got 'Ist das so wichtig?', exp: 'Hat das funktioniert?\\n'\n",
      "Original 'On this point only do I agree, and only just.\\n', got 'Darüber habe ich mich bei der Kommission gerne eine Antwort.', exp: 'Bei diesem Punkt stimme ich zu, aber das ist alles.\\n'\n",
      "Original '- Before the vote:\\n', got '- Vor der Abstimmung:', exp: '- Vor der Abstimmung\\n'\n",
      "Original 'It all depends how you look at it.\\n', got 'Das ist alles andere als das einzulehen.', exp: 'Das ist eine Frage der Sichtweise.\\n'\n",
      "Original 'We have not even been given a list of the cases.\\n', got 'Wir haben nicht genug gesagt, die Straßen zu verlieren.', exp: 'Wir bekommen nicht einmal eine Liste der Fälle.\\n'\n",
      "Original 'Alcohol is dangerous.\\n', got 'Albeitsplätze ist von großer Bedeutung.', exp: 'Alkohol ist gefährlich.\\n'\n",
      "Original '(The President cut off the speaker)\\n', got '(Der Präsident entzieht der Rednerin das Wort.)', exp: '(Der Präsident entzieht dem Redner das Wort.)\\n'\n",
      "Original 'So really only private entrepreneurs are living.\\n', got 'Die gesamte Erweiterung sind jedoch noch nicht zu bestehen.', exp: 'Eigentlich leben also nur die privaten Unternehmer.\\n'\n",
      "Original 'This was the first Statement of Assurance.\\n', got 'Das war die eine Strategie der Arbeitsplätze.', exp: 'Dies war die erste Zuverlässigkeitserklärung.\\n'\n",
      "Original 'The debate is closed.\\n', got 'Die Aussprache ist geschlossen.', exp: 'Die Aussprache ist geschlossen.\\n'\n",
      "Original 'I have seen none mentioned.\\n', got 'Ich habe noch nichts gesagt.', exp: 'Mir ist kein einziger solcher Fall zu Ohren gekommen.\\n'\n",
      "Original 'You have not been treated unfairly.\\n', got 'Sie haben eine sehr gute Idee gesprochen.', exp: 'Sie sind nicht ungerecht behandelt worden.\\n'\n",
      "Original 'Applause\\n', got 'Beifall', exp: 'Beifall\\n'\n",
      "Original 'The same can be said with regard to transfers.\\n', got 'Das Gleichgewicht kann doch einmal geschehen.', exp: 'Auf dem Gebiet der Transfers passiert dasselbe.\\n'\n",
      "Original 'Hoxha was no Tito, nor was he a Honecker.\\n', got 'Hoffnung war ein schlechter Fall von Frau Jachstel.', exp: 'Hodscha war nicht Tito, er war auch nicht Honecker.\\n'\n",
      "Original 'Phasing out nuclear power is hindering us.\\n', got 'Die Gesetzgebung ist in der Tat ein Ende.', exp: 'Der Ausstieg aus der Kernenergie hält uns auf.\\n'\n",
      "Original 'Naturally this would not be possible.\\n', got 'Das ist nicht möglich.', exp: 'Dies wäre natürlich ein Unding.\\n'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de[1:]!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T13:54:12.401823Z",
     "start_time": "2018-05-09T13:49:25.177452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8bfd8dadc94f38903e2ca2db28635c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average bleu score: 0.2279843693815539\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "try:\n",
    "    from spacy.lang.de import German\n",
    "except ModuleNotFoundError:\n",
    "    spacy.cli.download('de')\n",
    "    from spacy.lang.de import German\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "parser = German()\n",
    "chencherry = SmoothingFunction()  # to handle short sequences, see also http://www.nltk.org/_modules/nltk/translate/bleu_score.html#SmoothingFunction.method3\n",
    "\n",
    "def remove_spaces_and_puncts(tokens):\n",
    "     return [token.orth_ for token in tokens if not (token.is_space or token.is_punct)]  \n",
    "\n",
    "bleu_scores = np.zeros(TEST_SIZE)\n",
    "nist_scores = np.zeros(TEST_SIZE)\n",
    "\n",
    "for i in tqdm(range(TEST_SIZE)):\n",
    "    pred_tokens = remove_spaces_and_puncts(parser(predict(df.iloc[i].input_texts)))\n",
    "    ref_tokens = remove_spaces_and_puncts(parser(df.iloc[i].target_texts[1:]))\n",
    "    bleu_scores[i] = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=chencherry.method3)\n",
    "    \n",
    "print(\"Average bleu score:\", bleu_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T13:05:34.447388Z",
     "start_time": "2018-05-09T12:14:37.916Z"
    }
   },
   "outputs": [],
   "source": [
    "## Conclusion\n",
    "\n",
    "# It doesn't work perfect, but fine enough to show that seq2seq works in some way. I wouldn't be surprised if the mean average error is better than average human bias for calculating without any tools.\n",
    "# For improvements and further discussions I'll move to a real problem (translating) and main steps will be:\n",
    "# * Bytepairencoding/Word embeddings\n",
    "# * Beam Search\n",
    "# * Attention models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 301,
   "position": {
    "height": "40px",
    "left": "987px",
    "right": "23px",
    "top": "124px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
