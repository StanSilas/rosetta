{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple seq2seq model in keras that translates english <-> german"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step I take the model used for the [toy problem of adding/subtracting numbers](SimpleKerasModelForAddingAndSubstraction.ipynb) and train it with english/german data for machine translation.\n",
    "\n",
    "As trainings set I use the [European Parliament Proceedings Parallel Corpus 1996-2011](http://statmt.org/europarl/) German-English corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:23.601933Z",
     "start_time": "2018-05-09T09:25:22.255175Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# technical detail so that an instance (maybe running in a different window)\n",
    "# doesn't take all the GPU memory resulting in some strange error messages\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:23.864650Z",
     "start_time": "2018-05-09T09:25:23.603166Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "import keras\n",
    "import keras.layers as L\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Fixing random state ensure reproducible results\n",
    "RANDOM_STATE=42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.set_random_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:23.868184Z",
     "start_time": "2018-05-09T09:25:23.865970Z"
    }
   },
   "outputs": [],
   "source": [
    "START = '^'\n",
    "END = '\\n'\n",
    "\n",
    "MAX_INPUT_LENGTH = 25\n",
    "MAX_TARGET_LENGTH = 35\n",
    "LATENT_DIM = 512\n",
    "EMBEDDING_DIM = 64\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:23.876033Z",
     "start_time": "2018-05-09T09:25:23.869558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n"
     ]
    }
   ],
   "source": [
    "def download_file(fname, url):\n",
    "    print(f\"Downloading {fname} from {url} ...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    total_size = int(response.headers.get('content-length', 0)); \n",
    "    block_size = 1024\n",
    "\n",
    "    download = tqdm(\n",
    "        response.iter_content(block_size),\n",
    "        total=math.ceil(total_size // block_size),\n",
    "        unit='KB',\n",
    "        unit_scale=True\n",
    "    )\n",
    "    with open(f\"{fname}\", \"wb\") as handle:\n",
    "        for data in download:\n",
    "            handle.write(data)\n",
    "\n",
    "PATH = 'data'\n",
    "FILES = {\n",
    "    'de-en.tgz': 'http://statmt.org/europarl/v7/de-en.tgz',  # incredible: really only http, not https :-o\n",
    "}\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "for name, url in FILES.items():\n",
    "    fname = os.path.join(PATH, name)\n",
    "    exists = os.path.exists(fname)\n",
    "    size = os.path.getsize(fname) if exists else -1\n",
    "    if exists and size > 0:\n",
    "        print(f'{name} already downloaded ({size / 2**20:3.1f} MB)')\n",
    "        continue\n",
    "    download_file(fname, url)\n",
    "    if (fname.endswith(\".tgz\")):\n",
    "        tar = tarfile.open(fname, \"r:gz\")\n",
    "        tar.extractall(path=PATH)\n",
    "        tar.close()\n",
    "        print(f'Extracted {fname} ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:25.422556Z",
     "start_time": "2018-05-09T09:25:23.877617Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 60)\n",
    "df = pd.DataFrame(data={\n",
    "    'input_texts': open(f'{PATH}/europarl-v7.de-en.en', 'r').readlines(),\n",
    "    'target_texts': open(f'{PATH}/europarl-v7.de-en.de', 'r').readlines(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:26.801251Z",
     "start_time": "2018-05-09T09:25:25.425123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920209"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session\\n</td>\n",
       "      <td>^Wiederaufnahme der Sitzungsperiode\\n</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European Parliament...</td>\n",
       "      <td>^Ich erkläre die am Freitag, dem 17. Dezember unterbroch...</td>\n",
       "      <td>208</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded 'millennium...</td>\n",
       "      <td>^Wie Sie feststellen konnten, ist der gefürchtete \"Mille...</td>\n",
       "      <td>192</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in the cours...</td>\n",
       "      <td>^Im Parlament besteht der Wunsch nach einer Aussprache i...</td>\n",
       "      <td>106</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a minute' s si...</td>\n",
       "      <td>^Heute möchte ich Sie bitten - das ist auch der Wunsch e...</td>\n",
       "      <td>233</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input_texts  \\\n",
       "0                                  Resumption of the session\\n   \n",
       "1  I declare resumed the session of the European Parliament...   \n",
       "2  Although, as you will have seen, the dreaded 'millennium...   \n",
       "3  You have requested a debate on this subject in the cours...   \n",
       "4  In the meantime, I should like to observe a minute' s si...   \n",
       "\n",
       "                                                  target_texts  input_length  \\\n",
       "0                        ^Wiederaufnahme der Sitzungsperiode\\n            26   \n",
       "1  ^Ich erkläre die am Freitag, dem 17. Dezember unterbroch...           208   \n",
       "2  ^Wie Sie feststellen konnten, ist der gefürchtete \"Mille...           192   \n",
       "3  ^Im Parlament besteht der Wunsch nach einer Aussprache i...           106   \n",
       "4  ^Heute möchte ich Sie bitten - das ist auch der Wunsch e...           233   \n",
       "\n",
       "   target_length  \n",
       "0             36  \n",
       "1            220  \n",
       "2            187  \n",
       "3            112  \n",
       "4            219  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)\n",
    "df.target_texts = '^' + df.target_texts  # encode a start symbol (doesn't occur in texts)\n",
    "df['input_length'] = df.input_texts.apply(len)\n",
    "df['target_length'] = df.target_texts.apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only use short translations right now\n",
    "\n",
    "So, first I plot sentence length on a logarithmic scale, \n",
    "then I only choose short input texts (and a bit longer target texts as german is more verbose than english)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:27.254311Z",
     "start_time": "2018-05-09T09:25:26.802961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = df.input_length\n",
    "logbins = np.logspace(1,5,20)\n",
    "plt.hist(x, bins=logbins)\n",
    "plt.xscale('log')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:27.341432Z",
     "start_time": "2018-05-09T09:25:27.255838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38869"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_empty = (df.input_length > 1) & (df.target_length > 1)  # there are empty phrases like '\\n' --> 'Frau Präsidentin\\n'\n",
    "short_inputs = (df.input_length < MAX_INPUT_LENGTH) & (df.target_length < MAX_TARGET_LENGTH)\n",
    "sum(short_inputs)\n",
    "df = df[non_empty & short_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:27.348239Z",
     "start_time": "2018-05-09T09:25:27.343048Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = pd.concat([df.input_texts, df.target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:28.161281Z",
     "start_time": "2018-05-09T09:25:27.350251Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=100, filters=None, char_level=True, oov_token='~')\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "df['input_sequences'] = tokenizer.texts_to_sequences(df.input_texts)\n",
    "df['target_sequences'] = tokenizer.texts_to_sequences(df.target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:28.172580Z",
     "start_time": "2018-05-09T09:25:28.162809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 139610),\n",
       " ('e', 125420),\n",
       " ('s', 84275),\n",
       " ('\\n', 77738),\n",
       " ('i', 73905),\n",
       " ('t', 64268),\n",
       " ('n', 61648),\n",
       " ('a', 59430),\n",
       " ('r', 56788),\n",
       " ('h', 45028),\n",
       " ('.', 42011),\n",
       " ('o', 39448),\n",
       " ('^', 38869),\n",
       " ('l', 38270),\n",
       " ('d', 31131),\n",
       " ('u', 30441),\n",
       " ('c', 28894),\n",
       " ('g', 18621),\n",
       " ('p', 17190),\n",
       " ('\\xa0', 16655),\n",
       " ('m', 15943),\n",
       " ('b', 12933),\n",
       " ('f', 11191),\n",
       " ('D', 11113),\n",
       " ('A', 11021),\n",
       " ('T', 10666),\n",
       " ('w', 9392),\n",
       " ('k', 8609),\n",
       " ('!', 8388),\n",
       " ('(', 8289),\n",
       " (')', 8125),\n",
       " ('y', 7334),\n",
       " ('W', 7278),\n",
       " ('?', 6852),\n",
       " ('P', 6599),\n",
       " ('ä', 6250),\n",
       " ('B', 6080),\n",
       " ('H', 5920),\n",
       " ('I', 5808),\n",
       " ('v', 4337),\n",
       " ('S', 4330),\n",
       " ('E', 3409),\n",
       " ('z', 3206),\n",
       " (',', 2831),\n",
       " ('V', 2802),\n",
       " ('M', 2778),\n",
       " ('F', 2361),\n",
       " ('N', 2215),\n",
       " (':', 2146),\n",
       " ('ü', 1857),\n",
       " ('-', 1762),\n",
       " ('G', 1692),\n",
       " ('L', 1687),\n",
       " ('R', 1340),\n",
       " ('K', 1182),\n",
       " ('C', 1171),\n",
       " ('O', 1167),\n",
       " ('–', 1060),\n",
       " ('ö', 992),\n",
       " ('Z', 898),\n",
       " ('U', 807),\n",
       " ('ß', 802),\n",
       " ('1', 723),\n",
       " ('0', 708),\n",
       " ('j', 617),\n",
       " ('x', 588),\n",
       " ('2', 566),\n",
       " ('J', 429),\n",
       " ('Y', 344),\n",
       " ('q', 341),\n",
       " ('3', 265),\n",
       " ('9', 255),\n",
       " (\"'\", 237),\n",
       " ('4', 207),\n",
       " ('5', 201),\n",
       " ('7', 188),\n",
       " ('Q', 172),\n",
       " ('6', 162),\n",
       " ('\"', 161),\n",
       " ('Ä', 158),\n",
       " ('8', 131),\n",
       " ('/', 122),\n",
       " ('%', 60),\n",
       " ('é', 43),\n",
       " ('X', 38),\n",
       " ('á', 35),\n",
       " ('Ü', 33),\n",
       " ('°', 28),\n",
       " ('í', 27),\n",
       " ('’', 20),\n",
       " ('*', 17),\n",
       " ('Ö', 15),\n",
       " ('è', 13),\n",
       " ('“', 11),\n",
       " (';', 11),\n",
       " ('…', 10),\n",
       " ('‘', 10),\n",
       " (']', 8),\n",
       " ('[', 8),\n",
       " ('Å', 7),\n",
       " ('ó', 7),\n",
       " ('ã', 7),\n",
       " ('ń', 6),\n",
       " ('č', 6),\n",
       " ('ă', 5),\n",
       " ('š', 5),\n",
       " ('ò', 5),\n",
       " ('„', 4),\n",
       " ('ñ', 4),\n",
       " ('π', 4),\n",
       " ('ň', 4),\n",
       " ('о', 4),\n",
       " ('•', 4),\n",
       " ('ł', 4),\n",
       " ('Š', 3),\n",
       " ('Α', 3),\n",
       " ('ï', 3),\n",
       " ('´', 2),\n",
       " ('<', 2),\n",
       " ('ç', 2),\n",
       " ('ą', 2),\n",
       " ('ή', 2),\n",
       " ('χ', 2),\n",
       " ('ο', 2),\n",
       " ('ά', 2),\n",
       " ('τ', 2),\n",
       " ('α', 2),\n",
       " ('Κ', 2),\n",
       " ('ρ', 2),\n",
       " ('έ', 2),\n",
       " ('Υ', 2),\n",
       " ('ž', 2),\n",
       " ('ǎ', 2),\n",
       " ('ő', 2),\n",
       " ('¡', 2),\n",
       " ('ý', 2),\n",
       " ('и', 2),\n",
       " ('л', 2),\n",
       " ('ш', 2),\n",
       " ('д', 2),\n",
       " ('е', 2),\n",
       " ('р', 2),\n",
       " ('б', 2),\n",
       " ('Д', 2),\n",
       " ('+', 2),\n",
       " ('û', 2),\n",
       " ('à', 2),\n",
       " ('Í', 2),\n",
       " ('ì', 1),\n",
       " ('ę', 1),\n",
       " ('æ', 1),\n",
       " ('Ο', 1),\n",
       " ('Τ', 1),\n",
       " ('Ν', 1),\n",
       " ('ī', 1),\n",
       " ('Ï', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(sorted(tokenizer.word_counts.items(), key=lambda d: d[1])))\n",
    "sum(1 for w, count in tokenizer.word_counts.items() if count > 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:28.180754Z",
     "start_time": "2018-05-09T09:25:28.174275Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len_input = df.input_length.max()\n",
    "max_len_target = df.target_length.max()\n",
    "nr_tokens = len(tokenizer.word_index) + 1  # add 0 padding not in word_index contained\n",
    "\n",
    "# one hot encoded y_t_output wouldn't fit into memory any longer\n",
    "# so need to train/validate on batches generated on the fly\n",
    "def create_batch_generator(samples_ids):\n",
    "    \n",
    "    def batch_generator():\n",
    "        nr_batches = np.ceil(len(samples_ids) / BATCH_SIZE)\n",
    "        while True:\n",
    "            shuffled_ids = np.random.permutation(samples_ids)\n",
    "            batch_splits = np.array_split(shuffled_ids, nr_batches)\n",
    "            for batch_ids in batch_splits:\n",
    "                batch_X = pad_sequences(df.iloc[batch_ids].input_sequences, padding='post', maxlen=max_len_input)\n",
    "                batch_y = pad_sequences(df.iloc[batch_ids].target_sequences, padding='post', maxlen=max_len_target)\n",
    "                batch_y_t_output = keras.utils.to_categorical(batch_y[:,1:], num_classes=len(tokenizer.word_index)+1)\n",
    "                batch_x_t_input = batch_y[:,:-1]\n",
    "                yield ([batch_X, batch_x_t_input], batch_y_t_output)\n",
    "    \n",
    "    return batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:28.190423Z",
     "start_time": "2018-05-09T09:25:28.182258Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(df.shape[0]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:28.197099Z",
     "start_time": "2018-05-09T09:25:28.191761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_tokens\n",
    "len(tokenizer.word_index)\n",
    "len(train_ids), len(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:29.336008Z",
     "start_time": "2018-05-09T09:25:28.198916Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_gru = L.Bidirectional(\n",
    "    L.GRU(LATENT_DIM // 2, dropout=DROPOUT, return_state=True, name='encoder_gru'),\n",
    "    name='encoder_bidirectional'\n",
    ")\n",
    "decoder_gru = L.GRU(LATENT_DIM, dropout=DROPOUT, return_sequences=True, return_state=True, name='decoder_gru')\n",
    "decoder_dense = L.Dense(nr_tokens, activation='softmax', name='decoder_outputs')\n",
    "\n",
    "shared_embedding = L.Embedding(nr_tokens, EMBEDDING_DIM, mask_zero=True, name='shared_embedding')\n",
    "\n",
    "encoder_inputs = L.Input(shape=(max_len_input, ), dtype='int32', name='encoder_inputs')\n",
    "encoder_embeddings = shared_embedding(encoder_inputs)\n",
    "_, encoder_state_1, encoder_state_2 = encoder_gru(encoder_embeddings)\n",
    "encoder_states = L.concatenate([encoder_state_1, encoder_state_2])\n",
    "\n",
    "decoder_inputs = L.Input(shape=(max_len_target-1, ), dtype='int32', name='decoder_inputs')\n",
    "decoder_mask = L.Masking(mask_value=0)(decoder_inputs)\n",
    "decoder_embeddings_inputs = shared_embedding(decoder_mask)\n",
    "decoder_embeddings_outputs, _ = decoder_gru(decoder_embeddings_inputs, initial_state=encoder_states) \n",
    "decoder_outputs = decoder_dense(decoder_embeddings_outputs)\n",
    "\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "\n",
    "inference_encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "inference_decoder_state_inputs = L.Input(shape=(LATENT_DIM, ), dtype='float32', name='inference_decoder_state_inputs')\n",
    "inference_decoder_embeddings_outputs, inference_decoder_states = decoder_gru(\n",
    "    decoder_embeddings_inputs, initial_state=inference_decoder_state_inputs\n",
    ")\n",
    "inference_decoder_outputs = decoder_dense(inference_decoder_embeddings_outputs)\n",
    "\n",
    "inference_decoder_model = Model(\n",
    "    [decoder_inputs, inference_decoder_state_inputs], \n",
    "    [inference_decoder_outputs, inference_decoder_states]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:29.341277Z",
     "start_time": "2018-05-09T09:25:29.337362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, 33)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 33)           0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_inputs (InputLayer)     (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    multiple             10112       encoder_inputs[0][0]             \n",
      "                                                                 masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bidirectional (Bidirect [(None, 512), (None, 493056      shared_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           encoder_bidirectional[0][1]      \n",
      "                                                                 encoder_bidirectional[0][2]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 33, 512), (N 886272      shared_embedding[1][0]           \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 33, 158)      81054       decoder_gru[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,470,494\n",
      "Trainable params: 1,470,494\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, 33)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 33)           0           decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "shared_embedding (Embedding)    multiple             10112       masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "inference_decoder_state_inputs  (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(None, 33, 512), (N 886272      shared_embedding[1][0]           \n",
      "                                                                 inference_decoder_state_inputs[0]\n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, 33, 158)      81054       decoder_gru[1][0]                \n",
      "==================================================================================================\n",
      "Total params: 977,438\n",
      "Trainable params: 977,438\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "inference_decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:25:29.375394Z",
     "start_time": "2018-05-09T09:25:29.343169Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(clipnorm=1.), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:31:49.774228Z",
     "start_time": "2018-05-09T09:25:29.376679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "274/274 [==============================] - 20s 71ms/step - loss: 2.0358 - val_loss: 1.1416\n",
      "Epoch 2/20\n",
      "274/274 [==============================] - 18s 66ms/step - loss: 1.0139 - val_loss: 0.8499\n",
      "Epoch 3/20\n",
      "274/274 [==============================] - 18s 66ms/step - loss: 0.8176 - val_loss: 0.7277\n",
      "Epoch 4/20\n",
      "274/274 [==============================] - 19s 68ms/step - loss: 0.7160 - val_loss: 0.6672\n",
      "Epoch 5/20\n",
      "274/274 [==============================] - 19s 68ms/step - loss: 0.6490 - val_loss: 0.6228\n",
      "Epoch 6/20\n",
      "274/274 [==============================] - 18s 67ms/step - loss: 0.6009 - val_loss: 0.5927\n",
      "Epoch 7/20\n",
      "274/274 [==============================] - 19s 69ms/step - loss: 0.5613 - val_loss: 0.5674\n",
      "Epoch 8/20\n",
      "274/274 [==============================] - 18s 67ms/step - loss: 0.5291 - val_loss: 0.5516\n",
      "Epoch 9/20\n",
      "274/274 [==============================] - 18s 67ms/step - loss: 0.5019 - val_loss: 0.5243\n",
      "Epoch 10/20\n",
      "274/274 [==============================] - 19s 69ms/step - loss: 0.4791 - val_loss: 0.5359\n",
      "Epoch 11/20\n",
      "274/274 [==============================] - 18s 67ms/step - loss: 0.4585 - val_loss: 0.5249\n",
      "Epoch 12/20\n",
      "274/274 [==============================] - 19s 71ms/step - loss: 0.4406 - val_loss: 0.5179\n",
      "Epoch 13/20\n",
      "274/274 [==============================] - 19s 71ms/step - loss: 0.4228 - val_loss: 0.5115\n",
      "Epoch 14/20\n",
      "274/274 [==============================] - 19s 68ms/step - loss: 0.4078 - val_loss: 0.5099\n",
      "Epoch 15/20\n",
      "274/274 [==============================] - 20s 73ms/step - loss: 0.3933 - val_loss: 0.5013\n",
      "Epoch 16/20\n",
      "274/274 [==============================] - 20s 74ms/step - loss: 0.3810 - val_loss: 0.5049\n",
      "Epoch 17/20\n",
      "274/274 [==============================] - 19s 70ms/step - loss: 0.3699 - val_loss: 0.5005\n",
      "Epoch 18/20\n",
      "274/274 [==============================] - 19s 71ms/step - loss: 0.3567 - val_loss: 0.5162\n",
      "Epoch 19/20\n",
      "274/274 [==============================] - 19s 69ms/step - loss: 0.3483 - val_loss: 0.5048\n",
      "Epoch 20/20\n",
      "274/274 [==============================] - 19s 69ms/step - loss: 0.3377 - val_loss: 0.5180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f48deb9bdd8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = create_batch_generator(train_ids)\n",
    "val_generator = create_batch_generator(val_ids)\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(len(train_ids) / BATCH_SIZE),\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=np.ceil(len(val_ids) / BATCH_SIZE),\n",
    ")\n",
    "#model.fit([X, x_t_input], y_t_output, validation_split=0.1, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:32:53.973842Z",
     "start_time": "2018-05-09T09:32:53.968053Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = inference_encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, max_len_target-1))\n",
    "    target_seq[0, 0] = tokenizer.word_index[START]\n",
    "    \n",
    "    tokens = {idx: token for (token, idx) in tokenizer.word_index.items()}\n",
    "    \n",
    "    decoded_sequence = ''\n",
    "    for i in range(max_len_target):\n",
    "        output_tokens, output_states = inference_decoder_model.predict(\n",
    "            [target_seq, states_value]\n",
    "        )\n",
    "        \n",
    "        # greedy search\n",
    "        sampled_token_idx = np.argmax(output_tokens[0, 0, :])\n",
    "        sampled_token = tokens.get(sampled_token_idx, '.')\n",
    "        if sampled_token == END:\n",
    "            break\n",
    "        decoded_sequence += sampled_token\n",
    "            \n",
    "        target_seq[0, 0] = sampled_token_idx\n",
    "        states_value = output_states\n",
    "    \n",
    "    return decoded_sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:33:02.917189Z",
     "start_time": "2018-05-09T09:33:02.912815Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    return decode_sequence(keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences([sentence]),\n",
    "        padding='post',\n",
    "        maxlen=max_len_input,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:33:05.815749Z",
     "start_time": "2018-05-09T09:33:03.359738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hello.\\n' --> 'Hall.'\n",
      "'You are welcome.\\n' --> 'Sie haben vollkommen recht.'\n",
      "'How do you do?\\n' --> 'Wie können wir also tun?'\n",
      "'I hate mondays.\\n' --> 'Ich habe mich geman.'\n",
      "'I am a programmer.\\n' --> 'Ich bit ein Problem.'\n",
      "'Data is the new oil.\\n' --> 'Haute sind nicht nur das.'\n",
      "'It could be worse\\n' --> 'Sie haben der Kommission'\n",
      "'I am on top of it\\n' --> 'Ich bin auch der Meinung'\n",
      "'N° Uno\\n' --> 'Noxin'\n",
      "'Awesome!\\n' --> 'Einverstanden!'\n",
      "'Put your feet up!\\n' --> 'Das ist auch meine Antwort!'\n",
      "'From the start till the end!\\n' --> 'Der Beschäftigung für hinart!'\n",
      "'From dusk till dawn.\\n' --> 'Vor wende auf Ziel.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse',\n",
    "    \"I am on top of it\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{en!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:33:20.755420Z",
     "start_time": "2018-05-09T09:33:19.187499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original '\\n', got 'Herr Präsident!', exp: 'Frau Präsidentin!\\n'\n",
      "Original 'Agenda\\n', got 'Arbeitsplan', exp: 'Arbeitsplan\\n'\n",
      "Original 'Relating to Wednesday:\\n', got 'Zum Dienstag:', exp: 'Zum Mittwoch:\\n'\n",
      "Original 'That was the decision.\\n', got 'Das war die Frage.', exp: 'Das war der Beschluß.\\n'\n",
      "Original 'We have agreed to this.\\n', got 'Das haben wir getan.', exp: 'Wir haben dem zugestimmt.\\n'\n",
      "Original 'It is not a lot to ask.\\n', got 'Das ist nicht möglich.', exp: 'Das ist nicht zuviel verlangt.\\n'\n",
      "Original 'Thank you very much.\\n', got 'Vielen Dank.', exp: 'Vielen Dank.\\n'\n",
      "Original 'That did not happen.\\n', got 'Das geht nicht!', exp: 'Dazu kam es nicht.\\n'\n",
      "Original 'The debate is closed.\\n', got 'Die Aussprache ist geschlossen.', exp: 'Die Aussprache ist geschlossen.\\n'\n",
      "Original '\\n', got 'Herr Präsident!', exp: 'Herr Präsident!\\n'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in df[['input_texts', 'target_texts']][:10].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de[1:]!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:31:49.950890Z",
     "start_time": "2018-05-09T09:25:22.282Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['input_texts', 'target_texts']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:31:49.951578Z",
     "start_time": "2018-05-09T09:25:22.284Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Mean average error on a test set\n",
    "# test_df = create_equations_df(size=1000)\n",
    "# test_df['y_pred'] = test_df.input_texts.apply(predict).astype(int)\n",
    "# test_df['y_true'] = test_df.result\n",
    "# print(\"MAE\", np.mean(np.abs(test_df.y_pred - test_df.y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T09:31:49.952095Z",
     "start_time": "2018-05-09T09:25:22.286Z"
    }
   },
   "outputs": [],
   "source": [
    "## Conclusion\n",
    "\n",
    "# It doesn't work perfect, but fine enough to show that seq2seq works in some way. I wouldn't be surprised if the mean average error is better than average human bias for calculating without any tools.\n",
    "# For improvements and further discussions I'll move to a real problem (translating) and main steps will be:\n",
    "# * Bytepairencoding/Word embeddings\n",
    "# * Beam Search\n",
    "# * Attention models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 301,
   "position": {
    "height": "40px",
    "left": "987px",
    "right": "23px",
    "top": "124px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
