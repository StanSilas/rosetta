{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on large dataset with attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing [Beamsearch on a large dataset](BeamSearchOnLargeDataset.ipynb), I'll now add an attention model.\n",
    "As trainings set I use the [European Parliament Proceedings Parallel Corpus 1996-2011](http://statmt.org/europarl/).\n",
    "\n",
    "Here, it would helped to use `tf.keras` instead of `Keras` as there is attention model in tensorflow. There are also plans to add an attention layer to `Keras`, so I won't reimplement here, allthough it would be instructive. I'll use [keras-attention](https://github.com/datalogue/keras-attention) with an [AttentionDecoder Layer](https://github.com/datalogue/keras-attention/blob/master/models/custom_recurrents.py#L10). There is also a tutorial of how to use it at [machine learning mastery](https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/) and a [medium article about](https://medium.com/datalogue/attention-in-keras-1892773a4f22)\n",
    "\n",
    "Again, I'll refactor the code a bit, putting most of the implementation details into modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:14:27.924827Z",
     "start_time": "2018-06-02T14:14:25.788725Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed random seed to 42\n",
      "Set gpu memory fraction to 0.5\n"
     ]
    }
   ],
   "source": [
    "# import gc\n",
    "# import os\n",
    "# \n",
    "import keras\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import tensorflow as tf\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "#  \n",
    "# import bytepairencoding as bpe\n",
    "import seq2seq\n",
    "from utils.download import download_and_extract_resources\n",
    "from utils.linguistic import bleu_scores_europarl, preprocess_input_europarl as preprocess\n",
    "# \n",
    "# \n",
    "# # Fixing random state ensure reproducible results\n",
    "# RANDOM_STATE=42\n",
    "# np.random.seed(RANDOM_STATE)\n",
    "# tf.set_random_seed(RANDOM_STATE)\n",
    "# \n",
    "# pd.set_option('max_colwidth', 60)  # easier to read texts in e.g. df.head()\n",
    "# \n",
    "# # technical detail so that an instance (maybe running in a different window)\n",
    "# # doesn't take all the GPU memory resulting in some strange error messages\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "# set_session(tf.Session(config=config))\n",
    "\n",
    "from utils.preparation import Europarl, RANDOM_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:14:27.928620Z",
     "start_time": "2018-06-02T14:14:27.926244Z"
    }
   },
   "outputs": [],
   "source": [
    "# MAX_INPUT_LENGTH = 100  # was 50\n",
    "# MAX_TARGET_LENGTH = 125  # was 65\n",
    "# LATENT_DIM = 512\n",
    "# EMBEDDING_DIM = 300\n",
    "# BPE_MERGE_OPERATIONS = 5_000  # I'd love to use 10_000 x 300, but this one is broken: https://github.com/bheinzerling/bpemb/issues/6\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "DROPOUT = 0.5\n",
    "TEST_SIZE = 25 # 2_500  \n",
    "EMBEDDING_TRAINABLE = True  # Improves results significant and for at least it's not the most dominant training time factor (that's the output softmax layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:14:27.948405Z",
     "start_time": "2018-06-02T14:14:27.930131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n"
     ]
    }
   ],
   "source": [
    "# PATH = 'data'\n",
    "# INPUT_LANG = 'en'\n",
    "# TARGET_LANG = 'de'\n",
    "# LANGUAGES = [INPUT_LANG, TARGET_LANG]\n",
    "# BPE_URL = {lang: f'http://cosyne.h-its.org/bpemb/data/{lang}/' for lang in LANGUAGES}\n",
    "# BPE_MODEL_NAME = {lang: f'{lang}.wiki.bpe.op{BPE_MERGE_OPERATIONS}.model' for lang in LANGUAGES}\n",
    "# BPE_WORD2VEC_NAME = {lang: f'{lang}.wiki.bpe.op{BPE_MERGE_OPERATIONS}.d{EMBEDDING_DIM}.w2v.bin' for lang in LANGUAGES}\n",
    "# \n",
    "# EXTERNAL_RESOURCES = {\n",
    "#     # Europarl Corpus\n",
    "#     'de-en.tgz': 'http://statmt.org/europarl/v7/de-en.tgz',\n",
    "#     \n",
    "#     # Bytepairencoding subwords (_MODEL_) and pretrained embeddings (_WORD2VEC_)\n",
    "#     BPE_MODEL_NAME[INPUT_LANG]: f'{BPE_URL[INPUT_LANG]}/{BPE_MODEL_NAME[INPUT_LANG]}',\n",
    "#     BPE_WORD2VEC_NAME[INPUT_LANG] + '.tar.gz': f'{BPE_URL[INPUT_LANG]}/{BPE_WORD2VEC_NAME[INPUT_LANG]}' + '.tar.gz',\n",
    "#     BPE_MODEL_NAME[TARGET_LANG]: f'{BPE_URL[TARGET_LANG]}/{BPE_MODEL_NAME[TARGET_LANG]}',\n",
    "#     BPE_WORD2VEC_NAME[TARGET_LANG] + '.tar.gz': f'{BPE_URL[TARGET_LANG]}/{BPE_WORD2VEC_NAME[TARGET_LANG]}' + '.tar.gz',\n",
    "# }\n",
    "\n",
    "europarl = Europarl()\n",
    "download_and_extract_resources(fnames_and_urls=europarl.external_resources, dest_path=europarl.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:14:31.638742Z",
     "start_time": "2018-06-02T14:14:27.950354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unfiltered translations 1920209\n",
      "Filtered translations with length between (1, input=20/target=25) characters: 14943\n"
     ]
    }
   ],
   "source": [
    "europarl.load_and_preprocess(max_input_length=20, max_target_length=25)\n",
    "# df = pd.DataFrame(data={\n",
    "#     'input_texts': read_europarl(INPUT_LANG),\n",
    "#     'target_texts': read_europarl(TARGET_LANG)\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:14:31.659383Z",
     "start_time": "2018-06-02T14:14:31.639842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "      <th>input_sequences</th>\n",
       "      <th>target_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>agenda</td>\n",
       "      <td>arbeitsplan</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>[1, 631, 222, 34, 2]</td>\n",
       "      <td>[1, 941, 197, 3454, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>what is the result?</td>\n",
       "      <td>was sind die folgen?</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>[1, 781, 14, 3, 714, 2426, 2]</td>\n",
       "      <td>[1, 748, 126, 6, 2374, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>with what aim?</td>\n",
       "      <td>zu welchem zweck?</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>[1, 23, 781, 2973, 2426, 2]</td>\n",
       "      <td>[1, 26, 2740, 156, 155, 142, 359, 188, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>why?</td>\n",
       "      <td>wieso?</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>[1, 958, 38, 2426, 2]</td>\n",
       "      <td>[1, 167, 1659, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>no.</td>\n",
       "      <td>nein.</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, 220, 5, 2]</td>\n",
       "      <td>[1, 124, 191, 3, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              input_texts          target_texts  input_length  target_length  \\\n",
       "67                 agenda           arbeitsplan             6             11   \n",
       "704   what is the result?  was sind die folgen?            19             20   \n",
       "1261       with what aim?     zu welchem zweck?            14             17   \n",
       "1401                 why?                wieso?             4              6   \n",
       "1403                  no.                 nein.             3              5   \n",
       "\n",
       "                    input_sequences  \\\n",
       "67             [1, 631, 222, 34, 2]   \n",
       "704   [1, 781, 14, 3, 714, 2426, 2]   \n",
       "1261    [1, 23, 781, 2973, 2426, 2]   \n",
       "1401          [1, 958, 38, 2426, 2]   \n",
       "1403                 [1, 220, 5, 2]   \n",
       "\n",
       "                                     target_sequences  \n",
       "67                             [1, 941, 197, 3454, 2]  \n",
       "704                   [1, 748, 126, 6, 2374, 3720, 2]  \n",
       "1261  [1, 26, 2740, 156, 155, 142, 359, 188, 3720, 2]  \n",
       "1401                          [1, 167, 1659, 3720, 2]  \n",
       "1403                              [1, 124, 191, 3, 2]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"Nr total input:\", len(df))\n",
    "# df['input_length'] = df.input_texts.apply(len)\n",
    "# df['target_length'] = df.target_texts.apply(len)\n",
    "europarl.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter translations (only sentences shorter than a given length)\n",
    "\n",
    "With a full working machine translation system, it's of course better to train on all data (plus maybe some augmented data). Without attention (and maybe copy mechanism, dynamic memory, ...) there's no point anyway in it, but it also reduces training time (a full training on ~2 Mio translations might take days, even with a good GPU).\n",
    "I use different length for input (english) than target (german) language as german is more verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:14:31.674735Z",
     "start_time": "2018-06-02T14:14:31.660811Z"
    }
   },
   "outputs": [],
   "source": [
    "# non_empty = (df.input_length > 1) & (df.target_length > 1)  # there are empty phrases like '\\n' --> 'Frau Präsidentin\\n'\n",
    "# short_inputs = (df.input_length < MAX_INPUT_LENGTH) & (df.target_length < MAX_TARGET_LENGTH)\n",
    "# print(f'Sentences with length between (1, input={MAX_INPUT_LENGTH}/target={MAX_TARGET_LENGTH}) characters:', sum(non_empty & short_inputs))\n",
    "# df = df[non_empty & short_inputs]\n",
    "# gc.collect();  # df with filtered sentences is significant smaller, so time to garbage collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load (pretrained) Bytepairs\n",
    "\n",
    "I need the subwords dictionary (in `BPE_WORD2VEC_NAME`), the pretrained embeddings (in `BPE_MODEL_NAME`) and a [sentencepiece](https://github.com/google/sentencepiece) handler that can encode/decode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:14:31.686521Z",
     "start_time": "2018-06-02T14:14:31.676076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English subwords ['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n",
      "German subwords ['▁das', '▁ist', '▁ein', '▁test', '▁für', '▁v', 'ort', 'rain', 'ierte', '▁zeich', 'eng', 'ruppen']\n"
     ]
    }
   ],
   "source": [
    "# bpe_input, bpe_target = [bpe.Bytepairencoding(\n",
    "#     word2vec_fname=os.path.join(PATH, BPE_WORD2VEC_NAME[lang]),\n",
    "#     sentencepiece_fname=os.path.join(PATH, BPE_MODEL_NAME[lang]),\n",
    "# ) for lang in [INPUT_LANG, TARGET_LANG]] \n",
    "print(\"English subwords\", europarl.bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", europarl.bpe_target.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:14:31.704776Z",
     "start_time": "2018-06-02T14:14:31.688017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now encode the texts into sequences of indexes of bytepairs\n",
    "# df['input_sequences'] = df.input_texts.apply(bpe_input.subword_indices)\n",
    "# df['target_sequences'] = df.target_texts.apply(bpe_target.subword_indices)\n",
    "# df[['input_sequences', 'target_sequences']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:14:31.736237Z",
     "start_time": "2018-06-02T14:14:31.708410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Those will be the inputs for the seq2seq model (that needs to know how long the sequences can get)\n",
    "max_len_input = europarl.df.input_sequences.apply(len).max()\n",
    "max_len_target = europarl.df.target_sequences.apply(len).max()\n",
    "(max_len_input, max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:14:31.749213Z",
     "start_time": "2018-06-02T14:14:31.740751Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(europarl.df.shape[0]), test_size=0.1, random_state=RANDOM_STATE)  # fixed random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:16:10.799549Z",
     "start_time": "2018-06-02T14:14:31.754274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (?, ?, 512)\n"
     ]
    }
   ],
   "source": [
    "s2s = seq2seq.Seq2SeqWithBPE(\n",
    "    bpe_input=europarl.bpe_input,\n",
    "    bpe_target=europarl.bpe_target,\n",
    "    max_len_input=max_len_input,\n",
    "    max_len_target=max_len_target\n",
    ")\n",
    "#s2s.model.compile(optimizer=keras.optimizers.Adam(clipnorm=1., clipvalue=.5), loss='categorical_crossentropy')\n",
    "import keras.layers as L\n",
    "rnn_encoded = L.Bidirectional(\n",
    "    L.GRU(s2s.latent_dim // 2, return_sequences=True),\n",
    "    name='bidirectional_1',\n",
    "    merge_mode='concat'\n",
    ")(s2s.encoder_embeddings)\n",
    "attention_decoder = seq2seq.AttentionDecoder(\n",
    "    s2s.latent_dim,\n",
    "    len(europarl.bpe_target.tokens),\n",
    "    trainable=EMBEDDING_TRAINABLE,\n",
    ")(rnn_encoded)\n",
    "\n",
    "# train_generator = s2s.create_batch_generator(\n",
    "#     train_ids, europarl.df.input_sequences, europarl.df.target_sequences, BATCH_SIZE\n",
    "# )\n",
    "# val_generator = s2s.create_batch_generator(\n",
    "#     val_ids, europarl.df.input_sequences, europarl.df.target_sequences, BATCH_SIZE\n",
    "# )\n",
    "# \n",
    "# s2s.model.fit_generator(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=np.ceil(len(train_ids) / BATCH_SIZE),\n",
    "#     epochs=EPOCHS,\n",
    "#     validation_data=val_generator,\n",
    "#     validation_steps=np.ceil(len(val_ids) / BATCH_SIZE),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:16:10.826023Z",
     "start_time": "2018-06-02T14:16:10.800778Z"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=s2s.encoder_inputs, outputs=attention_decoder)\n",
    "model.compile(optimizer=keras.optimizers.Adam(clipnorm=1., clipvalue=.5), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:16:10.830629Z",
     "start_time": "2018-06-02T14:16:10.827218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_inputs (InputLayer)  (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "input_embedding (Embedding)  (None, 15, 302)           1760358   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 15, 512)           858624    \n",
      "_________________________________________________________________\n",
      "AttentionDecoder (AttentionD (None, 15, 5367)          44911432  \n",
      "=================================================================\n",
      "Total params: 47,530,414\n",
      "Trainable params: 47,530,414\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:38:11.733812Z",
     "start_time": "2018-06-02T14:16:10.831748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "421/421 [==============================] - 69s 164ms/step - loss: 2.7113 - val_loss: 2.2238\n",
      "Epoch 2/20\n",
      "421/421 [==============================] - 65s 154ms/step - loss: 1.7722 - val_loss: 1.9453\n",
      "Epoch 3/20\n",
      "421/421 [==============================] - 65s 154ms/step - loss: 1.4260 - val_loss: 1.8952\n",
      "Epoch 4/20\n",
      "421/421 [==============================] - 65s 154ms/step - loss: 1.1826 - val_loss: 1.7461\n",
      "Epoch 5/20\n",
      "421/421 [==============================] - 65s 155ms/step - loss: 1.0005 - val_loss: 1.8526\n",
      "Epoch 6/20\n",
      "421/421 [==============================] - 65s 154ms/step - loss: 0.8614 - val_loss: 1.8351\n",
      "Epoch 7/20\n",
      "421/421 [==============================] - 65s 154ms/step - loss: 0.7547 - val_loss: 1.9537\n",
      "Epoch 8/20\n",
      "421/421 [==============================] - 65s 154ms/step - loss: 0.6726 - val_loss: 1.9248\n",
      "Epoch 9/20\n",
      "421/421 [==============================] - 65s 155ms/step - loss: 0.6092 - val_loss: 2.0469\n",
      "Epoch 10/20\n",
      "421/421 [==============================] - 65s 155ms/step - loss: 0.5615 - val_loss: 2.0441\n",
      "Epoch 11/20\n",
      "421/421 [==============================] - 66s 156ms/step - loss: 0.5207 - val_loss: 2.0288\n",
      "Epoch 12/20\n",
      "421/421 [==============================] - 66s 157ms/step - loss: 0.4976 - val_loss: 2.1052\n",
      "Epoch 13/20\n",
      "421/421 [==============================] - 66s 158ms/step - loss: 0.4761 - val_loss: 1.9816\n",
      "Epoch 14/20\n",
      "421/421 [==============================] - 67s 158ms/step - loss: 0.4552 - val_loss: 2.1515\n",
      "Epoch 15/20\n",
      "421/421 [==============================] - 67s 158ms/step - loss: 0.4505 - val_loss: 2.0802\n",
      "Epoch 16/20\n",
      "421/421 [==============================] - 67s 158ms/step - loss: 0.4350 - val_loss: 2.1075\n",
      "Epoch 17/20\n",
      "421/421 [==============================] - 67s 158ms/step - loss: 0.4244 - val_loss: 2.1949\n",
      "Epoch 18/20\n",
      "421/421 [==============================] - 67s 158ms/step - loss: 0.4170 - val_loss: 2.1398\n",
      "Epoch 19/20\n",
      "421/421 [==============================] - 67s 158ms/step - loss: 0.4129 - val_loss: 2.1651\n",
      "Epoch 20/20\n",
      "421/421 [==============================] - 67s 158ms/step - loss: 0.3994 - val_loss: 2.1481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6fd7b2f470>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_batch_generator(\n",
    "    samples_ids, input_sequences, target_sequences, batch_size\n",
    "):\n",
    "\n",
    "    def batch_generator():\n",
    "        nr_batches = np.ceil(len(samples_ids) / batch_size)\n",
    "        while True:\n",
    "            shuffled_ids = np.random.permutation(samples_ids)\n",
    "            batch_splits = np.array_split(shuffled_ids, nr_batches)\n",
    "            for batch_ids in batch_splits:\n",
    "                batch_X = pad_sequences(\n",
    "                    input_sequences.iloc[batch_ids],\n",
    "                    padding='post',\n",
    "                    maxlen=max_len_input\n",
    "                )\n",
    "                batch_y = pad_sequences(\n",
    "                    target_sequences.iloc[batch_ids],\n",
    "                    padding='post',\n",
    "                    maxlen=max_len_target\n",
    "                )\n",
    "                batch_y_t_output = keras.utils.to_categorical(\n",
    "                    batch_y[:, 1:],\n",
    "                    num_classes=len(europarl.bpe_target.tokens)\n",
    "                )\n",
    "                batch_x_t_input = batch_y[:, :-1]\n",
    "                #yield ([batch_X, batch_x_t_input], batch_y_t_output)\n",
    "                yield(batch_X, batch_y_t_output)\n",
    "        \n",
    "    return batch_generator()\n",
    "\n",
    "train_generator = create_batch_generator(\n",
    "    train_ids, europarl.df.input_sequences, europarl.df.target_sequences, BATCH_SIZE\n",
    ")\n",
    "val_generator = create_batch_generator(\n",
    "    val_ids, europarl.df.input_sequences, europarl.df.target_sequences, BATCH_SIZE\n",
    ")\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(len(train_ids) / BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=np.ceil(len(val_ids) / BATCH_SIZE),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T14:38:12.181428Z",
     "start_time": "2018-06-02T14:38:11.736284Z"
    }
   },
   "outputs": [],
   "source": [
    "name = 'attentionmodel'\n",
    "model.save_weights(f'data/{name}_model_weights.h5') \n",
    "# s2s.model.save_weights(f'data/{name}_model_weights.h5')  # https://drive.google.com/open?id=10Sv-JnAiUT_fvU_cw1_H7mkcTAipC5aA\n",
    "# s2s.inference_encoder_model.save_weights(f'data/{name}_inference_encoder_model_weights.h5')  # https://drive.google.com/open?id=1gNBrn_Wij0PyeE-jJsEnlv7aHXkYuAup\n",
    "# s2s.inference_decoder_model.save_weights(f'data/{name}_inference_decoder_model_weights.h5')  # https://drive.google.com/open?id=1LCU53Hnb4m42QO3qsZTAkyYyroqz2vbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T12:17:53.095449Z",
     "start_time": "2018-06-02T12:17:53.092403Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence, beam_width=5):\n",
    "    return s2s.decode_beam_search(pad_sequences(\n",
    "        [europarl.bpe_input.subword_indices(preprocess(sentence))],\n",
    "        padding='post',\n",
    "        maxlen=max_len_input,\n",
    "    ), beam_width=beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T12:17:56.589523Z",
     "start_time": "2018-06-02T12:17:54.146989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello.' --> 'hallo!'\n",
      "'you are welcome.' --> 'sie sind willkommen!'\n",
      "'how do you do?' --> 'wie können sie das tun?'\n",
      "'i hate mondays.' --> 'das tut mir leid.'\n",
      "'i am a programmer.' --> 'darauf bin ich gespannt.'\n",
      "'data is the new oil.' --> 'das ist die folge.'\n",
      "'it could be worse.' --> 'dies war überbucht.'\n",
      "'i am on top of it.' --> 'ich bin es leid.'\n",
      "'n° uno' --> 'zärger beifall)'\n",
      "'awesome!' --> 'klar!'\n",
      "'put your feet up!' --> 'bitte fahren sie sich!'\n",
      "'from the start till the end!' --> 'im gegenteil!'\n",
      "'from dusk till dawn.' --> 'ja, frau harms.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T12:18:18.512473Z",
     "start_time": "2018-06-02T12:18:15.885640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'what is the result?', got 'mit welchem ergebnis?', exp: 'was sind die folgen?'\n",
      "Original 'with what aim?', got 'zu welchem zweck?', exp: 'zu welchem zweck?'\n",
      "Original 'why?', got 'warum?', exp: 'wieso?'\n",
      "Original 'no.', got 'nein.', exp: 'nein.'\n",
      "Original 'just like europol.', got 'genau wie europol.', exp: 'genau wie europol.'\n",
      "Original 'vote', got 'abstimmungen', exp: 'abstimmungen'\n",
      "Original 'why not?', got 'warum nicht?', exp: 'warum?'\n",
      "Original 'and now the erika.', got 'und zu den philippinen', exp: 'und nun erika.'\n",
      "Original 'they want answers.', got 'sie wollen antworten.', exp: 'sie wollen antworten.'\n",
      "Original 'storms in europe', got 'stürme in europa', exp: 'stürme in europa'\n",
      "Original 'food safety', got 'lebensmittelsicherheit', exp: 'lebensmittelsicherheit'\n",
      "Original 'first part', got 'teil i', exp: 'teil i'\n",
      "Original 'if not, why not?', got 'wenn nicht, warum?', exp: 'wenn nicht, warum nicht?'\n",
      "Original 'second part', got 'teil ii', exp: 'teil ii'\n",
      "Original '0 discharge', got 'entlastung 0', exp: 'entlastung 0'\n",
      "Original 'that is important.', got 'das ist wichtig.', exp: 'das ist wichtig.'\n",
      "Original 'is this possible?', got 'kann das sein?', exp: 'kann das sein?'\n",
      "Original 'let us be honest.', got 'seien wir doch ehrlich.', exp: 'seien wir doch ehrlich.'\n",
      "Original 'where is the beef?', got 'wo ist die risiko?', exp: 'where is the beef?'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in europarl.df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T12:18:29.597180Z",
     "start_time": "2018-06-02T12:18:27.199158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'yes.', got 'ja.', exp: 'ja.'\n",
      "Original 'why?', got 'warum?', exp: 'warum?'\n",
      "Original '(loud applause)', got '(lebhafter beifall)', exp: '(lebhafter beifall)'\n",
      "Original 'loud applause', got 'lebhafter beifall', exp: 'lebhafter beifall'\n",
      "Original 'why?', got 'warum?', exp: 'warum?'\n",
      "Original 'president.', got 'der präsident.', exp: 'die präsidentin.'\n",
      "Original 'consumer protection', got 'verbraucherschutz', exp: 'verbraucherschutz'\n",
      "Original 'biocidal products', got 'biozid-produkte', exp: 'biozid-produkte'\n",
      "Original '(applause)', got '(beifall)', exp: '(beifall)'\n",
      "Original 'applause', got 'beifall', exp: 'beifall'\n",
      "Original 'tempus fugit!', got 'zu den tat!', exp: 'die zeit drängt!'\n",
      "Original 'hence this debate.', got 'das wäre nett.', exp: 'deshalb diese debatte.'\n",
      "Original '(applause)', got '(beifall)', exp: '(beifall)'\n",
      "Original 'maes (verts/ale).', got 'maes (verts/ale).', exp: 'maes (verts/ale).'\n",
      "Original '(applause)', got '(beifall)', exp: '(beifall)'\n",
      "Original 'how can this be?', got 'wie ist das möglich!', exp: 'aus welchem grund?'\n",
      "Original '(applause)', got '(beifall)', exp: '(beifall)'\n",
      "Original 'thank you.', got 'vielen dank.', exp: 'danke.'\n",
      "Original 'we were deceived.', got 'wir übrigens auch.', exp: 'wir wurden betrogen.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = europarl.df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T12:19:12.897529Z",
     "start_time": "2018-06-02T12:19:09.811397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fb26f9bd97418f9c098e1a43699dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.27822399794374275\n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Translations for short sentences are looking decent. But it's also obvious that for longer sentences the translation gets lost somehow in the sentence and alltough the translated sentence is related to a real translation, it's also confusing and self-repeating.\n",
    "It's worth to notice that the sentences are not too long for a LSTM/GRU model (52, 71) bytepairs for encoding/decoding network. LSTM/GRUs are known to handle sequences up to 100 elements and start decreasing performance at around 60 (for at least that's what the Stanford courses say). So, it could be that a long enough training (we can see here that the training progresses epoch for epoch what's really nice to see for large data) would solve the problem for the choosen sentence lengths here. But of course, it's better to do what humans do also and applicate an attention model instead of trying to keep everything condensed in 512 float32 embedding while also generating bytepair for bytepair.\n",
    "This model is also already a realistic model in terms of training time. I needed around 18h on a GTX1080. Beside implementing attention model, it is tempting to see how a convolutional network might improve the runtime performance (and also quality). But let's get first to Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 294,
   "position": {
    "height": "40px",
    "left": "553px",
    "right": "192px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
