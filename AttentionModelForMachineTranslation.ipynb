{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on large dataset with attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing [Beamsearch on a large dataset](BeamSearchOnLargeDataset.ipynb), I'll now add an attention model.\n",
    "As trainings set I use the [European Parliament Proceedings Parallel Corpus 1996-2011](http://statmt.org/europarl/).\n",
    "\n",
    "Here, it would helped to use `tf.keras` instead of `Keras` as there is attention model in tensorflow. There are also plans to add an attention layer to `Keras`, so I won't reimplement here, allthough it would be instructive. I'll use [keras-attention](https://github.com/datalogue/keras-attention) with an [AttentionDecoder Layer](https://github.com/datalogue/keras-attention/blob/master/models/custom_recurrents.py#L10). There is also a tutorial of how to use it at [machine learning mastery](https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/) and a [medium article about](https://medium.com/datalogue/attention-in-keras-1892773a4f22)\n",
    "\n",
    "Again, I'll refactor the code a bit, putting most of the implementation details into modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T12:35:56.481412Z",
     "start_time": "2018-06-03T12:35:54.739928Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed random seed to 42\n",
      "Set gpu memory fraction to 0.5\n"
     ]
    }
   ],
   "source": [
    "# import gc\n",
    "# import os\n",
    "# \n",
    "import keras\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import tensorflow as tf\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "#  \n",
    "# import bytepairencoding as bpe\n",
    "import seq2seq\n",
    "from utils.download import download_and_extract_resources\n",
    "from utils.linguistic import bleu_scores_europarl, preprocess_input_europarl as preprocess\n",
    "# \n",
    "# \n",
    "# # Fixing random state ensure reproducible results\n",
    "# RANDOM_STATE=42\n",
    "# np.random.seed(RANDOM_STATE)\n",
    "# tf.set_random_seed(RANDOM_STATE)\n",
    "# \n",
    "# pd.set_option('max_colwidth', 60)  # easier to read texts in e.g. df.head()\n",
    "# \n",
    "# # technical detail so that an instance (maybe running in a different window)\n",
    "# # doesn't take all the GPU memory resulting in some strange error messages\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "# set_session(tf.Session(config=config))\n",
    "\n",
    "from utils.preparation import Europarl, RANDOM_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T12:35:56.485915Z",
     "start_time": "2018-06-03T12:35:56.483148Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 20 #100  # was 50\n",
    "MAX_TARGET_LENGTH = 25 # 125  # was 65\n",
    "# LATENT_DIM = 512\n",
    "# EMBEDDING_DIM = 300\n",
    "# BPE_MERGE_OPERATIONS = 5_000  # I'd love to use 10_000 x 300, but this one is broken: https://github.com/bheinzerling/bpemb/issues/6\n",
    "EPOCHS = 5 #20\n",
    "BATCH_SIZE = 32\n",
    "DROPOUT = 0.5\n",
    "TEST_SIZE = 25 # 2_500  \n",
    "EMBEDDING_TRAINABLE = True  # Improves results significant and for at least it's not the most dominant training time factor (that's the output softmax layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T12:35:56.498488Z",
     "start_time": "2018-06-03T12:35:56.487588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n"
     ]
    }
   ],
   "source": [
    "# PATH = 'data'\n",
    "# INPUT_LANG = 'en'\n",
    "# TARGET_LANG = 'de'\n",
    "# LANGUAGES = [INPUT_LANG, TARGET_LANG]\n",
    "# BPE_URL = {lang: f'http://cosyne.h-its.org/bpemb/data/{lang}/' for lang in LANGUAGES}\n",
    "# BPE_MODEL_NAME = {lang: f'{lang}.wiki.bpe.op{BPE_MERGE_OPERATIONS}.model' for lang in LANGUAGES}\n",
    "# BPE_WORD2VEC_NAME = {lang: f'{lang}.wiki.bpe.op{BPE_MERGE_OPERATIONS}.d{EMBEDDING_DIM}.w2v.bin' for lang in LANGUAGES}\n",
    "# \n",
    "# EXTERNAL_RESOURCES = {\n",
    "#     # Europarl Corpus\n",
    "#     'de-en.tgz': 'http://statmt.org/europarl/v7/de-en.tgz',\n",
    "#     \n",
    "#     # Bytepairencoding subwords (_MODEL_) and pretrained embeddings (_WORD2VEC_)\n",
    "#     BPE_MODEL_NAME[INPUT_LANG]: f'{BPE_URL[INPUT_LANG]}/{BPE_MODEL_NAME[INPUT_LANG]}',\n",
    "#     BPE_WORD2VEC_NAME[INPUT_LANG] + '.tar.gz': f'{BPE_URL[INPUT_LANG]}/{BPE_WORD2VEC_NAME[INPUT_LANG]}' + '.tar.gz',\n",
    "#     BPE_MODEL_NAME[TARGET_LANG]: f'{BPE_URL[TARGET_LANG]}/{BPE_MODEL_NAME[TARGET_LANG]}',\n",
    "#     BPE_WORD2VEC_NAME[TARGET_LANG] + '.tar.gz': f'{BPE_URL[TARGET_LANG]}/{BPE_WORD2VEC_NAME[TARGET_LANG]}' + '.tar.gz',\n",
    "# }\n",
    "\n",
    "europarl = Europarl()\n",
    "download_and_extract_resources(fnames_and_urls=europarl.external_resources, dest_path=europarl.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T12:36:00.115701Z",
     "start_time": "2018-06-03T12:35:56.500299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unfiltered translations 1920209\n",
      "Filtered translations with length between (1, input=20/target=25) characters: 14943\n"
     ]
    }
   ],
   "source": [
    "europarl.load_and_preprocess(max_input_length=MAX_INPUT_LENGTH, max_target_length=MAX_TARGET_LENGTH)\n",
    "# df = pd.DataFrame(data={\n",
    "#     'input_texts': read_europarl(INPUT_LANG),\n",
    "#     'target_texts': read_europarl(TARGET_LANG)\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T12:36:00.136684Z",
     "start_time": "2018-06-03T12:36:00.117034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "      <th>input_sequences</th>\n",
       "      <th>target_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>agenda</td>\n",
       "      <td>arbeitsplan</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>[1, 631, 222, 34, 2]</td>\n",
       "      <td>[1, 941, 197, 3454, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>what is the result?</td>\n",
       "      <td>was sind die folgen?</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>[1, 781, 14, 3, 714, 2426, 2]</td>\n",
       "      <td>[1, 748, 126, 6, 2374, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>with what aim?</td>\n",
       "      <td>zu welchem zweck?</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>[1, 23, 781, 2973, 2426, 2]</td>\n",
       "      <td>[1, 26, 2740, 156, 155, 142, 359, 188, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>why?</td>\n",
       "      <td>wieso?</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>[1, 958, 38, 2426, 2]</td>\n",
       "      <td>[1, 167, 1659, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>no.</td>\n",
       "      <td>nein.</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, 220, 5, 2]</td>\n",
       "      <td>[1, 124, 191, 3, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              input_texts          target_texts  input_length  target_length  \\\n",
       "67                 agenda           arbeitsplan             6             11   \n",
       "704   what is the result?  was sind die folgen?            19             20   \n",
       "1261       with what aim?     zu welchem zweck?            14             17   \n",
       "1401                 why?                wieso?             4              6   \n",
       "1403                  no.                 nein.             3              5   \n",
       "\n",
       "                    input_sequences  \\\n",
       "67             [1, 631, 222, 34, 2]   \n",
       "704   [1, 781, 14, 3, 714, 2426, 2]   \n",
       "1261    [1, 23, 781, 2973, 2426, 2]   \n",
       "1401          [1, 958, 38, 2426, 2]   \n",
       "1403                 [1, 220, 5, 2]   \n",
       "\n",
       "                                     target_sequences  \n",
       "67                             [1, 941, 197, 3454, 2]  \n",
       "704                   [1, 748, 126, 6, 2374, 3720, 2]  \n",
       "1261  [1, 26, 2740, 156, 155, 142, 359, 188, 3720, 2]  \n",
       "1401                          [1, 167, 1659, 3720, 2]  \n",
       "1403                              [1, 124, 191, 3, 2]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"Nr total input:\", len(df))\n",
    "# df['input_length'] = df.input_texts.apply(len)\n",
    "# df['target_length'] = df.target_texts.apply(len)\n",
    "europarl.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter translations (only sentences shorter than a given length)\n",
    "\n",
    "With a full working machine translation system, it's of course better to train on all data (plus maybe some augmented data). Without attention (and maybe copy mechanism, dynamic memory, ...) there's no point anyway in it, but it also reduces training time (a full training on ~2 Mio translations might take days, even with a good GPU).\n",
    "I use different length for input (english) than target (german) language as german is more verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T12:36:00.148824Z",
     "start_time": "2018-06-03T12:36:00.137996Z"
    }
   },
   "outputs": [],
   "source": [
    "# non_empty = (df.input_length > 1) & (df.target_length > 1)  # there are empty phrases like '\\n' --> 'Frau Präsidentin\\n'\n",
    "# short_inputs = (df.input_length < MAX_INPUT_LENGTH) & (df.target_length < MAX_TARGET_LENGTH)\n",
    "# print(f'Sentences with length between (1, input={MAX_INPUT_LENGTH}/target={MAX_TARGET_LENGTH}) characters:', sum(non_empty & short_inputs))\n",
    "# df = df[non_empty & short_inputs]\n",
    "# gc.collect();  # df with filtered sentences is significant smaller, so time to garbage collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load (pretrained) Bytepairs\n",
    "\n",
    "I need the subwords dictionary (in `BPE_WORD2VEC_NAME`), the pretrained embeddings (in `BPE_MODEL_NAME`) and a [sentencepiece](https://github.com/google/sentencepiece) handler that can encode/decode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T12:36:00.162137Z",
     "start_time": "2018-06-03T12:36:00.150345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English subwords ['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n",
      "German subwords ['▁das', '▁ist', '▁ein', '▁test', '▁für', '▁v', 'ort', 'rain', 'ierte', '▁zeich', 'eng', 'ruppen']\n"
     ]
    }
   ],
   "source": [
    "# bpe_input, bpe_target = [bpe.Bytepairencoding(\n",
    "#     word2vec_fname=os.path.join(PATH, BPE_WORD2VEC_NAME[lang]),\n",
    "#     sentencepiece_fname=os.path.join(PATH, BPE_MODEL_NAME[lang]),\n",
    "# ) for lang in [INPUT_LANG, TARGET_LANG]] \n",
    "print(\"English subwords\", europarl.bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", europarl.bpe_target.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T12:36:00.177446Z",
     "start_time": "2018-06-03T12:36:00.163622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now encode the texts into sequences of indexes of bytepairs\n",
    "# df['input_sequences'] = df.input_texts.apply(bpe_input.subword_indices)\n",
    "# df['target_sequences'] = df.target_texts.apply(bpe_target.subword_indices)\n",
    "# df[['input_sequences', 'target_sequences']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T12:36:00.196786Z",
     "start_time": "2018-06-03T12:36:00.179115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Those will be the inputs for the seq2seq model (that needs to know how long the sequences can get)\n",
    "max_len_input = europarl.df.input_sequences.apply(len).max()\n",
    "max_len_target = europarl.df.target_sequences.apply(len).max()\n",
    "(max_len_input, max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T12:36:00.203241Z",
     "start_time": "2018-06-03T12:36:00.198320Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(europarl.df.shape[0]), test_size=0.1, random_state=RANDOM_STATE)  # fixed random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T12:36:00.266874Z",
     "start_time": "2018-06-03T12:36:00.204872Z"
    }
   },
   "outputs": [],
   "source": [
    "# modified version of https://github.com/datalogue/keras-attention/\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                            input_dim=None, output_dim=None,\n",
    "                            timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # maybe below is more clear implementation compared to older keras\n",
    "    # at least it works the same for tensorflow, but not tested on other backends\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    return x\n",
    "\n",
    "\n",
    "class AttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, alphabet_size,\n",
    "                 embedding_dim=30,\n",
    "                 is_monotonic=False,\n",
    "                 normalize_energy=False,\n",
    "                 activation='tanh',\n",
    "                 dropout=None,\n",
    "                 recurrent_dropout=None,\n",
    "                 return_probabilities=False,\n",
    "                 name='AttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states\n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param alphabet_size: output sequence alphabet size\n",
    "            (alphabet may contain <end_of_seq> but do not need to have <start_of_seq>,\n",
    "            because it is added internally inside the layer)\n",
    "        :param embedding_dim: size of internal embedding for output labels\n",
    "        :param is_monotonic: if True - Luong-style monotonic attention\n",
    "            if False - Bahdanau-style attention (non-monotonic)\n",
    "            See references for details\n",
    "        :param normalize_energy: whether attention weights are normalized\n",
    "        references:\n",
    "            (1) Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
    "            \"Neural machine translation by jointly learning to align and translate.\"\n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "            (2) Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglass Eck\n",
    "            \"Online and Linear-Time Attention by Enforcing Monotonic Alignments\"\n",
    "            arXiv arXiv:1704.00784 (2017)\n",
    "        notes:\n",
    "            with `is_monotonic=True`, `normalize_energy=True` equal to model in (2)\n",
    "            with `is_monotonic=False`, `normalize_energy=False` equal to model in (1)\n",
    "        \"\"\"\n",
    "        # self.start_token = alphabet_size\n",
    "        output_dim = alphabet_size  # alphabet + end_token\n",
    "\n",
    "        self.units = units\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.is_monotonic = is_monotonic\n",
    "        self.normalize_energy = normalize_energy\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = dropout\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.activation = activations.get(activation)\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "        self.return_state = False\n",
    "        self.stateful = False\n",
    "        self.uses_learning_phase = True\n",
    "\n",
    "    def add_scalar(self, initial_value=0, name=None, trainable=True):\n",
    "        scalar = K.variable(initial_value, name=name)\n",
    "        if trainable:\n",
    "            self._trainable_weights.append(scalar)\n",
    "        else:\n",
    "            self._non_trainable_weights.append(scalar)\n",
    "        return scalar\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "          See Luong 2017, arXiv:1704.00784\n",
    "          for model details that correspond to the scalars here.\n",
    "        \"\"\"\n",
    "        input_shape = input_shapes\n",
    "        if isinstance(input_shapes[0], (list, tuple)):\n",
    "            if len(input_shapes) > 2:\n",
    "                raise ValueError('Layer ' + self.name + ' expects ' +\n",
    "                                 '1 or 2 input tensors, but it received ' +\n",
    "                                 str(len(input_shapes)) + ' input tensors.')\n",
    "\n",
    "            self.input_spec = [InputSpec(shape=input_shape) for input_shape in input_shapes]\n",
    "            input_shape = input_shapes[0]\n",
    "        else:\n",
    "            self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super(AttentionDecoder, self).reset_states()\n",
    "\n",
    "        self.states = [None, None, None]  # y, s, t\n",
    "\n",
    "        \"\"\"\n",
    "            Embedding matrix for y (outputs)\n",
    "        \"\"\"\n",
    "        # self.E_y = self.add_weight(shape=(self.alphabet_size + 1, self.embedding_dim),  # +1 for start token\n",
    "        #                            name='E_y',\n",
    "        #                            initializer='orthogonal')\n",
    "        self.E_y = K.variable(s2s.bpe_target.embedding_matrix)\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for creating the context vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the r (reset) gate\n",
    "        \"\"\"\n",
    "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_r',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_r = self.add_weight(shape=(self.embedding_dim, self.units),\n",
    "                                   name='W_r',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_r = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_r',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for the z (update) gate\n",
    "        \"\"\"\n",
    "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_z',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_z = self.add_weight(shape=(self.embedding_dim, self.units),\n",
    "                                   name='W_z',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_z = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_z',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the proposal\n",
    "        \"\"\"\n",
    "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_p',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_p = self.add_weight(shape=(self.embedding_dim, self.units),\n",
    "                                   name='W_p',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_p = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_p',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for making the final prediction vector\n",
    "        \"\"\"\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.embedding_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim,),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "        if self.is_monotonic:\n",
    "            self.Energy_r = self.add_scalar(initial_value=-1,\n",
    "                                            name='r')\n",
    "            self.states.append(None)\n",
    "        if self.normalize_energy:\n",
    "            self.Energy_g = self.add_scalar(initial_value=1,\n",
    "                                            name='g')\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        # TODO: check that model is loading from .h5 correctly\n",
    "        # TODO: for now cannot be shared layer\n",
    "        # (only can it we use (or not use) teacher forcing in all cases simultationsly)\n",
    "        if isinstance(x, list):\n",
    "            # teacher forcing for training\n",
    "            self.x_seq, self.y_true = x\n",
    "            self.use_teacher_forcing = True\n",
    "        else:\n",
    "            # inference\n",
    "            self.x_seq = x\n",
    "            self.use_teacher_forcing = False\n",
    "\n",
    "        self.curr_batch_timesteps = tf.shape(self.x_seq)[1]\n",
    "\n",
    "        # apply a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # therefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             dropout=self.dropout,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units,\n",
    "                                             training=training)\n",
    "\n",
    "        return super(AttentionDecoder, self).call(self.x_seq, training=training)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        if isinstance(inputs, list):\n",
    "            assert len(inputs) == 2  # inputs == [encoder_outputs, y_true]\n",
    "            encoder_outputs = inputs[0]\n",
    "        else:\n",
    "            encoder_outputs = inputs\n",
    "\n",
    "        memory_shape = K.shape(encoder_outputs)\n",
    "\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        s0 = activations.tanh(K.dot(encoder_outputs[:, 0], self.W_s))\n",
    "\n",
    "        y0 = K.zeros((memory_shape[0],), dtype='int64') # + self.start_token\n",
    "        t0 = K.zeros((memory_shape[0],), dtype='int64')\n",
    "\n",
    "        initial_states = [y0, s0, t0]\n",
    "        if self.is_monotonic:\n",
    "            # initial attention has form: [1, 0, 0, ..., 0] for each sample in batch\n",
    "            alpha0 = K.ones((memory_shape[0], 1))\n",
    "            alpha0 = K.switch(K.greater(memory_shape[1], 1),\n",
    "                              lambda: K.concatenate([alpha0, K.zeros((memory_shape[0], memory_shape[1] - 1))], axis=-1),\n",
    "                              alpha0)\n",
    "            # like energy, attention is stored in shape (samples, time, 1)\n",
    "            alpha0 = K.expand_dims(alpha0, -1)\n",
    "            initial_states.append(alpha0)\n",
    "\n",
    "        return initial_states\n",
    "\n",
    "    def step(self, x, states):\n",
    "        if self.is_monotonic:\n",
    "            ytm, stm, timestep, previous_attention = states\n",
    "        else:\n",
    "            ytm, stm, timestep = states\n",
    "\n",
    "        ytm = K.gather(self.E_y, K.cast(ytm, 'int32'))\n",
    "\n",
    "        if self.recurrent_dropout is not None and 0. < self.recurrent_dropout < 1.:\n",
    "            stm = K.in_train_phase(K.dropout(stm, self.recurrent_dropout), stm)\n",
    "            ytm = K.in_train_phase(K.dropout(ytm, self.recurrent_dropout), ytm)\n",
    "\n",
    "        et = self._compute_energy(stm)\n",
    "\n",
    "        if self.is_monotonic:\n",
    "            at = self._compute_probabilities(et, previous_attention)\n",
    "        else:\n",
    "            at = self._compute_probabilities(et)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "\n",
    "        # ~~~> calculate new hidden state\n",
    "\n",
    "        # first calculate the \"r\" gate:\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_r)\n",
    "            + K.dot(stm, self.U_r)\n",
    "            + K.dot(context, self.C_r)\n",
    "            + self.b_r)\n",
    "\n",
    "        # now calculate the \"z\" gate\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_z)\n",
    "            + K.dot(stm, self.U_z)\n",
    "            + K.dot(context, self.C_z)\n",
    "            + self.b_z)\n",
    "\n",
    "        # calculate the proposal hidden state:\n",
    "        s_tp = activations.tanh(\n",
    "            K.dot(ytm, self.W_p)\n",
    "            + K.dot((rt * stm), self.U_p)\n",
    "            + K.dot(context, self.C_p)\n",
    "            + self.b_p)\n",
    "\n",
    "        # new hidden state:\n",
    "        st = (1 - zt) * stm + zt * s_tp\n",
    "\n",
    "        yt = activations.softmax(\n",
    "            K.dot(ytm, self.W_o)\n",
    "            + K.dot(st, self.U_o)\n",
    "            + K.dot(context, self.C_o)\n",
    "            + self.b_o)\n",
    "\n",
    "        if self.use_teacher_forcing:\n",
    "            ys = K.in_train_phase(self.y_true[:, timestep[0]], K.argmax(yt, axis=-1))\n",
    "            ys = K.flatten(ys)\n",
    "        else:\n",
    "            ys = K.flatten(K.argmax(yt, axis=-1))\n",
    "\n",
    "        if self.return_probabilities:\n",
    "            output = at\n",
    "        else:\n",
    "            output = yt\n",
    "\n",
    "        next_states = [ys, st, timestep + 1]\n",
    "        if self.is_monotonic:\n",
    "            next_states.append(at)\n",
    "        return output, next_states\n",
    "\n",
    "    def _compute_energy(self, stm):\n",
    "        # \"concat\" energy function\n",
    "        # energy_i = g * V / |V| * tanh([stm, h_i] * W + b) + r\n",
    "        _stm = K.dot(stm, self.W_a)\n",
    "\n",
    "        V_a = self.V_a\n",
    "        if self.normalize_energy:\n",
    "            V_a = self.Energy_g * K.l2_normalize(self.V_a)\n",
    "\n",
    "        et = K.dot(activations.tanh(K.expand_dims(_stm, axis=1) + self._uxpb),\n",
    "                   K.expand_dims(V_a))\n",
    "\n",
    "        if self.is_monotonic:\n",
    "            et += self.Energy_r\n",
    "\n",
    "        return et\n",
    "\n",
    "    def _compute_probabilities(self, energy, previous_attention=None):\n",
    "        if self.is_monotonic:\n",
    "            # add presigmoid noise to encourage discreteness\n",
    "            sigmoid_noise = K.in_train_phase(1., 0.)\n",
    "            noise = K.random_normal(K.shape(energy), mean=0.0, stddev=sigmoid_noise)\n",
    "            # encourage discreteness in train\n",
    "            energy = K.in_train_phase(energy + noise, energy)\n",
    "\n",
    "            p = K.in_train_phase(K.sigmoid(energy),\n",
    "                                 K.cast(energy > 0, energy.dtype))\n",
    "            p = K.squeeze(p, -1)\n",
    "            p_prev = K.squeeze(previous_attention, -1)\n",
    "            # monotonic attention function from tensorflow\n",
    "            at = K.in_train_phase(\n",
    "                tf.contrib.seq2seq.monotonic_attention(p, p_prev, 'parallel'),\n",
    "                tf.contrib.seq2seq.monotonic_attention(p, p_prev, 'hard'))\n",
    "            at = K.expand_dims(at, -1)\n",
    "        else:\n",
    "            # softmax\n",
    "            at = keras.activations.softmax(energy, axis=1)\n",
    "\n",
    "        return at\n",
    "\n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        input_shape = input_shapes\n",
    "        if isinstance(input_shapes[0], (list, tuple)):\n",
    "            input_shape = input_shapes[0]\n",
    "\n",
    "        timesteps = input_shape[1]\n",
    "        if self.return_probabilities:\n",
    "            return (None, timesteps, timesteps)\n",
    "        else:\n",
    "            return (None, timesteps, self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'alphabet_size': self.alphabet_size,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'return_probabilities': self.return_probabilities,\n",
    "            'is_monotonic': self.is_monotonic,\n",
    "            'normalize_energy': self.normalize_energy\n",
    "        }\n",
    "        base_config = super(AttentionDecoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T12:36:03.578658Z",
     "start_time": "2018-06-03T12:36:00.268528Z"
    }
   },
   "outputs": [],
   "source": [
    "#s2s = Seq2SeqWithAttention(\n",
    "s2s = seq2seq.Seq2SeqWithBPE(\n",
    "    bpe_input=europarl.bpe_input,\n",
    "    bpe_target=europarl.bpe_target,\n",
    "    max_len_input=max_len_input,\n",
    "    max_len_target=max_len_target\n",
    ")\n",
    "# s2s.model.compile(optimizer=keras.optimizers.Adam(clipnorm=1., clipvalue=.5), loss='categorical_crossentropy')\n",
    "# s2s.model.summary()\n",
    "import keras.layers as L\n",
    "rnn_encoded = L.Bidirectional(\n",
    "    L.GRU(s2s.latent_dim // 2, return_sequences=True),\n",
    "    name='bidirectional_1',\n",
    "    merge_mode='concat'\n",
    ")(s2s.encoder_embeddings)\n",
    "attention_decoder = AttentionDecoder(\n",
    "    s2s.latent_dim,\n",
    "    len(europarl.bpe_target.tokens),\n",
    "    trainable=EMBEDDING_TRAINABLE,\n",
    "    embedding_dim=s2s.bpe_target.embedding_dim,\n",
    ")(rnn_encoded)\n",
    "\n",
    "# train_generator = s2s.create_batch_generator(\n",
    "#     train_ids, europarl.df.input_sequences, europarl.df.target_sequences, BATCH_SIZE\n",
    "# )\n",
    "# val_generator = s2s.create_batch_generator(\n",
    "#     val_ids, europarl.df.input_sequences, europarl.df.target_sequences, BATCH_SIZE\n",
    "# )\n",
    "# \n",
    "# s2s.model.fit_generator(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=np.ceil(len(train_ids) / BATCH_SIZE),\n",
    "#     epochs=EPOCHS,\n",
    "#     validation_data=val_generator,\n",
    "#     validation_steps=np.ceil(len(val_ids) / BATCH_SIZE),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T12:36:03.607399Z",
     "start_time": "2018-06-03T12:36:03.579974Z"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=s2s.encoder_inputs, outputs=attention_decoder)\n",
    "model.compile(optimizer=keras.optimizers.Adam(clipnorm=1., clipvalue=.5), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T12:36:03.611579Z",
     "start_time": "2018-06-03T12:36:03.608555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_inputs (InputLayer)  (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "input_embedding (Embedding)  (None, 15, 302)           1760358   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 15, 512)           858624    \n",
      "_________________________________________________________________\n",
      "AttentionDecoder (AttentionD (None, 15, 5367)          9947737   \n",
      "=================================================================\n",
      "Total params: 12,566,719\n",
      "Trainable params: 12,566,719\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T13:01:57.005573Z",
     "start_time": "2018-06-03T12:36:03.612907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "421/421 [==============================] - 310s 736ms/step - loss: 2.6987 - val_loss: 2.1716\n",
      "Epoch 2/5\n",
      "421/421 [==============================] - 307s 729ms/step - loss: 1.7508 - val_loss: 1.9120\n",
      "Epoch 3/5\n",
      "421/421 [==============================] - 307s 729ms/step - loss: 1.4160 - val_loss: 1.8793\n",
      "Epoch 4/5\n",
      "421/421 [==============================] - 306s 728ms/step - loss: 1.1772 - val_loss: 1.8524\n",
      "Epoch 5/5\n",
      "421/421 [==============================] - 321s 763ms/step - loss: 0.9972 - val_loss: 1.8574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd36c05ffd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_batch_generator(\n",
    "    samples_ids, input_sequences, target_sequences, batch_size\n",
    "):\n",
    "\n",
    "    def batch_generator():\n",
    "        nr_batches = np.ceil(len(samples_ids) / batch_size)\n",
    "        while True:\n",
    "            shuffled_ids = np.random.permutation(samples_ids)\n",
    "            batch_splits = np.array_split(shuffled_ids, nr_batches)\n",
    "            for batch_ids in batch_splits:\n",
    "                batch_X = pad_sequences(\n",
    "                    input_sequences.iloc[batch_ids],\n",
    "                    padding='post',\n",
    "                    maxlen=max_len_input\n",
    "                )\n",
    "                batch_y = pad_sequences(\n",
    "                    target_sequences.iloc[batch_ids],\n",
    "                    padding='post',\n",
    "                    maxlen=max_len_target\n",
    "                )\n",
    "                batch_y_t_output = keras.utils.to_categorical(\n",
    "                    batch_y[:, 1:],\n",
    "                    num_classes=len(europarl.bpe_target.tokens)\n",
    "                )\n",
    "                batch_x_t_input = batch_y[:, :-1]\n",
    "                #yield ([batch_X, batch_x_t_input], batch_y_t_output)\n",
    "                yield(batch_X, batch_y_t_output)\n",
    "        \n",
    "    return batch_generator()\n",
    "\n",
    "train_generator = create_batch_generator(\n",
    "    train_ids, europarl.df.input_sequences, europarl.df.target_sequences, BATCH_SIZE\n",
    ")\n",
    "val_generator = create_batch_generator(\n",
    "    val_ids, europarl.df.input_sequences, europarl.df.target_sequences, BATCH_SIZE\n",
    ")\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(len(train_ids) / BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=np.ceil(len(val_ids) / BATCH_SIZE),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T13:01:57.258461Z",
     "start_time": "2018-06-03T13:01:57.007278Z"
    }
   },
   "outputs": [],
   "source": [
    "name = 'attentionmodel'\n",
    "# model.save_weights(f'data/{name}_model_weights.h5') \n",
    "s2s.model.save_weights(f'data/{name}_model_weights.h5')  # https://drive.google.com/open?id=10Sv-JnAiUT_fvU_cw1_H7mkcTAipC5aA\n",
    "s2s.inference_encoder_model.save_weights(f'data/{name}_inference_encoder_model_weights.h5')  # https://drive.google.com/open?id=1gNBrn_Wij0PyeE-jJsEnlv7aHXkYuAup\n",
    "s2s.inference_decoder_model.save_weights(f'data/{name}_inference_decoder_model_weights.h5')  # https://drive.google.com/open?id=1LCU53Hnb4m42QO3qsZTAkyYyroqz2vbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T13:07:25.484397Z",
     "start_time": "2018-06-03T13:07:25.457844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "      <th>input_sequences</th>\n",
       "      <th>target_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>agenda</td>\n",
       "      <td>arbeitsplan</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>[1, 631, 222, 34, 2]</td>\n",
       "      <td>[1, 941, 197, 3454, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>what is the result?</td>\n",
       "      <td>was sind die folgen?</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>[1, 781, 14, 3, 714, 2426, 2]</td>\n",
       "      <td>[1, 748, 126, 6, 2374, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>with what aim?</td>\n",
       "      <td>zu welchem zweck?</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>[1, 23, 781, 2973, 2426, 2]</td>\n",
       "      <td>[1, 26, 2740, 156, 155, 142, 359, 188, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>why?</td>\n",
       "      <td>wieso?</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>[1, 958, 38, 2426, 2]</td>\n",
       "      <td>[1, 167, 1659, 3720, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>no.</td>\n",
       "      <td>nein.</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, 220, 5, 2]</td>\n",
       "      <td>[1, 124, 191, 3, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              input_texts          target_texts  input_length  target_length  \\\n",
       "67                 agenda           arbeitsplan             6             11   \n",
       "704   what is the result?  was sind die folgen?            19             20   \n",
       "1261       with what aim?     zu welchem zweck?            14             17   \n",
       "1401                 why?                wieso?             4              6   \n",
       "1403                  no.                 nein.             3              5   \n",
       "\n",
       "                    input_sequences  \\\n",
       "67             [1, 631, 222, 34, 2]   \n",
       "704   [1, 781, 14, 3, 714, 2426, 2]   \n",
       "1261    [1, 23, 781, 2973, 2426, 2]   \n",
       "1401          [1, 958, 38, 2426, 2]   \n",
       "1403                 [1, 220, 5, 2]   \n",
       "\n",
       "                                     target_sequences  \n",
       "67                             [1, 941, 197, 3454, 2]  \n",
       "704                   [1, 748, 126, 6, 2374, 3720, 2]  \n",
       "1261  [1, 26, 2740, 156, 155, 142, 359, 188, 3720, 2]  \n",
       "1401                          [1, 167, 1659, 3720, 2]  \n",
       "1403                              [1, 124, 191, 3, 2]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T13:08:49.891047Z",
     "start_time": "2018-06-03T13:08:49.834469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 15, 5367)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = model.predict(pad_sequences(\n",
    "    [europarl.bpe_input.subword_indices(preprocess('agenda'))],\n",
    "    padding='post',\n",
    "    maxlen=max_len_input\n",
    "), verbose=True)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T13:08:05.439022Z",
     "start_time": "2018-06-03T13:08:05.434562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2s.bpe_target.tokens[np.argmax(p[0, 3, :])]\n",
    "model.call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T08:36:35.781710Z",
     "start_time": "2018-06-03T08:36:29.538Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence, beam_width=5):\n",
    "    return s2s.decode_beam_search(pad_sequences(\n",
    "        [europarl.bpe_input.subword_indices(preprocess(sentence))],\n",
    "        padding='post',\n",
    "        maxlen=max_len_input,\n",
    "    ), beam_width=beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T08:36:35.782283Z",
     "start_time": "2018-06-03T08:36:29.540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T08:36:35.782904Z",
     "start_time": "2018-06-03T08:36:29.542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performance on training set:\n",
    "for en, de in europarl.df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T08:36:35.783442Z",
     "start_time": "2018-06-03T08:36:29.544Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performance on validation set\n",
    "val_df = europarl.df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T08:36:35.783971Z",
     "start_time": "2018-06-03T08:36:29.548Z"
    }
   },
   "outputs": [],
   "source": [
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Translations for short sentences are looking decent. But it's also obvious that for longer sentences the translation gets lost somehow in the sentence and alltough the translated sentence is related to a real translation, it's also confusing and self-repeating.\n",
    "It's worth to notice that the sentences are not too long for a LSTM/GRU model (52, 71) bytepairs for encoding/decoding network. LSTM/GRUs are known to handle sequences up to 100 elements and start decreasing performance at around 60 (for at least that's what the Stanford courses say). So, it could be that a long enough training (we can see here that the training progresses epoch for epoch what's really nice to see for large data) would solve the problem for the choosen sentence lengths here. But of course, it's better to do what humans do also and applicate an attention model instead of trying to keep everything condensed in 512 float32 embedding while also generating bytepair for bytepair.\n",
    "This model is also already a realistic model in terms of training time. I needed around 18h on a GTX1080. Beside implementing attention model, it is tempting to see how a convolutional network might improve the runtime performance (and also quality). But let's get first to Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 294,
   "position": {
    "height": "40px",
    "left": "553px",
    "right": "192px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
